---
title: DeepSeek 开源 FlashMLA 源码分析：一、MLA 是什么？
tags: [C++]
category: 人工智能
toc: true
description: 
---

作为一个后端开发，之前对 LLM 的原理部分了解并不多，最近想着可以入门一下，刚好 DeepSeek 开源了 [FlashMLA](https://github.com/deepseek-ai/FlashMLA) 的源码，就想着可以学习一下。之前自己也没有相关 LLM 推理的基础，好在现在各种 AI 还是很厉害，看看能不能在 AI 的辅助下，吃透 FlashMLA 的源码。

FlashMLA 是一种高效的解码内核，专为 DeepSeek 的多层注意力 MLA 机制设计，优化了 Hopper GPU 的性能。它支持 BF16 精度和分页式键值缓存，适合处理变长序列。它的性能表现卓越，在 H800 SXM5 GPU 上可达 3000 GB/s（内存限制场景）和 580 TFLOPS（计算限制场景）。

上面解释对于刚入门的我来说，有点类似天书了，咱先一步步简单理解下这里的概念，先大致明白 flashmla 是做什么的，然后再慢慢研究它的实现。当然，可能需要系列文章，才能搞明白这里的各种细节了。

<!-- more -->

## 多头注意力的简单理解

在开始了解 MLA 之前，先简单理解下多头注意力机制的背景。下面内容也有不少名词，遇到不懂的自己可以去查查资料，或者先跳过，这里咱们只用了解大概的意思即可。

在深度学习应用于自然语言处理 NLP 之前，业界主要使用 RNN 循环神经网络 或 CNN 卷积神经网络 进行序列建模。但这两种方式都有一定缺点，对于比较长的文本，RNN 难以捕捉远距离文本之间的依赖关系。CNN 主要适用于计算机视觉中的局部模式识别，难以捕捉文本的上下文关系。  

为了克服 RNN/CNN 在长序列文本预测上的局限性，Bahdanau 在机器翻译任务中提出了注意力机制（Attention Mechanism），目的是让模型可以“动态关注”输入序列的不同部分。其核心思想是对输入序列的所有位置进行加权求和，权重由当前解码状态决定，而不是只依赖最后一个隐藏状态。然后通过学习的注意力分数（类似软对齐机制），模型可以更精准地对齐源语言和目标语言的对应关系。

后来，Vaswani 提出的 Self-Attention（自注意力）进一步改进了注意力机制。Self-Attention 直接用于整个输入序列，使得每个词能够关注整个句子中的所有词。这个机制实现了全局依赖建模，但所有单词共享相同的注意力权重计算方式，限制了模型学习不同类型的关系。

为了解决 Self-Attention 可能无法捕捉多种不同的语义关系的问题，Transformer 引入了 多头注意力（Multi-Head Attention, MHA），其核心思想是：**多个独立的注意力头并行计算注意力，每个头学习不同的关注模式**。这样每个头都能捕捉不同的语义模式，比如：一个头可能关注句子的 主谓宾结构，另一个头可能关注 指代关系，还有一个头可能关注 上下文中的时序信息。

## MHA 的缺点以及优化


## 参考文章

[理解Attention:从起源到MHA,MQA和GQA](https://zhuanlan.zhihu.com/p/686149289)