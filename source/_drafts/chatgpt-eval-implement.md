---
title: Evals 源码剖析：深入理解 ChatGPT 评测工具的实现
tags: [ChatGPT, Prompt]
category: 人工智能
toc: true
description: ChatGPT Prompt 最佳指南系列的第六篇文章，
date: 
---

在 [ChatGPT Prompt 最佳指南六：系统基准评测](https://selfboot.cn/2023/07/25/gpt4_prompt_evals/) 一文中，我们一起了解了 OpenAI 开源的 evals 评测工具，evals 能帮助开发者全面评估 GPT 性能，判断不同模型版本或提示词的优劣。上篇文章主要列举了一些中文评测集的例子，如楚辞、翻译、字谜等，简单介绍了匹配评测和翻译评分的方法。本文主要从源码角度分析 evals 的具体实现，来看看它是如何设计和构建的，以及如何完成各种评测任务。

Evals 评测工具是模块化设计的，核心包括输入数据的处理、模型调用、评分计算及输出结果的整理。这种模块化设计不仅使代码易于阅读和维护，还为将来的拓展提供了很好的基础。

## 整体结构
### 数据处理
输入数据的处理模块主要负责读取评测集，将其转化为模型可识别的格式，并对输出数据进行相应处理，为评分模块提供输入。evals 支持多种数据源格式，例如 CSV、JSON 和文本文件等，可以轻松处理各种不同的评测任务。

### 模型调用
这部分的代码主要负责与 GPT 模型进行交互，发送请求并获取模型的输出。它使用了 OpenAI 的 API，但也为其他可能的接口提供了扩展点。

### 评分计算
根据不同的评测任务，evals 提供了多种评分算法。例如，在翻译任务中，它使用 BLEU 评分；在匹配评测中，则采用了精确度、召回率等指标。此模块灵活地支持多种评分策略，并能轻松拓展新的评分方法。

### 结果整理
最后，evals 将评测结果整理成易于理解的格式，并提供了丰富的输出选项，包括文本输出、图表展示等，帮助开发者直观地评估模型的性能。


OpenAI 的 evals 评测工具为开发者提供了一个方便、高效的评测平台，使得模型的性能评估变得更加直观和准确。从源码层面来看，evals 的设计思路和实现方法都值得我们学习和借鉴。希望通过本文的介绍，大家能够更深入地了解 evals 的内部工作机制。