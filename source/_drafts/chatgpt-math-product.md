---
title: 跟 ChatGPT 学数学：向量点积和余弦相似度
tags: [ChatGPT, 数学]
mathjax: true
category: 计算机基础
toc: true
description: 
---


`点积(Dot Product)` 有以下几个重要性质，使其适用于衡量向量间的相似度：

- 夹角的度量：点积与两向量之间的夹角有关。当两个向量的方向完全相同（即夹角为0度）时，点积最大。反之，当两个向量正交（即夹角为90度）时，点积为零。
- 长度的影响：点积同时考虑了向量的长度和方向。因此，长的、方向相似的向量会有更大的点积。
- 计算简便：点积计算相对简单，只涉及基础的算术运算，这使得它在大规模数据处理中非常实用。
- 可解释性：点积的结果可以轻易地通过夹角和长度进行解释，这在许多应用场景（如自然语言处理、推荐系统等）中是有用的。
- 线性性质：点积满足分配律和结合律，这意味着它可以轻易地应用于更复杂的数学结构和算法中。

<!-- more -->
因此，通过计算两个向量的点积，我们可以快速有效地获得它们之间的相似度信息。

## 怎么理解相似？

两个向量的相似度并不是类似于数字的相等，而是更多地关注于两个向量是否“指向”相同或相似的方向。在**不同的应用场景下，相似度的概念也可能有所不同**，但通常包括以下几个方面：

- 方向：向量的方向是相似度最直观的一面。在二维或三维空间里，你可以直观地想象两个向量是否大致指向同一方向。如果两个向量之间的夹角趋近于0度，那么它们就被认为是非常相似的。
- 长度：向量的长度（或模）也可以是衡量相似度的一个因素，尤其是在需要考虑“强度”或“大小”的场合。然而，在很多应用中，例如文本相似度计算，通常会忽略向量长度的影响，只考虑方向。
- 度量方法：不同的度量方法会给出不同类型的相似性信息。除了点积外，还有余弦相似度、欧氏距离、曼哈顿距离等多种方法。
    - 点积: 如前所述，主要衡量方向和长度。
    - 余弦相似度(Cosine Similarity): 只考虑方向，不考虑长度。是点积的一个归一化版本。
    - 欧氏距离: 衡量两点之间的“距离”，而非方向。

在不同的应用领域，相似度的解释和重要性也会有所不同。例如，在自然语言处理中，词向量的相似度常用于衡量词语的**语义相似性**；在推荐系统中，可能用于衡量**用户或物品的相似性**。

总体来说，两个向量的相似度是一个多面的概念，取决于你**关注的是方向、长度，还是其他特性**，以及你所处的具体应用场景。

## 怎么理解多维?

多维向量的相似度实际上是对高维空间中两点关系的一种量化，与二维或三维空间的概念在本质上是相同的，只不过更难以直观地想象。在多维空间中，我们依然可以使用点积、余弦相似度、欧氏距离等方法来计算向量的相似度。

多维向量的点积是所有维度上对应元素相乘然后求和，这个值反映了两个向量在多维空间中的相似程度。数学表达式与低维空间中的相同：


在机器学习、数据挖掘、自然语言处理等多种应用场景下，多维向量的相似度计算都起着关键作用。例如，在文本分类、推荐系统、图像识别等问题中，都会用到这些相似度或距离计算方法。

因此，即使在多维空间中，我们依然可以通过这些方法有效地量化向量间的相似度或差异性。只是与低维空间相比，高维空间更容易受到“**维度灾难**”（Curse of Dimensionality）的影响，这可能会让某些距离或相似度计算方法在应用上变得不那么直观或有效。

## 应用场景

在自然语言处理（NLP）领域，点积和余弦相似度有着广泛的应用。下面是一些具体的例子：

- 文本相似性和文档检索：通过计算文档或句子的词向量（通常是TF-IDF或词嵌入向量）的点积或余弦相似度，可以快速找出内容相似的文档或句子。这在信息检索、文档分类和聚类等任务中非常有用。
- 词义相似性：词嵌入技术（如Word2Vec、GloVe等）生成的词向量可以用点积或余弦相似度来衡量词义的相似性或相关性。
- 机器翻译：在序列到序列的模型，比如 Transformer 中，注意力机制经常使用点积来计算不同词或短语之间的关联强度。
- 情感分析：在情感分析中，可以通过计算文本与预定义情感词的点积或余弦相似度，来评估文本的情感倾向。

这些只是点积和余弦相似度在自然语言处理中应用的一部分，其实还有更多的用途和扩展。这两个概念因其计算简便和解释性强而成为NLP中非常重要的工具。

## 向量哪里来

其实对于自然语言处理，或者其他领域来说，计算向量点积和余弦相似度从来不是难点，难点在于找出一组合适的高维向量，能够提取出关键信息。在上面的应用场景中，提到了一些生成向量的方法，例如词嵌入、TF-IDF等。其实生成向量的方法有很多，比如：

基于词袋模型（Bag-of-Words）：

- 词频（TF）：仅使用词频（Term Frequency）来表示文本。
- TF-IDF（Term Frequency-Inverse Document Frequency）：使用词频（TF）和逆文档频率（IDF）的乘积来表示文本。

基于词嵌入（Word Embeddings）：

- Word2Vec：利用神经网络模型来构建高维的词向量。
- GloVe（Global Vectors for Word Representation）：通过全局统计信息来构建词向量。
- FastText：与 Word2Vec 类似，但考虑了词内的子结构（如字符 n-grams）。

基于语言模型：

- BERT（Bidirectional Encoder Representations from Transformers）：使用 Transformer 模型，并考虑了上下文信息。
- GPT（Generative Pre-trained Transformer）：类似于 BERT，但通常用于生成任务。

选择哪种方法取决于具体的应用场景、可用资源以及所需的准确性。通常，更复杂的方法（如 BERT 或 GPT）能提供更高的准确性，但计算成本也更高。

## 具体计算步骤

下图是一个简单示例，有两个3维向量，\\( \mathbf{A} = [2, 4, 3] \\) 和 \\( \mathbf{B} = [1, 3, 2] \\)

![三维向量的可视化](https://slefboot-1251736664.cos.ap-beijing.myqcloud.com/20230912_chatgpt_math_product_3d_vector.png)

### 人工计算

给定两个 \\( n \\)-维向量 ，它们的点积计算公式为：

\\[
\mathbf{A} \cdot \mathbf{B} = a_1 \cdot b_1 + a_2 \cdot b_2 + \ldots + a_n \cdot b_n
\\]

在这个例子中：

\\[
\mathbf{A} \cdot \mathbf{B} = 2 \times 1 + 4 \times 3 + 3 \times 2 = 2 + 12 + 6 = 20
\\]

余弦相似度是通过计算两个向量的点积并将其**归一化**得到的，具体计算公式为：

\\[
\text{Cosine Similarity} = \frac{\mathbf{A} \cdot \mathbf{B}}{\| \mathbf{A} \| \times \| \mathbf{B} \|}
\\]

其中，\\( \| \mathbf{A} \| \\) 和 \\( \| \mathbf{B} \| \\) 是两个向量的模（长度），可以通过以下公式计算：

\\[
\| \mathbf{A} \| = \sqrt{a_1^2 + a_2^2 + \ldots + a_n^2}
\\]
\\[
\| \mathbf{B} \| = \sqrt{b_1^2 + b_2^2 + \ldots + b_n^2}
\\]

在这个例子中，余弦相似度计算如下：

\\[
\text{Cosine Similarity} = \frac{20}{\sqrt{2^2 + 4^2 + 3^2} \times \sqrt{1^2 + 3^2 + 2^2}} \approx \frac{20}{\sqrt{29} \times \sqrt{14}} \approx 0.993
\\]

点积为20，这是一个相对较大的值，说明两个向量在多维空间中有很好的对齐性。余弦相似度接近1（最大值为1），表示两个向量几乎指向相同的方向。

### Python 高纬计算

用 Python 来计算余弦也是比较方便的，下面例子中，我们随机生成两个 100 维的向量，计算它们的点积和余弦相似度。

```python
import numpy as np

# 随机生成两个 100 维的向量
vector1 = np.random.rand(100)
vector2 = np.random.rand(100)

# 计算点积
dot_product = np.dot(vector1, vector2)

# 计算余弦相似度
cos_similarity = dot_product / (np.linalg.norm(vector1) * np.linalg.norm(vector2))

dot_product, cos_similarity
```

如果重复执行上面步骤 1000 次，计算余弦相似度的分布的话，得到了下面的结果：

![随机 1000 次计算的余弦相似度的一个分布](https://slefboot-1251736664.cos.ap-beijing.myqcloud.com/20230912_chatgpt_math_product_random.png)

从图中可以看出，余弦相似度主要集中在 0.6 到 0.8 的范围内。这是因为我们是随机生成的向量，它们的点积和余弦相似度会服从一种特定的概率分布。
