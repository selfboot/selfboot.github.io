---
title: 如何理解文本向量嵌入(Text Embedding)？
tags:
  - Python
  - 数学
category: 人工智能
toc: true
description: 
mathjax: true
date: 
---

大语言模型火了后，检索增强生成（RAG，Retrieval Augmented Generation）成为目前 LLM 最火的应用场景之一。目前已经有一些很优秀的产品落地，比如智能搜索引擎 [perplexity](https://www.perplexity.ai/), [devv.ai](https://devv.ai/) 等。 RAG 的产品落地上有不少挑战，本文不做探讨，只聚焦 RAG 中的一小部分：文本向量嵌入（Text Embedding）。

![文本向量嵌入 embeding](https://slefboot-1251736664.file.myqcloud.com/20240126_what_is_text_embeding_cover.png)

RAG 系统中，在检索阶段，需要将用户输入的 query 转换为文本向量，然后与文档库中的文档向量进行相似度计算，最后返回相似度最高的文档。这个过程中，query 和文档都需要转换为向量，这就是文本向量嵌入。那么为什么需要将文本转为向量，又是如何转换为向量的呢？文本向量会导致哪些问题呢？本来接下来尝试回答这些问题。

<!-- more -->

注：本文作者并非是专业的 NLP 研究人员，只是对 NLP 感兴趣，所以本文的内容可能存在错误，欢迎指正。

## 为什么要向量化？

**数字是计算机的“母语”**，计算机在最底层上是通过电信号工作的，这些信号只有两种状态：开（1）和关（0）。这种二进制系统就像是计算机的母语，它只理解由0和1组成的语言。所以，当我们使用数字（尤其是二进制数字）与计算机交流时，它能够非常高效地理解和处理这些信息。为了让计算机理解我们生活中的文本、图片、声音等信息，需要将它们转换成数字形式。这就像是把一本书翻译成计算机能理解的语言，一旦转换成数字，计算机就可以方便对这些信息进行处理和分析了。

在自然语言处理（NLP）中，将文本转换为数字是一个非常关键的步骤。这个过程通常被称为“向量化”或“数值化”。将文本转换为数字的做法有以下几个主要好处：

1. **机器可读性**：计算机处理数字数据远比处理原始文本更高效。文本数据通常是**非结构化**的，包含大量的变化和复杂性。将文本转换为数字可以简化数据，使之成为计算机可以有效处理的结构化形式。
2. **数学运算能力**：数字表示使我们能够使用数学和统计方法来分析文本数据。例如，我们可以计算词汇的频率、分布、相似度等，这些都是文本挖掘和模式识别中的重要工具。
3. **模型训练**：现代的机器学习模型，特别是深度学习模型，都是基于数值数据进行训练的。将文本转换为数字允许我们应用这些强大的算法来进行自然语言处理任务，如情感分析、文本分类、机器翻译等。
4. **捕捉语义关系**：通过适当的数值表示方法（如词嵌入），我们可以捕捉词之间的语义关系。例如，在嵌入空间中，语义上相似的词汇会被映射到彼此接近的点上，这有助于提高处理复杂语言结构的能力。
5. **降低维度和复杂性**：文本数据往往具有高维度和复杂性。通过向量化，我们可以降低这些复杂性，同时保留重要的信息。这对于提高计算效率和模型性能至关重要。

将文本转为数字并不容易，因为文本数据通常是非结构化的，包含大量的变化和复杂性。我们希望在转换后的数字数据中保留尽可能多的信息，同时尽可能地降低数据的维度和复杂性。自然语言处理经过多年的发展，出现了很多不同的方法，如词袋模型、TF-IDF、词嵌入等，下面一起来看看。

## 词袋模型

那么如何将文本转换为数字呢？最简单的方法就是将文本中的每个字符转换为数字，比如将 `hello` 转换为 `[8, 5, 12, 12, 15]`。这种方法的缺点是，文本中的每个字符都是独立的，没有考虑到字符之间的关系，比如 `hello` 和 `world` 都包含 `l` 和 `o`，但是这两个词的意思是完全不同的。所以，这种方法并不能很好的表示文本的语义信息。

比直接映射稍微好一点的方法是**词袋模型（Bag of Words, BoW）**。通过这种方法，文本被转换为数值特征向量，每个单词或短语对应向量中的一个维度。可以这样理解词袋模型：

1. **词汇表的创建**：首先，需要从所有文档中创建一个包含不重复单词的列表，称为词汇表。
2. **构建特征向量**：对于词汇表中的每个单词，计算它在单个文档中出现的次数。每个文档都会被转换成一个与词汇表等长的向量，向量中的每个元素是对应单词在文档中的出现次数。
3. **应用于机器学习**：这些特征向量可以被用作机器学习模型的输入，用于分类、聚类或其他任务。

假设有这样两个文本：

- 文本A：“我喜欢看电影，尤其喜欢科幻电影。”
- 文本B：“我不喜欢看书，但喜欢听音乐。”

这里词汇表假设是[“我”, “喜欢”, “看”, “电影”, “书”, “不”, “科幻”, “但”, “听”, “音乐”]。根据词袋模型，这两个文本的向量表示将是：

- 文本A：“我喜欢看电影，尤其喜欢科幻电影。” -> [1, 2, 1, 2, 0, 0, 1, 0, 0, 0]
- 文本B：“我不喜欢看书，但喜欢听音乐。” -> [1, 2, 1, 0, 1, 1, 0, 1, 1, 1]

这里，每个数字代表对应词汇在文本中出现的次数。例如，“喜欢”在文本A中出现了两次，因此对应的向量中该词的位置是2。通过这种方式，即使丢失了单词顺序信息，我们仍然可以获取到词汇的出现信息，这对于很多类型的文本分析来说是足够的。但它有几个主要的缺点：

- **高维度**：词袋模型通常会产生非常高维的数据（取决于词汇表的大小），这在机器学习中导致了所谓的“维数灾难”，即模型变得过于复杂且计算效率低下。
- **忽略词序和语法结构**：词袋模型不考虑单词在文本中的顺序和语法上的联系，这可能导致丢失重要的语义信息。比如，“不是非常喜欢”和“非常不喜欢”在词袋模型中可能会被相似地表示，尽管它们的意义完全相反。
- **稀疏矩阵问题**：如果词汇表非常大，大多数文档中只会用到其中的一小部分词汇，这就导致了大量的零值出现在特征向量中，形成了稀疏矩阵。这不仅增加了存储和计算的负担，而且可能影响某些机器学习算法的性能。

由于这些限制，后来发展出了更多先进的模型，如 TF-IDF、词嵌入（word embeddings）和BERT等深度学习方法，以更好地捕捉文本数据的语义和结构特征。

## TF-IDF 

TF-IDF（**Term Frequency-Inverse Document Frequency**）是一种在文本挖掘和信息检索中常用的权重计算方法，用于评估一个词语对于一个文件集或一个语料库中的其中一份文件的重要性。它通过考虑词语在文档中出现的频率（TF）以及在语料库中出现的反向文档频率（IDF）来增强常用词汇的权重，并抑制常见但信息量小的词汇。通过这种方式，TF-IDF能够减少那些在多个文档中普遍出现的常见词（如"the", "is"等）的影响，同时增强对特定文档更有意义的词的重要性。

词频（TF, Term Frequency）是指词条在文档中出现的频率。这个数字是对词条在文档中出现次数的归一化，以防止它偏向长的文件。

$$
TF(t, d) = \frac{\text{词条 } t \text{ 在文档 } d \text{ 中的出现次数}}{\text{文档 } d \text{ 中的词条总数}}
$$

逆向文件频率（IDF, Inverse Document Frequency）的主要思想是：如果包含词条 t 的文档越少，IDF越大，则说明词条 t 具有很好的类别区分能力。IDF是一个词条重要性增加的度量。

$$
IDF(t, D) = \log \frac{|D|}{|\{d \in D : t \in d\}|}
$$

这里，$|D|$ 是语料库中的文档总数，$|\{d \in D : t \in d\}|$ 是包含词条 t 的文档数目（即，n个文档看作 n次出现的词条）。如果词条不在数据集中，则会导致分母为零，因此一般会在分母上加一，以避免出现除零的情况。

TF-IDF 值由 TF 和 IDF 的乘积得出。计算公式如下：

$$
TFIDF(t, d, D) = TF(t, d) \times IDF(t, D)
$$

这样，TF-IDF 综合考虑了词频和词条在整个语料库中的分布情况，能够有效地表示**词条在文本中的重要性**。这使得TF-IDF非常适用于在各种文本数据中的信息检索和用户的信息需求，如搜索引擎、文档分类和关键词提取等应用。不过 TF-IDF 也有一些缺点，比如：

- **语义信息的缺失**：TF-IDF 仅仅基于词频，没有考虑词语之间的语义关系。例如，同义词会被当作完全不同的词处理。
- **无法处理新词**：TF-IDF 依赖于预先构建的词汇表，新出现的词语如果不在词汇表中，则无法正确处理。
- **向量维度高且稀疏**：每个文档表示为词汇表大小的向量，导致特征空间很大并且向量大多数元素为零，这会影响某些类型的机器学习算法的性能。

## 词向量嵌入

为了更有效地表示文本数据，自然语言处理领域引入了 **Embedding** 技术。**embedding 是一种将大量类别（如词汇、语句、图片、声音等）映射到一个连续的向量空间的技术**。每个类别映射为一个**固定长度的数值向量**，这个向量就是该类别的 "嵌入"。这些嵌入向量通常较小维，比原始数据更易于处理和分析。**Embedding 的目的是为了捕捉类别间的关系，并以数学上更方便处理的形式表达**。

在开始文本向量嵌入前，先来看看最早的词向量嵌入。在自然语言处理中，词嵌入（word embedding）可以使得语义上或语法上相似的词汇**有相近的嵌入向量**。这样，模型可以**通过计算这些向量间的距离**来判断词汇间的相似性。Embedding 的好处包括：

- 降维：将高维数据（如词汇表中成千上万的词汇）转换为低维的向量表示，从而减少计算复杂度。
- 捕捉隐含信息：通过训练，嵌入向量能够捕捉数据间的隐含关系，如语义相近的词汇在向量空间中的距离较近。
- 提高模型性能：在许多机器学习任务中，使用嵌入作为输入可以提高模型的性能。

常见的词嵌入技术(Word Embeding)有 Word2Vec、GloVe（用于词嵌入）等，它们在自然语言处理领域中非常流行。在其他领域，如推荐系统中的用户或物品嵌入，也是使用类似的概念。

那么**如何将一个词转换为向量**呢？

Embedding 将词转换为向量的整体思想是基于“分布式假设”（Distributional Hypothesis），这个假设认为在相似上下文中出现的词汇具有相似的含义。换句话说，一个词的意义可以通过它周围的词来理解和表示。这种思想是嵌入技术的核心，特别是在自然语言处理领域中的词嵌入（Word Embedding）。

朴素贝叶斯模型不需要将文本向量化表示。因为朴素贝叶斯模型记录的是词语的条件概率值，只要对输入各词语的条件概率值进行计算即可得到预测数值。线性分类模型需要对文本进行向量化表示。因为线性模型记录的是线性表达式系数，必须输入一个数值向量才能计算得到预测数值。



## 参考文章

[没有思考过 Embedding，不足以谈 AI](https://zhuanlan.zhihu.com/p/643560252)
[Introducing text and code embeddings](https://openai.com/blog/introducing-text-and-code-embeddings)
[OpenAI doc: Embeddings](https://platform.openai.com/docs/guides/embeddings)