---
title: 如何理解文本向量嵌入(Text Embedding)？
tags:
  - Python
  - 数学
category: 人工智能
toc: true
description: 
date: 
---

大语言模型火了后，检索增强生成（RAG，Retrieval Augmented Generation）成为目前 LLM 最火的应用场景之一。目前已经有一些很优秀的产品落地，比如智能搜索引擎 [perplexity](https://www.perplexity.ai/), [devv.ai](https://devv.ai/) 等。 RAG 的产品落地上有不少挑战，本文不做探讨，只聚焦 RAG 中的一小部分：文本向量嵌入（Text Embedding）。

![文本向量嵌入 embeding](https://slefboot-1251736664.file.myqcloud.com/20240126_what_is_text_embeding_cover.png)

RAG 系统中，在检索阶段，需要将用户输入的 query 转换为文本向量，然后与文档库中的文档向量进行相似度计算，最后返回相似度最高的文档。这个过程中，query 和文档都需要转换为向量，这就是文本向量嵌入。那么为什么需要将文本转为向量，又是如何转换为向量的呢？本来接下来尝试回答这两个问题。

<!-- more -->

注：本文作者并非是专业的 NLP 研究人员，只是对 NLP 感兴趣，所以本文的内容可能存在错误，欢迎指正。

## 为什么要向量化？

**数字是计算机的“母语”**，计算机在最底层上是通过电信号工作的，这些信号只有两种状态：开（1）和关（0）。这种二进制系统就像是计算机的母语，它只理解由0和1组成的语言。所以，当我们使用数字（尤其是二进制数字）与计算机交流时，它能够非常高效地理解和处理这些信息。为了让计算机理解我们生活中的文本、图片、声音等信息，需要将它们转换成数字形式。这就像是把一本书翻译成计算机能理解的语言，一旦转换成数字，计算机就可以方便对这些信息进行处理和分析了。

在自然语言处理（NLP）中，将文本转换为数字是一个非常关键的步骤。这个过程通常被称为“向量化”或“数值化”。将文本转换为数字的做法有以下几个主要好处：

1. **机器可读性**：计算机处理数字数据远比处理原始文本更高效。文本数据通常是**非结构化**的，包含大量的变化和复杂性。将文本转换为数字可以简化数据，使之成为计算机可以有效处理的结构化形式。
2. **数学运算能力**：数字表示使我们能够使用数学和统计方法来分析文本数据。例如，我们可以计算词汇的频率、分布、相似度等，这些都是文本挖掘和模式识别中的重要工具。
3. **模型训练**：现代的机器学习模型，特别是深度学习模型，都是基于数值数据进行训练的。将文本转换为数字允许我们应用这些强大的算法来进行自然语言处理任务，如情感分析、文本分类、机器翻译等。
4. **捕捉语义关系**：通过适当的数值表示方法（如词嵌入），我们可以捕捉词之间的语义关系。例如，在嵌入空间中，语义上相似的词汇会被映射到彼此接近的点上，这有助于提高处理复杂语言结构的能力。
5. **降低维度和复杂性**：文本数据往往具有高维度和复杂性。通过向量化，我们可以降低这些复杂性，同时保留重要的信息。这对于提高计算效率和模型性能至关重要。

将文本转为数字并不容易，因为文本数据通常是非结构化的，包含大量的变化和复杂性。我们希望在转换后的数字数据中保留尽可能多的信息，同时尽可能地降低数据的维度和复杂性。自然语言处理经过多年的发展，出现了很多不同的方法，如词袋模型、TF-IDF、词嵌入等，下面一起来看看。


## 词袋模型


那么如何将文本转换为数字呢？最简单的方法就是将文本中的每个字符转换为数字，比如将 `hello` 转换为 `[8, 5, 12, 12, 15]`。这种方法的缺点是，文本中的每个字符都是独立的，没有考虑到字符之间的关系，比如 `hello` 和 `world` 都包含 `l` 和 `o`，但是这两个词的意思是完全不同的。所以，这种方法并不能很好的表示文本的语义信息。

文本数据的最开始的表示方法是“**词袋模型**”（Bag of Words, BoW）。这种方法简单地统计单词出现的频次。



但它有两个主要的缺点：

- 高维度：词袋模型通常会产生非常高维的数据（取决于词汇表的大小），这在机器学习中导致了所谓的“维数灾难”，即模型变得过于复杂且计算效率低下。
- 缺乏语义信息：在词袋模型中，单词仅仅被视为独立的符号，这意味着它们之间的语义关系（比如同义词或反义词）并没有得到体现。

TF-IDF是词袋模型的一种扩展，它不仅考虑了词在单个文档中的频率（TF，Term Frequency），还考虑了词在所有文档中的分布频率（IDF，Inverse Document Frequency）。
通过这种方式，TF-IDF能够减少那些在多个文档中普遍出现的常见词（如"the", "is"等）的影响，同时增强对特定文档更有意义的词的重要性。
因此，TF-IDF是一种更精细化的词袋模型表示，常用于文本挖掘和信息检索领域。



## 词向量嵌入

Embedding 在自然语言处理（NLP，） 中的出现，主要是为了解决如何有效表示文本数据的问题，特别是在计算机能够理解和处理的方式上。

简单来说，**embedding 是一种将大量类别（如词汇、语句、图片、声音等）映射到一个连续的向量空间的技术**。每个类别映射为一个**固定长度的数值向量**，这个向量就是该类别的 "嵌入"。这些嵌入向量通常较小维，比原始数据更易于处理和分析。**Embedding 的目的是为了捕捉类别间的关系，并以数学上更方便处理的形式表达**。



为了克服这些限制，embedding技术被引入。Embedding的基本思想是将单词映射到一个低维的连续向量空间中，这些向量能够捕捉单词之间的语义关系。



在开始文本向量嵌入前，先来看看最早的词向量嵌入。

在自然语言处理中，词嵌入（word embedding）可以使得语义上或语法上相似的词汇**有相近的嵌入向量**。这样，模型可以**通过计算这些向量间的距离**来判断词汇间的相似性。Embedding 的好处包括：

- 降维：将高维数据（如词汇表中成千上万的词汇）转换为低维的向量表示，从而减少计算复杂度。
- 捕捉隐含信息：通过训练，嵌入向量能够捕捉数据间的隐含关系，如语义相近的词汇在向量空间中的距离较近。
- 提高模型性能：在许多机器学习任务中，使用嵌入作为输入可以提高模型的性能。

常见的词嵌入技术(Word Embeding)有 Word2Vec、GloVe（用于词嵌入）等，它们在自然语言处理领域中非常流行。在其他领域，如推荐系统中的用户或物品嵌入，也是使用类似的概念。

那么**如何将一个词转换为向量**呢？

Embedding 将词转换为向量的整体思想是基于“分布式假设”（Distributional Hypothesis），这个假设认为在相似上下文中出现的词汇具有相似的含义。换句话说，一个词的意义可以通过它周围的词来理解和表示。这种思想是嵌入技术的核心，特别是在自然语言处理领域中的词嵌入（Word Embedding）。


朴素贝叶斯模型不需要将文本向量化表示。因为朴素贝叶斯模型记录的是词语的条件概率值，只要对输入各词语的条件概率值进行计算即可得到预测数值。线性分类模型需要对文本进行向量化表示。因为线性模型记录的是线性表达式系数，必须输入一个数值向量才能计算得到预测数值。


## 参考文章

[没有思考过 Embedding，不足以谈 AI](https://zhuanlan.zhihu.com/p/643560252)
[Introducing text and code embeddings](https://openai.com/blog/introducing-text-and-code-embeddings)
[OpenAI doc: Embeddings](https://platform.openai.com/docs/guides/embeddings)