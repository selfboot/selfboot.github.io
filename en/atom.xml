<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Just For Fun</title>
    <link>https://selfboot.cn/</link>
    
    <image>
      <url>https://www.gravatar.com/avatar/0de0c23d97c75300e32f8494b1485fb8</url>
      <title>Just For Fun</title>
      <link>https://selfboot.cn/</link>
    </image>
    
    <atom:link href="https://selfboot.cn/en/atom.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Fri, 18 Jul 2025 02:41:01 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    
    <item>
      <title>LevelDB Explained - Prefix Compression and Restart Points in BlockBuilder</title>
      <link>https://selfboot.cn/en/2025/07/18/leveldb_source_block_build/</link>
      <guid>https://selfboot.cn/en/2025/07/18/leveldb_source_block_build/</guid>
      <pubDate>Fri, 18 Jul 2025 12:00:00 GMT</pubDate>
      
      <description>This article explains in detail how LevelDB uses prefix compression to reduce key storage space and achieves a perfect balance between compression ratio and query performance through a clever restart points mechanism. We will use specific examples and code implementations to help you understand how BlockBuilder works and how the block_restart_interval parameter affects performance.</description>
      
      
      
      <content:encoded><![CDATA[<p>In LevelDB, SSTable (Sorted Strings Table) is the file format for storing key-value pairs. A previous article, <a href="https://selfboot.cn/en/2025/06/27/leveldb_source_table_build/">LevelDB Explained - A Step by Step Guide to SSTable Build</a>, introduced the creation process of SSTable files, where we learned that an SSTable file is composed of multiple data blocks, which are the <strong>fundamental units of the file</strong>.</p><p>These data blocks can be categorized into two types: key-value data blocks and filter data blocks. Accordingly, LevelDB implements two types of BlockBuilder classes to assemble them: BlockBuilder and FilterBlockBuilder. In this article, we’ll dive into the implementation of BlockBuilder.</p><p>First, let’s look at a simple diagram showing the storage structure of a DataBlock in LevelDB. The source for the diagram can be found at <a href="https://selfboot.cn/downloads/leveldb_datablock_en.dot">leveldb_datablock_en.dot</a>.</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250718_leveldb_source_block_build_total_en.webp" alt="LevelDB DataBlock Storage Structure"></p><span id="more"></span><p>Next, we’ll use this diagram to understand the prefix compression and restart point mechanisms.</p><h2 id="How-to-Store-Key-Value-Pairs-Efficiently"><a href="#How-to-Store-Key-Value-Pairs-Efficiently" class="headerlink" title="How to Store Key-Value Pairs Efficiently?"></a>How to Store Key-Value Pairs Efficiently?</h2><p>As we know, a DataBlock is used to store sorted key-value pairs. The simplest approach would be to store them one by one, perhaps in a format like <code>[keysize, key, valuesize, value]</code>. A possible storage result would look like this:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[3, &quot;app&quot;, 6, &quot;value1&quot;]</span><br><span class="line">[5, &quot;apple&quot;, 6, &quot;value2&quot;] </span><br><span class="line">[6, &quot;applet&quot;, 6, &quot;value3&quot;]</span><br><span class="line">[5, &quot;apply&quot;, 6, &quot;value4&quot;]</span><br></pre></td></tr></table></figure><p>Observing these keys, we notice a significant issue: <strong>a large number of shared prefixes</strong>.</p><ul><li>app, apple, applet, and apply all share the prefix “app”.</li><li>apple and applet also share the additional prefix “appl”.</li></ul><p>While this example is constructed, real-world scenarios often involve keys with many common prefixes. These shared prefixes waste a considerable amount of disk space and require transferring more redundant data during reads. If a DataBlock is cached in memory, <strong>this redundant data also consumes more memory</strong>.</p><h3 id="Prefix-Compression"><a href="#Prefix-Compression" class="headerlink" title="Prefix Compression"></a>Prefix Compression</h3><p>As a low-level storage component, LevelDB must consider storage efficiency. To address this, LevelDB uses a <strong>prefix compression</strong> storage format. The core idea is: <strong><span style="color: red;">for sorted key-value pairs, subsequent keys only store the part that differs from the previous key</span></strong>.</p><p>The specific storage format becomes:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[shared_len, non_shared_len, value_len, non_shared_key, value]</span><br></pre></td></tr></table></figure><p>Where shared_len is the length of the shared prefix with the previous key, non_shared_len is the length of the non-shared part, value_len is the length of the value, non_shared_key is the non-shared part of the key, and value is the actual value.</p><p>Let’s see the effect with our previous example, focusing on the change in key length after prefix compression:</p><table><thead><tr><th>Full Key</th><th>shared_len</th><th>non_shared_len</th><th>non_shared_key</th><th>Storage Cost Analysis</th></tr></thead><tbody><tr><td>app</td><td>0</td><td>3</td><td>“app”</td><td>Original: 1+3&#x3D;4, Compressed: 1+1+1+3&#x3D;6, <strong>uses 2 more bytes</strong></td></tr><tr><td>apple</td><td>3</td><td>2</td><td>“le”</td><td>Original: 1+5&#x3D;6, Compressed: 1+1+1+2&#x3D;5, <strong>saves 1 byte</strong></td></tr><tr><td>applet</td><td>5</td><td>1</td><td>“t”</td><td>Original: 1+6&#x3D;7, Compressed: 1+1+1+1&#x3D;4, <strong>saves 3 bytes</strong></td></tr><tr><td>apply</td><td>4</td><td>1</td><td>“y”</td><td>Original: 1+5&#x3D;6, Compressed: 1+1+1+1&#x3D;4, <strong>saves 2 bytes</strong></td></tr></tbody></table><p>Of course, for simplicity, we assume the length fields are 1 byte. In reality, LevelDB uses variable-length encoding (Varint), but for small lengths, it is indeed 1 byte. A careful calculation reveals that prefix compression isn’t just about saving on repeated prefixes; it requires a <strong>trade-off between the prefix length and the storage overhead of additional metadata</strong>.</p><p>In this example, we save a total of (1+3+2-2) &#x3D; 4 bytes. For most business scenarios, this can definitely save a significant amount of storage space.</p><h3 id="Restart-Points-Mechanism"><a href="#Restart-Points-Mechanism" class="headerlink" title="Restart Points Mechanism"></a>Restart Points Mechanism</h3><p>Looks perfect, right? Not so fast. Let’s see what problems we encounter when reading key-value pairs. If we want to find the key “apply”, all we see in the prefix-compressed storage is:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[4, 1, 4, &quot;y&quot;, ...]</span><br></pre></td></tr></table></figure><p>To reconstruct the full key, we have to start from the first key, sequentially read and rebuild each key’s full content until we find our target. What’s the problem with this? It’s inefficient! The reason we store keys in sorted order is to use binary search for fast lookups. Now, with prefix compression, we’re forced into a linear scan, which can be very slow in large data blocks.</p><p>So, what’s the solution? Abandon prefix compression? Or find another way? In computer science, we often face similar problems, and the solution is usually a compromise, finding a balance between storage and lookup efficiency.</p><p>In its implementation, LevelDB introduces <strong>Restart Points</strong> to balance storage and lookup efficiency. The method is quite simple: every N keys, the full key content is stored. A key stored with its full content is called a restart point.</p><p>But restart points alone are not enough. <strong>We also need an index to quickly find all the full keys within a block</strong>. LevelDB’s approach here is also straightforward: it records the offset of each restart point at the end of the DataBlock.</p><p>During a query, by using the restart point offsets stored at the end, we can read all the complete keys at these restart points. Then, we can use binary search to quickly locate the interval where the key should be. After that, we can start a sequential read from that restart point until we find the target key. In this case, we’d need to read at most N keys to find the target. We’ll explore the logic for this in a future article.</p><h2 id="Building-a-DataBlock-Code-Walkthrough"><a href="#Building-a-DataBlock-Code-Walkthrough" class="headerlink" title="Building a DataBlock: Code Walkthrough"></a>Building a DataBlock: Code Walkthrough</h2><p>With the overall logic clear, let’s look at the code implementation. It’s located in <a href="https://github.com/google/leveldb/blob/main/table/block_builder.cc">table&#x2F;block_builder.cc</a>, and the code is relatively short and easy to understand.</p><p>First, let’s look at a few internal member variables. Seeing them gives us a good idea of the implementation logic. In <a href="https://github.com/google/leveldb/blob/main/table/block_builder.h#L43">table&#x2F;block_builder.h</a>:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BlockBuilder</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">BlockBuilder</span><span class="params">(<span class="type">const</span> Options* options)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">BlockBuilder</span>(<span class="type">const</span> BlockBuilder&amp;) = <span class="keyword">delete</span>;</span><br><span class="line">  BlockBuilder&amp; <span class="keyword">operator</span>=(<span class="type">const</span> BlockBuilder&amp;) = <span class="keyword">delete</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="type">const</span> Options* options_;</span><br><span class="line">  std::string buffer_;              <span class="comment">// Destination buffer</span></span><br><span class="line">  std::vector&lt;<span class="type">uint32_t</span>&gt; restarts_;  <span class="comment">// Restart points</span></span><br><span class="line">  <span class="type">int</span> counter_;                     <span class="comment">// Number of entries emitted since restart</span></span><br><span class="line">  <span class="type">bool</span> finished_;                   <span class="comment">// Has Finish() been called?</span></span><br><span class="line">  std::string last_key_;</span><br></pre></td></tr></table></figure><p>Here, buffer_ is where the DataBlock data is stored, and the restarts_ vector records the offsets of all restart points. counter_ is used to count the number of key-value pairs stored since the last restart point; when it reaches a configured threshold, a new restart point is set.</p><p>finished_ records whether the build process is complete and is used to write the trailer data. last_key_ stores the previous key for prefix compression.</p><h3 id="Adding-a-Key-Value-Pair"><a href="#Adding-a-Key-Value-Pair" class="headerlink" title="Adding a Key-Value Pair"></a>Adding a Key-Value Pair</h3><p>The BlockBuilder has two core methods: Add and Finish. Let’s first look at <a href="https://github.com/google/leveldb/blob/main/table/block_builder.cc#L71">BlockBuilder::Add</a>. The logic is clear (some assert checks are removed for brevity).</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">BlockBuilder::Add</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">const</span> Slice&amp; value)</span> </span>&#123;</span><br><span class="line">  <span class="function">Slice <span class="title">last_key_piece</span><span class="params">(last_key_)</span></span>;</span><br><span class="line">  <span class="type">size_t</span> shared = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (counter_ &lt; options_-&gt;block_restart_interval) &#123;</span><br><span class="line">    <span class="comment">// See how much sharing to do with previous string</span></span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> min_length = std::<span class="built_in">min</span>(last_key_piece.<span class="built_in">size</span>(), key.<span class="built_in">size</span>());</span><br><span class="line">    <span class="keyword">while</span> ((shared &lt; min_length) &amp;&amp; (last_key_piece[shared] == key[shared])) &#123;</span><br><span class="line">      shared++;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Restart compression</span></span><br><span class="line">    restarts_.<span class="built_in">push_back</span>(buffer_.<span class="built_in">size</span>());</span><br><span class="line">    counter_ = <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">const</span> <span class="type">size_t</span> non_shared = key.<span class="built_in">size</span>() - shared;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Add &quot;&lt;shared&gt;&lt;non_shared&gt;&lt;value_size&gt;&quot; to buffer_</span></span><br><span class="line">  <span class="built_in">PutVarint32</span>(&amp;buffer_, shared);</span><br><span class="line">  <span class="built_in">PutVarint32</span>(&amp;buffer_, non_shared);</span><br><span class="line">  <span class="built_in">PutVarint32</span>(&amp;buffer_, value.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Add string delta to buffer_ followed by value</span></span><br><span class="line">  buffer_.<span class="built_in">append</span>(key.<span class="built_in">data</span>() + shared, non_shared);</span><br><span class="line">  buffer_.<span class="built_in">append</span>(value.<span class="built_in">data</span>(), value.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Update state</span></span><br><span class="line">  last_key_.<span class="built_in">resize</span>(shared);</span><br><span class="line">  last_key_.<span class="built_in">append</span>(key.<span class="built_in">data</span>() + shared, non_shared);</span><br><span class="line">  <span class="built_in">assert</span>(<span class="built_in">Slice</span>(last_key_) == key);</span><br><span class="line">  counter_++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The code is elegant and easy to understand. I’ll add a small detail about the optimization of last_key_. We see that last_key_ is a std::string. Each time last_key_ is updated, it first reuses the shared part (resize) and then appends the non-shared part. For keys with long common prefixes, this update method can save a lot of memory allocations.</p><p>When all keys have been added, the caller invokes the Finish method, which writes the restart point array and its size to the end of the buffer and returns the entire buffer as a Slice object.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Slice <span class="title">BlockBuilder::Finish</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Append restart array</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; restarts_.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">    <span class="built_in">PutFixed32</span>(&amp;buffer_, restarts_[i]);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">PutFixed32</span>(&amp;buffer_, restarts_.<span class="built_in">size</span>());</span><br><span class="line">  finished_ = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">Slice</span>(buffer_);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The caller then uses this Slice object to write to the SSTable file.</p><h2 id="Choosing-the-Restart-Interval"><a href="#Choosing-the-Restart-Interval" class="headerlink" title="Choosing the Restart Interval"></a>Choosing the Restart Interval</h2><p>So far, we’ve covered the optimization details and code implementation of the DataBlock build process in LevelDB. We haven’t mentioned the restart interval size, which is controlled by the block_restart_interval option in <a href="https://github.com/google/leveldb/blob/main/include/leveldb/options.h#L106">options.h</a>, with a default value of 16.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Number of keys between restart points for delta encoding of keys.</span></span><br><span class="line"><span class="comment">// This parameter can be changed dynamically.  Most clients should</span></span><br><span class="line"><span class="comment">// leave this parameter alone.</span></span><br><span class="line"><span class="type">int</span> block_restart_interval = <span class="number">16</span>;</span><br></pre></td></tr></table></figure><p><strong>Why is this value 16? Can it be adjusted for our own use cases?</strong></p><p>First, the default of 16 in LevelDB is likely a magic number chosen by the authors after testing. However, looking at the open-source code, there’s no benchmark data for different intervals. The table_test.cc file only contains functional tests for different intervals.</p><p>Second, how should we choose this interval for our own services? We need to understand that this interval is primarily a trade-off between compression and query performance. If it’s set too small, the compression ratio will decrease. If it’s set too large, the compression ratio improves, but the number of keys to scan linearly during a lookup increases.</p><p>The default block size in LevelDB is 4KB. Assuming an average key-value pair is 100 bytes, a 4KB block can store about 40 key-value pairs. If the restart interval is 16, then there would be about 3 restart points per block.</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">restart_point</span>[<span class="number">0</span>]: <span class="string">&quot;user:12345:profile&quot;</span> (keys <span class="number">1</span>-<span class="number">16</span>)</span><br><span class="line"><span class="attribute">restart_point</span>[<span class="number">1</span>]: <span class="string">&quot;user:12350:account&quot;</span> (keys <span class="number">17</span>-<span class="number">32</span>)  &lt;-- Target interval</span><br><span class="line"><span class="attribute">restart_point</span>[<span class="number">2</span>]: <span class="string">&quot;user:12355:profile&quot;</span> (keys <span class="number">33</span>-<span class="number">40</span>)</span><br></pre></td></tr></table></figure><p>A binary search would take at most 2 comparisons to find the interval. Then, the subsequent scan would require reading at most 15 keys to find the target. The overall lookup cost is quite acceptable.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Once you understand prefix compression and the restart point mechanism, the process of building a DataBlock is actually quite simple. Next, I will continue to analyze the process of reading and parsing a DataBlock, as well as the construction and parsing of the FilterBlock.</p>]]></content:encoded>
      
      
      <category domain="https://selfboot.cn/categories/Source-Code-Analysis/">Source Code Analysis</category>
      
      
      <category domain="https://selfboot.cn/tags/C/">C++</category>
      
      <category domain="https://selfboot.cn/tags/LevelDB/">LevelDB</category>
      
      
      <comments>https://selfboot.cn/en/2025/07/18/leveldb_source_block_build/#disqus_thread</comments>
      
    </item>
    
    
    
    
    
    <item>
      <title>LevelDB Explained - A Step by Step Guide to SSTable Build</title>
      <link>https://selfboot.cn/en/2025/06/27/leveldb_source_table_build/</link>
      <guid>https://selfboot.cn/en/2025/06/27/leveldb_source_table_build/</guid>
      <pubDate>Fri, 27 Jun 2025 13:00:00 GMT</pubDate>
      
      <description>A deep dive into the creation process and internal structure of SSTable files in LevelDB. This article takes a problem-driven approach to analyze how SSTable achieves high read/write performance through mechanisms like block storage, index optimization, and filters. It focuses on the implementation details of the TableBuilder class, including the construction of DataBlocks, IndexBlocks, and FilterBlocks, as well as engineering techniques like index key optimization and compression strategies. Through source code analysis, it demonstrates how LevelDB solves key problems in large-scale data storage, quickly locating data blocks, reducing unnecessary disk I/O, and balancing storage space with query efficiency. With concrete code examples and flowcharts, the article helps readers deeply understand the ingenious design of the SSTable file format and the core implementation principles of LevelDB as a high-performance key-value storage engine.</description>
      
      
      
      <content:encoded><![CDATA[<p>In LevelDB, when the key-value pairs in the in-memory table (MemTable) reach a certain size, they are flushed to disk as an SSTable file. The disk files are also layered, with each layer containing multiple SSTable files. During runtime, LevelDB will merge and reorganize SSTable files at appropriate times, continuously compacting data into lower layers.</p><p>SSTable has a specific format for organizing data, aiming to ensure data is sorted and can be looked up quickly. So, how does SSTable store these key-value pairs internally, and how does it improve read and write performance? What are the optimization points in the implementation of the entire SSTable file?</p><p>In this article, we will carefully analyze the creation process of SSTable files, breaking it down step by step to see how it’s implemented. Before we begin, here’s a high-level diagram to give you a general idea.</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250627_leveldb_source_table_build_total_en.webp" alt="Overview of SSTable file building steps"></p><span id="more"></span><h2 id="The-Rationale-Behind-SSTable-File-Format-Design"><a href="#The-Rationale-Behind-SSTable-File-Format-Design" class="headerlink" title="The Rationale Behind SSTable File Format Design"></a>The Rationale Behind SSTable File Format Design</h2><p>Before we start, we need to understand a key question: <strong><span style="color: red;">How should a file format be designed to balance efficient writes and fast reads?</span></strong> Let’s start with a few fundamental questions to deduce how the author designed the data format.</p><h3 id="Question-1-How-should-key-value-pairs-be-stored"><a href="#Question-1-How-should-key-value-pairs-be-stored" class="headerlink" title="Question 1: How should key-value pairs be stored?"></a>Question 1: How should key-value pairs be stored?</h3><p>First, the core data, the user’s key-value pairs, needs a place to be stored. The simplest method is to put all key-value pairs sequentially into one large SSTable file. What’s the problem with this? First, when reading, if you want to find a specific key, you have to scan the entire large file. Second, during writes, if every key-value pair addition triggers a disk write, the write I&#x2F;O pressure would be huge, and throughput would suffer.</p><p>Since a single large file is not ideal, let’s break it into blocks. The “divide and conquer” principle from computer science is well-reflected in LevelDB. We can split the large file into different data blocks and store key-value pairs at the block granularity. Each block is about 4KB by default. When a block is full, it’s written to the file as a whole, and then we start writing to the next one. Writing in larger chunks reduces a lot of disk I&#x2F;O.</p><p>Let me add one more point here. Block-based storage not only reduces the number of disk write I&#x2F;Os but also works in conjunction with…</p><p>However, the lookup problem isn’t solved yet. We still have to iterate through all the blocks to find a key.</p><p>If we could quickly locate which DataBlock a key is in, we would only need to scan a single block, which would greatly improve efficiency.</p><h3 id="Question-2-How-to-quickly-locate-a-specific-Data-Block"><a href="#Question-2-How-to-quickly-locate-a-specific-Data-Block" class="headerlink" title="Question 2: How to quickly locate a specific Data Block?"></a>Question 2: How to quickly locate a specific Data Block?</h3><p>To solve this problem, we need a “directory.” In computer science, this is called an index, and thus the Index Block was born. This block stores a series of index entries, each pointing to a Data Block. With this record, we can quickly determine which DataBlock a key belongs to.</p><p>This way, when looking up a key, we can perform a binary search in the Index Block to quickly locate the DataBlock where the key might be. Since the Index Block only contains index data, it’s much smaller and can usually be loaded into memory, making lookups much faster.</p><p>With the Index Block, we’ve transformed a full file scan into a “look up the index -&gt; read a specific small block” operation, greatly improving efficiency. We’ll ignore the specific design of the index for now and analyze it in detail later.</p><h3 id="Question-3-How-to-avoid-unnecessary-disk-reads"><a href="#Question-3-How-to-avoid-unnecessary-disk-reads" class="headerlink" title="Question 3: How to avoid unnecessary disk reads?"></a>Question 3: How to avoid unnecessary disk reads?</h3><p>With the Index Block, we can find the block where a key <em>might</em> be, but it’s not 100% certain. Why? Because the Index Block only records key ranges, it doesn’t guarantee the key is definitely within that range. This will become clearer when we look at the code. This leads to a problem: what if we excitedly read a Data Block from disk into memory, only to find that the key we’re looking for doesn’t exist? That’s a wasted, precious disk I&#x2F;O. This waste can be significant, especially in scenarios with many lookups for non-existent keys.</p><p>Computer science has long had a solution for this kind of existence check: the Bloom filter. A Bloom filter is a combination of a bit array and hash functions that can quickly determine if an element is in a set. A previous article in this series, <a href="https://selfboot.cn/en/2024/08/08/leveldb_source_bloom_filter/">LevelDB Explained - Bloom Filter Implementation and Visualization</a>, provides a detailed introduction to the principles and implementation of Bloom filters.</p><p>LevelDB uses a similar approach. It supports an optional Filter Block. Before reading a Data Block, we can first check the Filter Block to see if the key exists. If it says the key is not present, we can return immediately. If it says the key might be present, we then proceed to read the Data Block for confirmation. This approach significantly reduces unnecessary queries for non-existent keys.</p><p>This all sounds great, but wait, there’s another problem. How do we know where the Index Block and Filter Block are located within the SSTable file?</p><h3 id="Question-4-How-to-locate-the-Index-and-Filter-Blocks"><a href="#Question-4-How-to-locate-the-Index-and-Filter-Blocks" class="headerlink" title="Question 4: How to locate the Index and Filter Blocks?"></a>Question 4: How to locate the Index and Filter Blocks?</h3><p>So now we have many data blocks, an Index Block, and a Filter Block. Another question arises: when we open an SSTable file, how do we know the locations of the Index Block and Filter Block?</p><p>The most straightforward idea is to place this metadata at a <strong>fixed offset in the file</strong>. However, if we put it at the beginning of the file, any change to this metadata would require moving the entire file’s data, which is obviously not acceptable.</p><p>What about putting it at the end of the file? That seems feasible, and it’s how LevelDB is designed. At the end of the file, there’s a fixed-size 48-byte Footer area. It records the offset of the Index Block and another block we haven’t mentioned, the Meta-Index Block.</p><p>Logically, the Footer could just store the locations of the Index Block and Filter Block. Why introduce a Meta-Index Block? The author mentioned in the code comments that it’s for extensibility. The Footer has a fixed size and cannot be expanded to include more information. What if future versions need more types of metadata blocks, like statistics blocks?</p><p>So the author added an index for metadata—the Meta-Index Block. This block acts as a directory for metadata. Its keys are the names of the metadata (e.g., “filter.leveldb.BuiltinBloomFilter2”), and its values are the offsets of the corresponding metadata blocks (like the Filter Block). Currently, it only contains filter block information, but it can be extended to include any number of metadata blocks in the future.</p><p>This ties the whole lookup process together. First, we read the fixed 48 bytes from the end of the file. From there, we parse the offsets of the Index Block and the Meta-Index Block. Then, from the Meta-Index Block, we get the offset of the Filter Block. Finally, we read the content of the Filter Block based on its offset. With the Index Block and Filter Block, we can quickly and efficiently “follow the map” to find key-value pairs.</p><h3 id="The-Answer-SSTable-Structure-Diagram"><a href="#The-Answer-SSTable-Structure-Diagram" class="headerlink" title="The Answer: SSTable Structure Diagram"></a>The Answer: SSTable Structure Diagram</h3><p>Now that we’ve analyzed how data blocks are organized in an SSTable, here’s a simple ASCII diagram to describe the various blocks in an SSTable for better understanding:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">+-------------------+</span><br><span class="line">|   Data Block 0    |</span><br><span class="line">+-------------------+</span><br><span class="line">|   Data Block 1    |</span><br><span class="line">+-------------------+</span><br><span class="line">|       ...         |</span><br><span class="line">+-------------------+</span><br><span class="line">|   Data Block N    |</span><br><span class="line">+-------------------+</span><br><span class="line">|   Filter Block    |  (optional) &lt;-- Indexed by Meta-Index Block</span><br><span class="line">+-------------------+</span><br><span class="line">| Meta Index Block  | &lt;-- Indexed by Footer</span><br><span class="line">+-------------------+</span><br><span class="line">|   Index Block     | &lt;-- Indexed by Footer</span><br><span class="line">+-------------------+</span><br><span class="line">|      Footer       | (Fixed size at the end of the file)</span><br><span class="line">+-------------------+</span><br></pre></td></tr></table></figure><p>However, providing the interface and saving key-value pairs in the above format involves many engineering details. By the way, <strong><span style="color: red;">the layered abstraction in LevelDB’s code is really well done, with one layer wrapping another, making the complex logic easy to understand and maintain</span></strong>. For example, how each block builds its data is encapsulated in a separate implementation, which I will detail in other articles.</p><p>In this article, we’ll focus on the engineering details of the SSTable file construction process. This part is implemented in <a href="https://github.com/google/leveldb/blob/main/table/table_builder.cc">table&#x2F;table_builder.cc</a>, mainly in the TableBuilder class.</p><p>This class has only one private member variable, a Rep* pointer, which stores various state information, such as the current DataBlock, IndexBlock, etc. The <code>Rep*</code> uses the Pimpl design pattern. You can learn more about Pimpl in this series’ article <a href="https://selfboot.cn/en/2024/08/13/leveldb_source_unstand_c++/#Pimpl-Class-Design">LevelDB Explained - Understanding Advanced C++ Techniques</a>.</p><p>The most important interface of this class is Add, which in turn calls other encapsulated functions to complete the addition of key-value pairs. Let’s start with this interface to analyze the implementation of the TableBuilder class.</p><h2 id="Add-Adding-Key-Value-Pairs"><a href="#Add-Adding-Key-Value-Pairs" class="headerlink" title="Add: Adding Key-Value Pairs"></a>Add: Adding Key-Value Pairs</h2><p>The <a href="https://github.com/google/leveldb/blob/main/table/table_builder.cc#L94">TableBuilder::Add</a> method is the core function for adding key-value pairs to an SSTable file. Adding a key-value pair requires modifying the various blocks mentioned above, such as the DataBlock, IndexBlock, and FilterBlock. To improve efficiency, there are many engineering optimization details. To better understand it, I’ve divided it into four main parts, which I’ll discuss one by one.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">TableBuilder::Add</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">const</span> Slice&amp; value)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 1. Pre-checks</span></span><br><span class="line">  <span class="comment">// 2. Handle index block</span></span><br><span class="line">  <span class="comment">// 3. Handle filter block</span></span><br><span class="line">  <span class="comment">// 4. Handle data block</span></span><br><span class="line">  <span class="comment">// 5. Flush to disk at the right time</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Pre-checks"><a href="#Pre-checks" class="headerlink" title="Pre-checks"></a>Pre-checks</h3><p>In the <code>Add</code> method, the first step is to read the data from <code>rep_</code> and perform some pre-checks, such as verifying that the file has not been closed and ensuring that the key-value pairs are in order.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Rep* r = rep_;</span><br><span class="line"><span class="built_in">assert</span>(!r-&gt;closed);</span><br><span class="line"><span class="keyword">if</span> (!<span class="built_in">ok</span>()) <span class="keyword">return</span>;</span><br><span class="line"><span class="keyword">if</span> (r-&gt;num_entries &gt; <span class="number">0</span>) &#123;</span><br><span class="line">  <span class="built_in">assert</span>(r-&gt;options.comparator-&gt;<span class="built_in">Compare</span>(key, <span class="built_in">Slice</span>(r-&gt;last_key)) &gt; <span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>LevelDB includes <strong><span style="color: red;">a lot of validation logic in its code to ensure that if there’s a problem, it fails early</span></strong>, a philosophy that is essential for a low-level library. The assert in the Add method here checks that subsequently inserted keys are always greater, which of course needs to be guaranteed by the caller. To implement this check, each TableBuilder’s Rep stores a last_key to record the last inserted key. This key is also used in index key optimization, which will be detailed later.</p><h3 id="Handling-Index-Records"><a href="#Handling-Index-Records" class="headerlink" title="Handling Index Records"></a>Handling Index Records</h3><p>Next, it adds a <strong>new index record at the appropriate time</strong>. We know that index records are used to quickly find the offset of the DataBlock where a key is located. Each complete DataBlock corresponds to one index record. Let’s first look at the <strong>timing of adding an index record</strong>. When a DataBlock is finished, <code>pending_index_entry</code> is set to true. Then, when the first key of the next new DataBlock is added, the index record for the previously completed DataBlock is updated.</p><p>Here is the core code for this part:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (r-&gt;pending_index_entry) &#123;</span><br><span class="line">  <span class="built_in">assert</span>(r-&gt;data_block.<span class="built_in">empty</span>());</span><br><span class="line">  r-&gt;options.comparator-&gt;<span class="built_in">FindShortestSeparator</span>(&amp;r-&gt;last_key, key);</span><br><span class="line">  std::string handle_encoding;</span><br><span class="line">  r-&gt;pending_handle.<span class="built_in">EncodeTo</span>(&amp;handle_encoding);</span><br><span class="line">  r-&gt;index_block.<span class="built_in">Add</span>(r-&gt;last_key, <span class="built_in">Slice</span>(handle_encoding));</span><br><span class="line">  r-&gt;pending_index_entry = <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The reason for waiting until the first key of a new DataBlock is added to update the index block is <strong><span style="color: red;">to minimize the length of the index key as much as possible, thereby reducing the size of the index block</span></strong>. This is another engineering optimization detail in LevelDB.</p><p>Let me expand on the background to make it easier to understand. Each index record in an SSTable consists of a separator key and a BlockHandle (offset + size) pointing to a data block. The purpose of this separator key is to partition the key space of different DataBlocks. For the N-th data block (Block N), its index key separator_key_N must satisfy the following conditions:</p><ul><li>separator_key_N &gt;&#x3D; any key in Block N</li><li>separator_key_N &lt; any key in Block N+1</li></ul><p>This way, when searching for a target key, if you find the first entry where separator_key_N &gt; target_key in the index block, then the target_key, if it exists, must be in the previous data block (Block N-1).</p><p>Intuitively, the simplest implementation of the index is to directly use the last key of Block N (last_key_N) as separator_key_N. But the problem is that last_key_N itself can be very long. This leads to long index entries, which in turn makes the entire index block large. <strong>The index block is usually loaded into memory, so the smaller the index block, the less memory it occupies, the higher the cache efficiency, and the faster the lookup speed.</strong></p><p>If we think about it, we don’t actually need a real key as the separating index key; we just need a “separator” that can separate the two blocks. This key only needs to satisfy last_key_N &lt;&#x3D; separator_key &lt; first_key_N+1. LevelDB does just that. It calls options.comparator-&gt;FindShortestSeparator to <strong>find the shortest separator string between the last key of the previous block and the first key of the next block</strong>. The default implementation of FindShortestSeparator is in <a href="https://github.com/google/leveldb/blob/main/util/comparator.cc#L31C8-L31C29">util&#x2F;comparator.cc</a>, which I won’t list here.</p><p>To better understand this optimization process, here’s a concrete example:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250620_leveldb_source_table_process_indexkey_en.webp" alt="SSTable DataBlock index separator key optimization"></p><p>Finally, let’s talk about the value of each index record here. It is the <strong>offset and size of the block within the file</strong>, which is recorded by pending_handle. When a DataBlock is written to the file via WriteRawBlock, the offset and size of pending_handle are updated. Then, when writing the index, EncodeTo is used to encode the offset and size into a string, which is inserted into the IndexBlock along with the preceding index key.</p><h3 id="Handling-Filter-Records"><a href="#Handling-Filter-Records" class="headerlink" title="Handling Filter Records"></a>Handling Filter Records</h3><p>Next is handling the FilterBlock. The index block we just discussed can only find the <strong>location of the block where the key should be</strong>. We still need to read the block’s content to know if the key actually exists. To quickly determine if a key is present, LevelDB supports a filter index block, which <strong>can quickly determine if a key exists in the current SSTable</strong>. If the filter index block is enabled, the index is added synchronously when a key is added. The core code is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (r-&gt;filter_block != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">  r-&gt;filter_block-&gt;<span class="built_in">AddKey</span>(key);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>After adding the key here, the index is only stored in memory. The FilterBlock is not written to the file until after TableBuilder has finished writing all the blocks. <strong>The FilterBlock itself is optional</strong>, and is set via options.filter_policy. When initializing TableBuilder::Rep, the FilterBlockBuilder pointer is initialized based on options.filter_policy, as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Rep</span>(<span class="type">const</span> Options&amp; opt, WritableFile* f)</span><br><span class="line">    : <span class="built_in">options</span>(opt),</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">      <span class="built_in">filter_block</span>(opt.filter_policy == <span class="literal">nullptr</span></span><br><span class="line">                       ? <span class="literal">nullptr</span></span><br><span class="line">                       : <span class="keyword">new</span> <span class="built_in">FilterBlockBuilder</span>(opt.filter_policy)),</span><br><span class="line">      <span class="built_in">pending_index_entry</span>(<span class="literal">false</span>) &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>It’s worth noting here that filter_block is a pointer mainly because, in addition to the default Bloom filter, you can also <strong>use your own filter through polymorphism</strong>. The object is created on the heap using new. To <strong>prevent memory leaks</strong>, the filter_block is deleted first in the TableBuilder destructor, followed by rep_.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">TableBuilder::~<span class="built_in">TableBuilder</span>() &#123;</span><br><span class="line">  <span class="built_in">assert</span>(rep_-&gt;closed);  <span class="comment">// Catch errors where caller forgot to call Finish()</span></span><br><span class="line">  <span class="keyword">delete</span> rep_-&gt;filter_block;</span><br><span class="line">  <span class="keyword">delete</span> rep_;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The reason rep_ needs to be deleted is because it was created on the heap in the TableBuilder constructor, as shown below:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TableBuilder::<span class="built_in">TableBuilder</span>(<span class="type">const</span> Options&amp; options, WritableFile* file)</span><br><span class="line">    : <span class="built_in">rep_</span>(<span class="keyword">new</span> <span class="built_in">Rep</span>(options, file)) &#123;</span><br><span class="line">  <span class="keyword">if</span> (rep_-&gt;filter_block != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">    rep_-&gt;filter_block-&gt;<span class="built_in">StartBlock</span>(<span class="number">0</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>For the implementation of LevelDB’s default Bloom filter, you can refer to <a href="/leveldb_source_filterblock">LevelDB Source Code Walkthrough: Bloom Filter Implementation</a>. I will write a separate article to detail the construction of the index block, so we won’t go into the details here.</p><h3 id="Handling-Data-Blocks"><a href="#Handling-Data-Blocks" class="headerlink" title="Handling Data Blocks"></a>Handling Data Blocks</h3><p>Next, the key-value pairs need to be added to the DataBlock. The DataBlock is where the actual key-value pairs are stored in the SSTable file. The code is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">r-&gt;last_key.<span class="built_in">assign</span>(key.<span class="built_in">data</span>(), key.<span class="built_in">size</span>());</span><br><span class="line">r-&gt;num_entries++;</span><br><span class="line">r-&gt;data_block.<span class="built_in">Add</span>(key, value);</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">size_t</span> estimated_block_size = r-&gt;data_block.<span class="built_in">CurrentSizeEstimate</span>();</span><br><span class="line"><span class="keyword">if</span> (estimated_block_size &gt;= r-&gt;options.block_size) &#123;</span><br><span class="line">  <span class="built_in">Flush</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, the Add method in BlockBuilder is called to add the key-value pair to the DataBlock. The implementation of BlockBuilder will be described in a separate article later. Haha, <strong>LevelDB’s layered abstraction is so well done here that our articles have to be layered as well</strong>. After each key-value pair is added, it checks if the current DataBlock size has exceeded block_size. If it has, the Flush method is called to write the DataBlock to the disk file. The block_size is set in options, with a default of 4KB. This is the size before key-value compression. If compression is enabled, the actual size written to the file will be smaller than block_size.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Approximate size of user data packed per block.  Note that the</span></span><br><span class="line"><span class="comment">// block size specified here corresponds to uncompressed data.  The</span></span><br><span class="line"><span class="comment">// actual size of the unit read from disk may be smaller if</span></span><br><span class="line"><span class="comment">// compression is enabled.  This parameter can be changed dynamically.</span></span><br><span class="line"><span class="type">size_t</span> block_size = <span class="number">4</span> * <span class="number">1024</span>;</span><br></pre></td></tr></table></figure><p>So how does Flush write to disk? Let’s continue.</p><h3 id="Flush-Writing-Data-Blocks"><a href="#Flush-Writing-Data-Blocks" class="headerlink" title="Flush: Writing Data Blocks"></a>Flush: Writing Data Blocks</h3><p>In the previous Add method, if a block’s size reaches 4KB, the Flush method is called to write it to the disk file. The implementation of Flush is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">TableBuilder::Flush</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Rep* r = rep_;</span><br><span class="line">  <span class="built_in">assert</span>(!r-&gt;closed);</span><br><span class="line">  <span class="keyword">if</span> (!<span class="built_in">ok</span>()) <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">if</span> (r-&gt;data_block.<span class="built_in">empty</span>()) <span class="keyword">return</span>;</span><br><span class="line">  <span class="built_in">assert</span>(!r-&gt;pending_index_entry);</span><br><span class="line">  <span class="built_in">WriteBlock</span>(&amp;r-&gt;data_block, &amp;r-&gt;pending_handle);</span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">ok</span>()) &#123;</span><br><span class="line">    r-&gt;pending_index_entry = <span class="literal">true</span>;</span><br><span class="line">    r-&gt;status = r-&gt;file-&gt;<span class="built_in">Flush</span>();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (r-&gt;filter_block != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">    r-&gt;filter_block-&gt;<span class="built_in">StartBlock</span>(r-&gt;offset);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The beginning part is just some pre-checks. Note that Flush is only for flushing the DataBlock part. If data_block is empty, it returns directly. Then it calls the WriteBlock method (detailed later) to write the DataBlock to the file, and then <strong>updates pending_index_entry to true, indicating that the index block needs to be updated when the next key is added</strong>.</p><p>Finally, it calls file-&gt;Flush() to have the system write the current in-memory data to disk. Note that this doesn’t guarantee that the data has been synchronized to the physical disk. The data might still be in the system cache, and if the operating system crashes, the unwritten data could be lost. For more details on file operations and flushing to disk, you can refer to this series’ article <a href="/2024/08/02/leveldb_source_env_posixfile/">LevelDB Source Code Walkthrough: Posix File Operation Interface Implementation Details</a>. If there is a filter_block, the StartBlock method also needs to be called. This method is also quite interesting and we will discuss it in detail when we specifically write about filter blocks.</p><h2 id="WriteBlock-Writing-to-a-File"><a href="#WriteBlock-Writing-to-a-File" class="headerlink" title="WriteBlock: Writing to a File"></a>WriteBlock: Writing to a File</h2><p>As mentioned above, Flush calls the WriteBlock method to write the DataBlock to the file. This method is also called by Finish, which we’ll discuss later, to write the index block, filter block, etc., at the end. The implementation of WriteBlock is relatively simple. It mainly handles the compression logic and then calls the actual file writing function WriteRawBlock to write the block content to the file.</p><p>Compression is not mandatory. If compression is enabled when calling LevelDB and the compression library is linked, the corresponding compression algorithm will be chosen to compress the Block. LevelDB also strikes a <strong>balance between compression performance and effectiveness. If the compression ratio is less than or equal to 0.85, the compressed data will be written to the file; otherwise, the original data will be written directly</strong>. The actual file writing part calls the WriteRawBlock method, with the main code as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">TableBuilder::WriteRawBlock</span><span class="params">(<span class="type">const</span> Slice&amp; block_contents,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 CompressionType type, BlockHandle* handle)</span> </span>&#123;</span><br><span class="line">  Rep* r = rep_;</span><br><span class="line">  handle-&gt;<span class="built_in">set_offset</span>(r-&gt;offset);</span><br><span class="line">  handle-&gt;<span class="built_in">set_size</span>(block_contents.<span class="built_in">size</span>());</span><br><span class="line">  r-&gt;status = r-&gt;file-&gt;<span class="built_in">Append</span>(block_contents);</span><br><span class="line">  <span class="keyword">if</span> (r-&gt;status.<span class="built_in">ok</span>()) &#123;</span><br><span class="line">    <span class="type">char</span> trailer[kBlockTrailerSize];</span><br><span class="line">    trailer[<span class="number">0</span>] = type;</span><br><span class="line">    <span class="type">uint32_t</span> crc = crc32c::<span class="built_in">Value</span>(block_contents.<span class="built_in">data</span>(), block_contents.<span class="built_in">size</span>());</span><br><span class="line">    crc = crc32c::<span class="built_in">Extend</span>(crc, trailer, <span class="number">1</span>);  <span class="comment">// Extend crc to cover block type</span></span><br><span class="line">    <span class="built_in">EncodeFixed32</span>(trailer + <span class="number">1</span>, crc32c::<span class="built_in">Mask</span>(crc));</span><br><span class="line">    r-&gt;status = r-&gt;file-&gt;<span class="built_in">Append</span>(<span class="built_in">Slice</span>(trailer, kBlockTrailerSize));</span><br><span class="line">    <span class="keyword">if</span> (r-&gt;status.<span class="built_in">ok</span>()) &#123;</span><br><span class="line">      r-&gt;offset += block_contents.<span class="built_in">size</span>() + kBlockTrailerSize;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, a 5-byte trailer is placed at the end of each block to verify data integrity. The first byte is the compression type; currently, the supported compression algorithms are Snappy and Zstd. The next 4 bytes are the CRC32 checksum. <code>crc32c::Value</code> is used to calculate the checksum of the data block, and then the compression type is also included in the checksum calculation. For more details on the CRC32 part, you can refer to this series’ article <a href="/2024/08/29/leveldb_source_utils/">LevelDB Source Code Walkthrough: Memory Allocator, Random Number Generator, CRC32, Integer Encoding&#x2F;Decoding</a>.</p><h2 id="Finish-Actively-Triggering-Disk-Flush"><a href="#Finish-Actively-Triggering-Disk-Flush" class="headerlink" title="Finish: Actively Triggering Disk Flush"></a>Finish: Actively Triggering Disk Flush</h2><p>All the operations above are mainly for continuously adding key-value pairs to the data block. If the DataBlock’s size limit is reached during this process, a flush of the DataBlock to disk is triggered. But the entire SSTable file also has an index block, a filter block, etc., which <strong>need to be actively triggered to be flushed to disk</strong>. So at what point is this triggered, and how is it flushed?</p><p>There are many occasions when an SSTable file is generated in LevelDB. Let’s take the flush triggered when saving an immutable MemTable as an example. When saving an immutable MemTable as an SSTable file, the process is as follows: first, iterate through the key-value pairs in the immutable MemTable, then call the Add method above to add them. The Add method will update the content of the relevant blocks. Whenever a DataBlock exceeds block_size, the Flush method is called to write the DataBlock to the file.</p><p>After all key-value pairs have been written, the Finish method is actively called to perform some <strong>finishing touches</strong>, such as writing the data of the last DataBlock to the file, and writing the IndexBlock, FilterBlock, etc.</p><p>The implementation of Finish is as follows. Before it begins, it first uses Flush to write the remaining DataBlock part to the disk file. Then it processes the other blocks and adds a fixed-size footer at the end of the file to record index information.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">TableBuilder::Finish</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Rep* r = rep_;</span><br><span class="line">  <span class="built_in">Flush</span>();</span><br><span class="line">  <span class="built_in">assert</span>(!r-&gt;closed);</span><br><span class="line">  r-&gt;closed = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Write filter block</span></span><br><span class="line">  <span class="comment">// Write metaindex block</span></span><br><span class="line">  <span class="comment">// Write index block</span></span><br><span class="line">  <span class="comment">// Write footer</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> r-&gt;status;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The way the various blocks are constructed here is also quite interesting. A builder is used to handle the content, while a handler is used to record the block’s offset and size. Let’s look at them separately.</p><h3 id="BlockBuilder-for-Block-Construction"><a href="#BlockBuilder-for-Block-Construction" class="headerlink" title="BlockBuilder for Block Construction"></a>BlockBuilder for Block Construction</h3><p>First, let’s consider a question: <strong>with so many types of blocks, does each block need its own Builder to assemble the data?</strong></p><p>To answer this, we need to look at the data structure of each block. The Data, Index, and MetaIndex Blocks all share the following common features:</p><ul><li><strong>Key-value structure</strong>: They all store data in a key-value format. Although the meaning of the keys and values in each block is different, they are all in a key-value format.</li><li><strong>Order requirement</strong>: The keys must be sorted because binary search or sequential scanning is required for lookups.</li></ul><p>Therefore, the construction logic for these three types of blocks is similar, and LevelDB uses the same BlockBuilder to handle them. The implementation is in <a href="https://github.com/google/leveldb/blob/main/table/block_builder.h">table&#x2F;block_builder.h</a>, and it also has many optimization details. For example, prefix compression optimization saves space by storing only the different parts of similar keys. The restart point mechanism sets a restart point every few entries to support binary search. I will write a separate article to detail this later. After encapsulation, it’s quite simple to use. Taking the MetaIndex Block as an example, you use Add to add key-values and then WriteBlock to flush to disk. The code is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">TableBuilder::Finish</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">BlockBuilder <span class="title">meta_index_block</span><span class="params">(&amp;r-&gt;options)</span></span>;</span><br><span class="line">    <span class="keyword">if</span> (r-&gt;filter_block != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">      <span class="comment">// Add mapping from &quot;filter.Name&quot; to location of filter data</span></span><br><span class="line">      std::string key = <span class="string">&quot;filter.&quot;</span>;</span><br><span class="line">      key.<span class="built_in">append</span>(r-&gt;options.filter_policy-&gt;<span class="built_in">Name</span>());</span><br><span class="line">      std::string handle_encoding;</span><br><span class="line">      filter_block_handle.<span class="built_in">EncodeTo</span>(&amp;handle_encoding);</span><br><span class="line">      meta_index_block.<span class="built_in">Add</span>(key, handle_encoding);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO(postrelease): Add stats and other meta blocks</span></span><br><span class="line">    <span class="built_in">WriteBlock</span>(&amp;meta_index_block, &amp;metaindex_block_handle);</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>On the other hand, the data structure of the filter block is different from the others. It stores the binary data of the Bloom filter, grouped by file offset, with one filter for every 2KB file range. Therefore, the construction logic of the filter block is different from the others and needs to be handled separately. The implementation is in <a href="https://github.com/google/leveldb/blob/main/table/filter_block.cc">table&#x2F;filter_block.cc</a>, which I will analyze separately later. Its usage is quite simple, as shown below:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Write filter block</span></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">ok</span>() &amp;&amp; r-&gt;filter_block != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">  <span class="built_in">WriteRawBlock</span>(r-&gt;filter_block-&gt;<span class="built_in">Finish</span>(), kNoCompression,</span><br><span class="line">                &amp;filter_block_handle);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, the Finish method returns the binary data of the filter block, and then the WriteRawBlock method is called to write the data to the file.</p><h3 id="BlockHandle-for-Recording-Offset-and-Size"><a href="#BlockHandle-for-Recording-Offset-and-Size" class="headerlink" title="BlockHandle for Recording Offset and Size"></a>BlockHandle for Recording Offset and Size</h3><p>The two builders above are used to construct the blocks, but the same handler class is used to record the offset and size of the blocks. The code is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BlockHandle filter_block_handle, metaindex_block_handle, index_block_handle;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The implementation of BlockHandle is in <a href="https://github.com/google/leveldb/blob/main/table/format.h#L23">table&#x2F;format.h</a>. It mainly tells the system that there is a block of size Y bytes at position X bytes in the file, and that’s it. However, in conjunction with the handle information of different blocks, it can conveniently store the offset and size of different blocks.</p><p>At this point, we have used two builders to construct various index blocks, and at the same time, used one handler to assist in recording the offset and size of the blocks. This completes the construction of the entire block.</p><h2 id="Complete-Steps-for-Creating-an-SSTable-File"><a href="#Complete-Steps-for-Creating-an-SSTable-File" class="headerlink" title="Complete Steps for Creating an SSTable File"></a>Complete Steps for Creating an SSTable File</h2><p>Finally, let’s see how the upper-level caller uses TableBuilder to construct an SSTable file.</p><p>A function BuildTable is encapsulated in <a href="https://github.com/google/leveldb/blob/main/db/builder.cc#L17">db&#x2F;builder.cc</a> to create an SSTable file, which is implemented by calling the interface of the TableBuilder class. Omitting other irrelevant code, the core code is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">BuildTable</span><span class="params">(<span class="type">const</span> std::string&amp; dbname, Env* env, <span class="type">const</span> Options&amp; options,</span></span></span><br><span class="line"><span class="params"><span class="function">                  TableCache* table_cache, Iterator* iter, FileMetaData* meta)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    TableBuilder* builder = <span class="keyword">new</span> <span class="built_in">TableBuilder</span>(options, file);</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    Slice key;</span><br><span class="line">    <span class="keyword">for</span> (; iter-&gt;<span class="built_in">Valid</span>(); iter-&gt;<span class="built_in">Next</span>()) &#123;</span><br><span class="line">      key = iter-&gt;<span class="built_in">key</span>();</span><br><span class="line">      builder-&gt;<span class="built_in">Add</span>(key, iter-&gt;<span class="built_in">value</span>());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// Finish and check for builder errors</span></span><br><span class="line">    s = builder-&gt;<span class="built_in">Finish</span>();</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">delete</span> builder;</span><br><span class="line">    <span class="comment">//..</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, an iterator iter is used to traverse the key-value pairs in the immutable MemTable, and then the Add method of TableBuilder is called to add the key-value pairs to the SSTable file. The size limit of the MemTable is 4MB by default (write_buffer_size &#x3D; 4<em>1024</em>1024). When adding key-value pairs with TableBuilder, the data is divided into data blocks according to block_size (4*1024). Whenever a DataBlock is filled, the data of the corresponding block is assembled, and then appended to the SSTable file on disk using flush. Finally, the Finish method of TableBuilder is called to write other blocks and complete the writing of the entire SSTable file.</p><p>Besides BuildTable writing data from an immutable MemTable to a level-0 SSTable file, there is another scenario during the Compact process, where multiple SSTable files are merged into a single SSTable file. This process is implemented in the DoCompactionWork function in <a href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L897">db&#x2F;db_impl.cc</a>. The overall flow is slightly more complex and the calls are deeper. We will analyze it in detail when we talk about Compact later.</p><p>However, let’s just mention one point here. During the Compact process, the Abandon method of TableBuilder is called in some failure scenarios to abandon the current TableBuilder writing process.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">compact-&gt;builder-&gt;<span class="built_in">Abandon</span>();</span><br></pre></td></tr></table></figure><p>Abandon mainly sets the closed flag in the TableBuilder’s Rep to true. The caller will then discard this TableBuilder instance and will not use it for any write operations (there are a bunch of assertions to check this state during writing).</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Returning to the question we raised at the beginning, how should a file format be designed to balance efficient writes and fast reads? Through a deep analysis of LevelDB’s SSTable file creation process, we can see how the author solved this problem step by step. First, the SSTable data format design has several important design principles:</p><ol><li><strong>Block-based storage</strong>: Splitting large files into 4KB DataBlocks makes them easier to manage, reduces unnecessary disk I&#x2F;O, and facilitates caching of hot data.</li><li><strong>Index acceleration</strong>: Using an IndexBlock turns a “full scan” into a “directory lookup + precise read,” reducing the number of disk I&#x2F;O operations.</li><li><strong>Filter optimization</strong>: Using a FilterBlock at the source reduces unnecessary disk reads, improving read performance.</li><li><strong>Centralized metadata management</strong>: The design of the Footer + Meta-IndexBlock ensures extensibility, making it easy to add more metadata blocks in the future.</li></ol><p>In the implementation of TableBuilder, we also saw many engineering details worth learning from, such as:</p><ul><li><strong>Index key optimization</strong>: Delaying the update of the index until the next block starts, and generating the shortest separator key using the FindShortestSeparator algorithm, significantly reduces the size of the index block. This optimization may seem minor, but its effect is significant with large-scale data.</li><li><strong>Error handling</strong>: The large number of assert statements in the code reflects the “fail fast, fail early” philosophy, which is crucial for a low-level storage system.</li><li><strong>Layered abstraction</strong>: The layered design of TableBuilder → BlockBuilder → FilterBlockBuilder makes the construction of a complex file format organized and orderly. Each layer has a clear boundary of responsibility.</li><li><strong>Performance balance</strong>: The 0.85 compression ratio threshold in the compression strategy reflects a trade-off between performance and effectiveness.</li></ul><p>In fact, the design of SSTable answers several fundamental questions in storage systems. It uses sequential writes to ensure write throughput and an index structure to ensure read performance. It uses block-based storage, on-demand loading, and caching to handle massive amounts of data with limited memory. At the same time, it uses compression and filters to balance storage space and query efficiency, and layered metadata to ensure system extensibility. These are all classic designs that have been refined over many years in computer software systems and are worth learning from.</p><p>After understanding the creation process of an SSTable, you may have some new questions: How is data organized within a DataBlock? What is the process for reading an SSTable? How do multiple SSTable files work together?</p><p>The answers to these questions form the complete picture of the ingenious storage engine that is LevelDB. I will continue to analyze them in depth in future articles, so stay tuned.</p>]]></content:encoded>
      
      
      <category domain="https://selfboot.cn/categories/Source-Code-Analysis/">Source Code Analysis</category>
      
      
      <category domain="https://selfboot.cn/tags/C/">C++</category>
      
      <category domain="https://selfboot.cn/tags/LevelDB/">LevelDB</category>
      
      
      <comments>https://selfboot.cn/en/2025/06/27/leveldb_source_table_build/#disqus_thread</comments>
      
    </item>
    
    
    
    
    
    <item>
      <title>LevelDB Explained - The Implementation Details of a High-Performance LRU Cache</title>
      <link>https://selfboot.cn/en/2025/06/13/leveldb_source_LRU_cache/</link>
      <guid>https://selfboot.cn/en/2025/06/13/leveldb_source_LRU_cache/</guid>
      <pubDate>Fri, 13 Jun 2025 21:00:00 GMT</pubDate>
      
      <description>This article delves into the implementation details of the high-performance LRU cache in LevelDB, covering its cache interface design, the LRUHandle data structure, doubly linked list optimizations, and sharding mechanism. By analyzing core designs like clever reference counting management, the dummy node technique, and lock sharding to reduce contention, it showcases the optimization strategies for an industrial-grade cache system. Combining code and diagrams, the article helps readers understand how LevelDB achieves a high-concurrency, high-performance cache and how these design techniques can be applied in their own projects.</description>
      
      
      
      <content:encoded><![CDATA[<p>In computer systems, caches are ubiquitous. From CPU caches to memory caches, from disk caches to network caches, they are everywhere. The core idea of a cache is to trade space for time by storing frequently accessed “hot” data in high-performance storage to improve performance. Since caching devices are expensive, their storage size is limited, which means some cached data must be evicted. The eviction policy is crucial here; an unreasonable policy might evict data that is about to be accessed, leading to a very low cache hit rate.</p><p>There are many cache eviction policies, such as LRU, LFU, and FIFO. Among them, LRU (Least Recently Used) is a classic strategy. Its core idea is: <strong>when the cache is full, evict the least recently used data</strong>. This is based on the empirical assumption that “<strong>if data has been accessed recently, it is more likely to be accessed again in the future</strong>.” As long as this assumption holds, LRU can significantly improve the cache hit rate.</p><p>LevelDB implements an in-memory LRU Cache to store hot data, enhancing read and write performance. By default, LevelDB caches sstable indexes and data blocks. The sstable cache is configured to hold 990 (1000-10) entries, while the data block cache is allocated 8MB by default.</p><p>The LRU cache implemented in LevelDB is a sharded LRU with many detailed optimizations, making it an excellent case study. This article will start with the classic LRU implementation and then progressively analyze the implementation details of LevelDB’s <a href="https://github.com/google/leveldb/blob/main/util/cache.cc">LRU Cache</a>.</p><h2 id="Classic-LRU-Implementation"><a href="#Classic-LRU-Implementation" class="headerlink" title="Classic LRU Implementation"></a>Classic LRU Implementation</h2><p>A well-implemented LRU needs to support insertion, lookup, and deletion operations in $O(1)$ time complexity. The classic approach uses <strong>a doubly linked list and a hash table</strong>:</p><ul><li><strong>The doubly linked list</strong> stores the cache entries and maintains their usage order. Recently accessed items are moved to the head of the list, while the least recently used items gradually move towards the tail. When the cache reaches its capacity and needs to evict data, the item at the tail of the list (the least recently used item) is removed.</li><li><strong>The hash table</strong> stores the mapping from keys to their corresponding nodes in the doubly linked list, allowing any data item to be accessed and located in constant time. The hash table’s keys are the data item keys, and the values are pointers to the corresponding nodes in the doubly linked list.</li></ul><p><strong>The doubly linked list ensures constant-time node addition and removal, while the hash table provides constant-time data access</strong>. For a get operation, the hash table quickly locates the node in the list. If it exists, it’s moved to the head of the list, marking it as recently used. For an insert operation, if the data already exists, its value is updated, and the node is moved to the head. If it doesn’t exist, a new node is inserted at the head, a mapping is added to the hash table, and if the capacity is exceeded, the tail node is removed from the list and its mapping is deleted from the hash table.</p><p>This implementation approach is familiar to anyone who has studied algorithms. There are LRU implementation problems on LeetCode, such as <a href="https://leetcode.com/problems/lru-cache/">146. LRU Cache</a>, which requires implementing the following interface:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LRUCache</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">LRUCache</span>(<span class="type">int</span> capacity) &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> key)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">put</span><span class="params">(<span class="type">int</span> key, <span class="type">int</span> value)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>However, implementing an industrial-grade, high-performance LRU cache is still quite challenging. Next, let’s see how LevelDB does it.</p><h2 id="Cache-Design-Dependency-Inversion"><a href="#Cache-Design-Dependency-Inversion" class="headerlink" title="Cache Design: Dependency Inversion"></a>Cache Design: Dependency Inversion</h2><p>Before diving into LevelDB’s LRU Cache implementation, let’s look at how the cache is used. For example, in <a href="https://github.com/google/leveldb/blob/main/db/table_cache.cc">db&#x2F;table_cache.cc</a>, to cache SSTable metadata, the TableCache class defines a member variable of type Cache and uses it to perform various cache operations.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Cache* <span class="title">cache_</span><span class="params">(NewLRUCache(entries))</span></span>;</span><br><span class="line"></span><br><span class="line">*handle = cache_-&gt;<span class="built_in">Lookup</span>(key);</span><br><span class="line">*handle = cache_-&gt;<span class="built_in">Insert</span>(key, tf, <span class="number">1</span>, &amp;DeleteEntry);</span><br><span class="line">cache_-&gt;<span class="built_in">Release</span>(handle);</span><br><span class="line">cache_-&gt;<span class="built_in">Erase</span>(<span class="built_in">Slice</span>(buf, <span class="built_in">sizeof</span>(buf)));</span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure><p>Here, Cache is an abstract class that defines the various interfaces for cache operations, as defined in <a href="https://github.com/google/leveldb/blob/main/include/leveldb/cache.h">include&#x2F;leveldb&#x2F;cache.h</a>. It specifies basic operations like Insert, Lookup, Release, and Erase. It also defines the Cache::Handle type to represent a cache entry. User code interacts only with this abstract interface, without needing to know the concrete implementation.</p><p>Then there is the LRUCache class, which is the concrete implementation of a complete LRU cache. This class is not directly exposed to the outside world, nor does it directly inherit from Cache. There is also a ShardedLRUCache class that inherits from Cache and implements the cache interfaces. It contains 16 LRUCache “shards,” each responsible for caching a portion of the data.</p><p>This design allows callers to <strong>easily swap out different cache implementations without modifying the code that uses the cache</strong>. Ha, isn’t this the classic <strong>Dependency Inversion Principle from SOLID object-oriented programming</strong>? The application layer depends on an abstract interface (Cache) rather than a concrete implementation (LRUCache). This reduces code coupling and improves the system’s extensibility and maintainability.</p><p>When using the cache, a factory function is used to create the concrete cache implementation, <a href="https://github.com/google/leveldb/blob/main/util/cache.cc#L339">ShardedLRUCache</a>:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Cache* <span class="title">NewLRUCache</span><span class="params">(<span class="type">size_t</span> capacity)</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">ShardedLRUCache</span>(capacity); &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// New cache implementations can be added at any time.</span></span><br><span class="line"><span class="comment">// Cache* NewClockCache(size_t capacity);</span></span><br></pre></td></tr></table></figure><p>LRUCache is the core part of the cache, but for now, let’s put aside its implementation and first look at the <strong>design of the cache entry, Handle</strong>.</p><h2 id="LRUHandle-Class-Implementation"><a href="#LRUHandle-Class-Implementation" class="headerlink" title="LRUHandle Class Implementation"></a>LRUHandle Class Implementation</h2><p>In LevelDB, a cached data item is an LRUHandle class, defined in <a href="https://github.com/google/leveldb/blob/main/util/cache.cc#L43">util&#x2F;cache.cc</a>. The comments here explain that LRUHandle is a heap-allocated, variable-length structure that is stored in a doubly-linked list, ordered by access time. Let’s look at the members of this struct:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">LRUHandle</span> &#123;</span><br><span class="line">  <span class="type">void</span>* value;</span><br><span class="line">  <span class="built_in">void</span> (*deleter)(<span class="type">const</span> Slice&amp;, <span class="type">void</span>* value);</span><br><span class="line">  LRUHandle* next_hash;</span><br><span class="line">  LRUHandle* next;</span><br><span class="line">  LRUHandle* prev;</span><br><span class="line">  <span class="type">size_t</span> charge;  <span class="comment">// TODO(opt): Only allow uint32_t?</span></span><br><span class="line">  <span class="type">size_t</span> key_length;</span><br><span class="line">  <span class="type">bool</span> in_cache;      <span class="comment">// Whether entry is in the cache.</span></span><br><span class="line">  <span class="type">uint32_t</span> refs;      <span class="comment">// References, including cache reference, if present.</span></span><br><span class="line">  <span class="type">uint32_t</span> hash;      <span class="comment">// Hash of key(); used for fast sharding and comparisons</span></span><br><span class="line">  <span class="type">char</span> key_data[<span class="number">1</span>];   <span class="comment">// Beginning of key</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This is a bit complex, and each field is quite important. Let’s go through them one by one.</p><ul><li><strong>value</strong>: Stores the actual value of the cache entry. It’s a <code>void*</code> pointer, meaning the cache layer is agnostic to the value’s structure; it only needs the object’s address.</li><li><strong>deleter</strong>: A function pointer to a callback used to delete the cached value. When a cache entry is removed, this is used to free the memory of the cached value.</li><li><strong>next_hash</strong>: The LRU cache implementation requires a hash table. LevelDB implements its own high-performance hash table. As we discussed in <a href="/en/2024/12/25/leveldb_source_hashtable/">LevelDB Explained - How to Design a High-Performance HashTable</a>, the next_hash field of LRUHandle is used to resolve hash collisions.</li><li><strong>prev&#x2F;next</strong>: Pointers to the previous&#x2F;next node in the doubly linked list, used to maintain the list for fast insertion and deletion of nodes.</li><li><strong>charge</strong>: Represents the cost of this cache entry (usually its memory size), used to calculate the total cache usage and determine if eviction is necessary.</li><li><strong>key_length</strong>: The length of the key, used to construct a Slice object representing the key.</li><li><strong>in_cache</strong>: A flag indicating whether the entry is in the cache. If true, it means the cache holds a reference to this entry.</li><li><strong>refs</strong>: A reference count, including the cache’s own reference (if in the cache) and references from users. When the count drops to 0, the entry can be deallocated.</li><li><strong>hash</strong>: The hash value of the key, used for fast lookups and sharding. Storing the hash value here avoids re-computation for the same key.</li><li><strong>key_data</strong>: A flexible array member that stores the actual key data. malloc is used to allocate enough space to store the entire key. We also discussed flexible array members in <a href="/en/2024/08/13/leveldb_source_unstand_c++/">LevelDB Explained - Understanding Advanced C++ Techniques</a>.</li></ul><p>The design of LRUHandle allows the cache to efficiently manage data items, track their references, and implement the LRU eviction policy. In particular, the combination of the in_cache and refs fields allows us to distinguish between items that are “<strong>cached but not referenced by clients</strong>“ and those that are “<strong>referenced by clients</strong>,” enabling an efficient eviction strategy using two linked lists.</p><p>Next, we’ll examine the implementation details of the LRUCache class to better understand the purpose of these fields.</p><h2 id="LRUCache-Class-Implementation-Details"><a href="#LRUCache-Class-Implementation-Details" class="headerlink" title="LRUCache Class Implementation Details"></a>LRUCache Class Implementation Details</h2><p>Having looked at the design of the cache entry, we can now examine the concrete implementation details of LevelDB’s LRUCache. The core caching logic is implemented in the LRUCache class in <a href="https://github.com/google/leveldb/blob/main/util/cache.cc#L151">util&#x2F;cache.cc</a>. This class contains the core logic for operations like insertion, lookup, deletion, and eviction.</p><p>The comments here (LevelDB’s comments are always worth reading carefully) mention that it uses two doubly linked lists to maintain cache items. Why is that?</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The cache keeps two linked lists of items in the cache.  All items in the</span></span><br><span class="line"><span class="comment">// cache are in one list or the other, and never both.  Items still referenced</span></span><br><span class="line"><span class="comment">// by clients but erased from the cache are in neither list.  The lists are:</span></span><br><span class="line"><span class="comment">// - in-use:  contains the items currently referenced by clients, in no</span></span><br><span class="line"><span class="comment">//   particular order.  (This list is used for invariant checking.  If we</span></span><br><span class="line"><span class="comment">//   removed the check, elements that would otherwise be on this list could be</span></span><br><span class="line"><span class="comment">//   left as disconnected singleton lists.)</span></span><br><span class="line"><span class="comment">// - LRU:  contains the items not currently referenced by clients, in LRU order</span></span><br><span class="line"><span class="comment">// Elements are moved between these lists by the Ref() and Unref() methods,</span></span><br><span class="line"><span class="comment">// when they detect an element in the cache acquiring or losing its only</span></span><br><span class="line"><span class="comment">// external reference.</span></span><br></pre></td></tr></table></figure><h3 id="Why-Use-Two-Doubly-Linked-Lists"><a href="#Why-Use-Two-Doubly-Linked-Lists" class="headerlink" title="Why Use Two Doubly Linked Lists?"></a>Why Use Two Doubly Linked Lists?</h3><p>As mentioned earlier, a typical LRU Cache implementation uses a single doubly linked list. Each time a cache item is used, it’s moved to the head of the list, making the tail the least recently used item. For eviction, the node at the tail is simply removed. The LeetCode problem mentioned earlier can be solved this way, where each cache item is just an int, and its value is copied on access. <strong>If the cached items are simple value types that can be copied directly on read without needing references, a single linked list is sufficient</strong>.</p><p>However, in LevelDB, the cached data items are LRUHandle objects, which are dynamically allocated, variable-length structures. For high concurrency and performance, they cannot be managed by simple value copying but must be managed through reference counting. If we were to use a single linked list, consider this scenario.</p><p>We access items A, C, and D in order, and finally access B. Item B is referenced by a client (refs &gt; 1) and is at the head of the list, as shown in the initial state in the diagram below. Over time, A, C, and D are accessed, but B is not. According to the LRU rule, A, C, and D are moved to the head. <strong>Although B is still referenced, its relative position moves towards the tail because it hasn’t been accessed for a long time</strong>. After A and D are accessed and quickly released, they have no external references. When eviction is needed, we start from the tail and find that item B (refs &gt; 1) cannot be evicted. We would have to skip it and continue traversing the list to find an evictable item.</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250612_leveldb_source_lru_cache_en.webp" alt="LRUCache Doubly Linked List State"></p><p>In other words, in this reference-based scenario, when evicting a node, if the node at the tail of the list is currently referenced externally (refs &gt; 1), it cannot be evicted. This requires <strong>traversing the list to find an evictable node, which is inefficient</strong>. In the worst case, if all nodes are referenced, the entire list might be traversed without being able to evict anything.</p><p>To solve this problem, the LRUCache implementation uses two doubly linked lists. One is <strong>in_use_</strong>, which stores referenced cache items. The other is <strong>lru_</strong>, which stores unreferenced cache items. Each cache item can only be in one of these lists at a time, never both. However, an item can move between the two lists depending on whether it’s currently referenced. This way, when a node needs to be evicted, it can be taken directly from the lru_ list without traversing the in_use_ list.</p><p>That’s the introduction to the dual linked list design. We’ll understand it better by looking at the core implementation of LRUCache.</p><h3 id="Cache-Insertion-Deletion-and-Lookup"><a href="#Cache-Insertion-Deletion-and-Lookup" class="headerlink" title="Cache Insertion, Deletion, and Lookup"></a>Cache Insertion, Deletion, and Lookup</h3><p>Let’s first look at node insertion, implemented in <a href="https://github.com/google/leveldb/blob/main/util/cache.cc#L267">util&#x2F;cache.cc</a>. In short, an LRUHandle object is created, placed in the in_use_ doubly linked list, and the hash table is updated. If the cache capacity is reached after insertion, a node needs to be evicted. However, the implementation has many subtle details, and LevelDB’s code is indeed concise.</p><p>Let’s look at the parameters: key and value are passed by the client, hash is the hash of the key, charge is the cost of the cache item, and deleter is the callback function for deleting the item. Since LRUHandle has a flexible array member at the end, we first manually calculate the size of the LRUHandle object, allocate memory, and then initialize its members. Here, refs is initialized to 1 because a Handle pointer is returned.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Cache::Handle* <span class="title">LRUCache::Insert</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">uint32_t</span> hash, <span class="type">void</span>* value,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">size_t</span> charge,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">void</span> (*deleter)(<span class="type">const</span> Slice&amp; key,</span></span></span><br><span class="line"><span class="params"><span class="function">                                                <span class="type">void</span>* value))</span> </span>&#123;</span><br><span class="line">  <span class="function">MutexLock <span class="title">l</span><span class="params">(&amp;mutex_)</span></span>;</span><br><span class="line"></span><br><span class="line">  LRUHandle* e =</span><br><span class="line">      <span class="built_in">reinterpret_cast</span>&lt;LRUHandle*&gt;(<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(LRUHandle) - <span class="number">1</span> + key.<span class="built_in">size</span>()));</span><br><span class="line">  e-&gt;value = value;</span><br><span class="line">  e-&gt;deleter = deleter;</span><br><span class="line">  e-&gt;charge = charge;</span><br><span class="line">  e-&gt;key_length = key.<span class="built_in">size</span>();</span><br><span class="line">  e-&gt;hash = hash;</span><br><span class="line">  e-&gt;in_cache = <span class="literal">false</span>;</span><br><span class="line">  e-&gt;refs = <span class="number">1</span>;  <span class="comment">// for the returned handle.</span></span><br><span class="line">  std::<span class="built_in">memcpy</span>(e-&gt;key_data, key.<span class="built_in">data</span>(), key.<span class="built_in">size</span>());</span><br></pre></td></tr></table></figure><p>The next part is also interesting. LevelDB’s LRUCache implementation supports a cache capacity of 0, which means no data is cached. To cache an item, in_cache is set to true, and the refs count is incremented because the Handle object is placed in the in_use_ list. The handle is also inserted into the hash table. Note the FinishErase call here, which is worth discussing.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (capacity_ &gt; <span class="number">0</span>) &#123;</span><br><span class="line">  e-&gt;refs++;  <span class="comment">// for the cache&#x27;s reference.</span></span><br><span class="line">  e-&gt;in_cache = <span class="literal">true</span>;</span><br><span class="line">  <span class="built_in">LRU_Append</span>(&amp;in_use_, e);</span><br><span class="line">  usage_ += charge;</span><br><span class="line">  <span class="built_in">FinishErase</span>(table_.<span class="built_in">Insert</span>(e));</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;  <span class="comment">// don&#x27;t cache. (capacity_==0 is supported and turns off caching.)</span></span><br><span class="line">  <span class="comment">// next is read by key() in an assert, so it must be initialized</span></span><br><span class="line">  e-&gt;next = <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>As we discussed in the hash table implementation, if a key already exists when inserting into the hash table, the old Handle object is returned. The FinishErase function is used to clean up this old Handle object.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// If e != nullptr, finish removing *e from the cache; it has already been</span></span><br><span class="line"><span class="comment">// removed from the hash table.  Return whether e != nullptr.</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">LRUCache::FinishErase</span><span class="params">(LRUHandle* e)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (e != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">    <span class="built_in">assert</span>(e-&gt;in_cache);</span><br><span class="line">    <span class="built_in">LRU_Remove</span>(e);</span><br><span class="line">    e-&gt;in_cache = <span class="literal">false</span>;</span><br><span class="line">    usage_ -= e-&gt;charge;</span><br><span class="line">    <span class="built_in">Unref</span>(e);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> e != <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The cleanup involves several steps. First, the old handle is removed from either the in_use_ or lru_ list. It’s not certain which list the old Handle object is in, but that’s okay—<strong>LRU_Remove can handle it without knowing which list it’s in</strong>. The LRU_Remove function is very simple, just two lines of code. If you’re unsure, try drawing a diagram:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">LRUCache::LRU_Remove</span><span class="params">(LRUHandle* e)</span> </span>&#123;</span><br><span class="line">  e-&gt;next-&gt;prev = e-&gt;prev;</span><br><span class="line">  e-&gt;prev-&gt;next = e-&gt;next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Next, in_cache is set to false, indicating it’s no longer in the cache. Then, the cache capacity is updated by decrementing usage_. Finally, Unref is called to decrement the reference count of this Handle object, which might still be referenced elsewhere. Only when all references are released will the Handle object be truly deallocated. The <a href="https://github.com/google/leveldb/blob/main/util/cache.cc#L226">Unref function</a> is also quite interesting; I’ll post the code here:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">LRUCache::Unref</span><span class="params">(LRUHandle* e)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">assert</span>(e-&gt;refs &gt; <span class="number">0</span>);</span><br><span class="line">  e-&gt;refs--;</span><br><span class="line">  <span class="keyword">if</span> (e-&gt;refs == <span class="number">0</span>) &#123;  <span class="comment">// Deallocate.</span></span><br><span class="line">    <span class="built_in">assert</span>(!e-&gt;in_cache);</span><br><span class="line">    (*e-&gt;deleter)(e-&gt;<span class="built_in">key</span>(), e-&gt;value);</span><br><span class="line">    <span class="built_in">free</span>(e);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (e-&gt;in_cache &amp;&amp; e-&gt;refs == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="comment">// No longer in use; move to lru_ list.</span></span><br><span class="line">    <span class="built_in">LRU_Remove</span>(e);</span><br><span class="line">    <span class="built_in">LRU_Append</span>(&amp;lru_, e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>First, the count is decremented. If it becomes 0, it means there are <strong>no external references, and the memory can be safely deallocated</strong>. Deallocation has two parts: first, the deleter callback is used to clean up the memory for the value, and then free is used to release the memory for the LRUHandle pointer. If the count becomes 1 and the handle is still in the cache, it means only the cache itself holds a reference. In this case, <strong>the Handle object needs to be removed from the in_use_ list and moved to the lru_ list</strong>. If a node needs to be evicted later, this node in the lru_ list can be evicted directly.</p><p>Now for the final step of the insertion operation: checking if the cache has remaining capacity. If not, eviction begins. As long as the capacity is insufficient, the node at the head of the lru_ list is taken, <strong>removed from the hash table, and then cleaned up using FinishErase</strong>. Checking if the doubly linked list is empty is also interesting; it uses a dummy node, which we’ll discuss later.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (usage_ &gt; capacity_ &amp;&amp; lru_.next != &amp;lru_) &#123;</span><br><span class="line">  LRUHandle* old = lru_.next;</span><br><span class="line">  <span class="built_in">assert</span>(old-&gt;refs == <span class="number">1</span>);</span><br><span class="line">  <span class="type">bool</span> erased = <span class="built_in">FinishErase</span>(table_.<span class="built_in">Remove</span>(old-&gt;<span class="built_in">key</span>(), old-&gt;hash));</span><br><span class="line">  <span class="keyword">if</span> (!erased) &#123;  <span class="comment">// to avoid unused variable when compiled NDEBUG</span></span><br><span class="line">    <span class="built_in">assert</span>(erased);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The entire insertion function, including the eviction logic, and indeed the entire LevelDB codebase, is filled with assert statements for various checks, ensuring that the process terminates immediately if something goes wrong, preventing error propagation.</p><p>After seeing insertion, deletion is straightforward. The implementation is simple: remove the node from the hash table and then call FinishErase to clean it up.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">LRUCache::Erase</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">uint32_t</span> hash)</span> </span>&#123;</span><br><span class="line">  <span class="function">MutexLock <span class="title">l</span><span class="params">(&amp;mutex_)</span></span>;</span><br><span class="line">  <span class="built_in">FinishErase</span>(table_.<span class="built_in">Remove</span>(key, hash));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Node lookup is also relatively simple. It looks up directly from the hash table. If found, the reference count is incremented, and the Handle object is returned. Just like insertion, returning a Handle object increments its reference count. So, if it’s not used externally, you must remember to call the Release method to manually release the reference, otherwise, you could have a memory leak.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Cache::Handle* <span class="title">LRUCache::Lookup</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">uint32_t</span> hash)</span> </span>&#123;</span><br><span class="line">  <span class="function">MutexLock <span class="title">l</span><span class="params">(&amp;mutex_)</span></span>;</span><br><span class="line">  LRUHandle* e = table_.<span class="built_in">Lookup</span>(key, hash);</span><br><span class="line">  <span class="keyword">if</span> (e != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">    <span class="built_in">Ref</span>(e);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">reinterpret_cast</span>&lt;Cache::Handle*&gt;(e);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">LRUCache::Release</span><span class="params">(Cache::Handle* handle)</span> </span>&#123;</span><br><span class="line">  <span class="function">MutexLock <span class="title">l</span><span class="params">(&amp;mutex_)</span></span>;</span><br><span class="line">  <span class="built_in">Unref</span>(<span class="built_in">reinterpret_cast</span>&lt;LRUHandle*&gt;(handle));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Additionally, the Cache interface also implements a Prune method for proactively cleaning the cache. The method is similar to the cleanup logic in insertion, but it clears out all nodes in the lru_ list. This function is not used anywhere in LevelDB.</p><h3 id="Doubly-Linked-List-Operations"><a href="#Doubly-Linked-List-Operations" class="headerlink" title="Doubly Linked List Operations"></a>Doubly Linked List Operations</h3><p>Let’s discuss the doubly linked list operations in more detail. We already know there are two lists: lru_ and in_use_. The comments make it clearer:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Dummy head of LRU list.</span></span><br><span class="line"><span class="comment">// lru.prev is newest entry, lru.next is oldest entry.</span></span><br><span class="line"><span class="comment">// Entries have refs==1 and in_cache==true.</span></span><br><span class="line"><span class="function">LRUHandle lru_ <span class="title">GUARDED_BY</span><span class="params">(mutex_)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Dummy head of in-use list.</span></span><br><span class="line"><span class="comment">// Entries are in use by clients, and have refs &gt;= 2 and in_cache==true.</span></span><br><span class="line"><span class="function">LRUHandle in_use_ <span class="title">GUARDED_BY</span><span class="params">(mutex_)</span></span>;</span><br></pre></td></tr></table></figure><p>The lru_ member is the list’s dummy node. Its next member points to the oldest cache item in the lru_ list, and its prev member points to the newest. In the LRUCache constructor, the next and prev of lru_ both point to itself, indicating an empty list. Remember how we checked for evictable nodes during insertion? It was with lru_.next !&#x3D; &amp;lru_.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">LRUCache::<span class="built_in">LRUCache</span>() : <span class="built_in">capacity_</span>(<span class="number">0</span>), <span class="built_in">usage_</span>(<span class="number">0</span>) &#123;</span><br><span class="line">  <span class="comment">// Make empty circular linked lists.</span></span><br><span class="line">  lru_.next = &amp;lru_;</span><br><span class="line">  lru_.prev = &amp;lru_;</span><br><span class="line">  in_use_.next = &amp;in_use_;</span><br><span class="line">  in_use_.prev = &amp;in_use_;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>A <strong>dummy node</strong> is a technique used in many data structure implementations to simplify boundary condition handling. In the context of an LRU cache, a dummy node is mainly used as the head of the list, so the head always exists, even when the list is empty. This approach simplifies insertion and deletion operations because <strong>you don’t need special handling for an empty list</strong>.</p><p>For example, when adding a new element to the list, you can insert it directly between the dummy node and the current first element without checking if the list is empty. Similarly, when deleting an element, you don’t have to worry about updating the list head after deleting the last element, because the dummy node is always there.</p><p>We’ve already seen LRU_Remove, which is just two lines of code. For adding a node to the list, I’ve created a diagram that, combined with the code, should make it easy to understand:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250613_leveldb_source_lru_cache_linkedlist.webp" alt="LRUCache Doubly Linked List Operations"></p><p>Here, e is the new node being inserted, and list is the dummy node of the list. I’ve used circles for list’s prev and next to indicate they can point to list itself, as in an initial empty list. The insertion happens before the dummy node, so list-&gt;prev is always the newest node in the list, and list-&gt;next is always the oldest. For this kind of list manipulation, drawing a diagram makes everything clear.</p><h3 id="reinterpret-cast-Conversion"><a href="#reinterpret-cast-Conversion" class="headerlink" title="reinterpret_cast Conversion"></a>reinterpret_cast Conversion</h3><p>Finally, let’s briefly touch on the use of reinterpret_cast in the code to convert between LRUHandle* and Cache::Handle*. <strong>reinterpret_cast forcibly converts a pointer of one type to a pointer of another type without any type checking</strong>. It doesn’t adjust the underlying data; it just tells the compiler: “Treat this memory address as if it were of this other type.” This operation is generally dangerous and not recommended.</p><p>However, LevelDB does this to separate the interface from the implementation. It exposes the concrete internal data structure LRUHandle* to external users as an abstract, opaque handle Cache::Handle*, while internally converting this opaque handle back to the concrete data structure for operations.</p><p><strong>In this specific, controlled design pattern, it is completely safe</strong>. This is because only the LRUCache internal code can create an LRUHandle. Any Cache::Handle* returned to an external user always points to a valid LRUHandle object. Any Cache::Handle* passed to LRUCache must have been previously returned by the same LRUCache instance.</p><p>As long as these conventions are followed, reinterpret_cast is just switching between “views” of the pointer; the pointer itself always points to a valid, correctly typed object. If a user tries to forge a Cache::Handle* or pass in an unrelated pointer, the program will have undefined behavior, but that’s a misuse of the API.</p><h2 id="ShardedLRUCache-Implementation"><a href="#ShardedLRUCache-Implementation" class="headerlink" title="ShardedLRUCache Implementation"></a>ShardedLRUCache Implementation</h2><p>In the LRUCache implementation, insertion, lookup, and deletion operations must be protected by a single mutex. In a multi-threaded environment, if there’s only one large cache, <strong>this lock becomes a global bottleneck</strong>. When multiple threads access the cache simultaneously, only one thread can acquire the lock, and all others must wait, which severely impacts concurrency performance.</p><p>To improve performance, ShardedLRUCache divides the cache into multiple shards (the shard_ array), each with its own independent lock. When a request arrives, it is routed to a specific shard based on the key’s hash value. This way, different threads accessing different shards can proceed in parallel because they acquire different locks, thereby reducing lock contention and increasing overall throughput. A diagram might make this clearer (Mermaid code is <a href="/downloads/mermaid_leveldb_lru_cache_shard.txt">here</a>).</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250613_leveldb_source_lru_cache_shard_en.webp" alt="ShardedLRUCache Implementation"></p><p>So how many shards are needed? LevelDB hardcodes $kNumShards &#x3D; 1 \ll kNumShardBits$, which evaluates to 16. This is an empirical choice. If the number of shards is too small, say 2 or 4, lock contention can still be severe on servers with many cores. If there are too many shards, the capacity of each shard becomes very small. This could lead to a “hot” shard frequently evicting data while a “cold” shard has plenty of free space, thus lowering the overall cache hit rate.</p><p>Choosing 16 provides sufficient concurrency for typical 8-core or 16-core servers without introducing excessive overhead. Also, choosing a power of two allows for fast shard index calculation using the bitwise operation $hash \gg (32 - kNumShardBits)$.</p><p>With sharding, the original LRUCache class is wrapped. The constructor needs to specify the number of shards, the capacity per shard, etc. The <a href="https://github.com/google/leveldb/blob/main/util/cache.cc#L352">implementation</a> is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span>:</span><br><span class="line"> <span class="function"><span class="keyword">explicit</span> <span class="title">ShardedLRUCache</span><span class="params">(<span class="type">size_t</span> capacity)</span> : last_id_(<span class="number">0</span>) &#123;</span></span><br><span class="line">   <span class="type">const</span> <span class="type">size_t</span> per_shard = (capacity + (kNumShards - <span class="number">1</span>)) / kNumShards;</span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> s = <span class="number">0</span>; s &lt; kNumShards; s++) &#123;</span><br><span class="line">     shard_[s].<span class="built_in">SetCapacity</span>(per_shard);</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>Other related cache operations, like insertion, lookup, and deletion, use the Shard function to determine which shard to operate on. Here is insertion as an example, <a href="https://github.com/google/leveldb/blob/main/util/cache.cc%23L359">implemented here</a>:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Handle* <span class="title">Insert</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">void</span>* value, <span class="type">size_t</span> charge,</span></span></span><br><span class="line"><span class="params"><span class="function">               <span class="type">void</span> (*deleter)(<span class="type">const</span> Slice&amp; key, <span class="type">void</span>* value))</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">  <span class="type">const</span> <span class="type">uint32_t</span> hash = <span class="built_in">HashSlice</span>(key);</span><br><span class="line">  <span class="keyword">return</span> shard_[<span class="built_in">Shard</span>(hash)].<span class="built_in">Insert</span>(key, hash, value, charge, deleter);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The HashSlice and Shard functions are straightforward, so we’ll skip them. It’s also worth noting that ShardedLRUCache inherits from the Cache abstract class and implements its various interfaces. This allows it to be used wherever a Cache interface is expected.</p><p>Finally, there’s one more small detail worth mentioning: the Cache interface has a NewId function. I haven’t seen other LRU cache implementations that support generating an ID from the cache. <strong>Why does LevelDB do this?</strong></p><h3 id="Cache-ID-Generation"><a href="#Cache-ID-Generation" class="headerlink" title="Cache ID Generation"></a>Cache ID Generation</h3><p>LevelDB provides comments, but they might not be clear without context. Let’s analyze this with a use case.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Return a new numeric id.  May be used by multiple clients who are</span></span><br><span class="line"><span class="comment">// sharing the same cache to partition the key space.  Typically the</span></span><br><span class="line"><span class="comment">// client will allocate a new id at startup and prepend the id to</span></span><br><span class="line"><span class="comment">// its cache keys.</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="type">uint64_t</span> <span class="title">NewId</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br></pre></td></tr></table></figure><p>For some background, when we open a LevelDB database, we can create a Cache object and pass it in options.block_cache to cache data blocks and filter blocks from SSTable files. If we don’t pass one, LevelDB creates an 8 MB ShardedLRUCache object by default. This <strong>Cache object is globally shared; all Table objects in the database use this same BlockCache instance to cache their data blocks</strong>.</p><p>In Table::Open in <a href="https://github.com/google/leveldb/blob/main/table/table.cc#L72">table&#x2F;table.cc</a>, we see that every time an SSTable file is opened, NewId is called to generate a cache_id. Under the hood, a mutex ensures that the generated ID is globally increasing. Later, when we need to read a data block at offset from an SSTable file, we use &lt;cache_id, offset&gt; as the cache key. This way, different SSTable files have different cache_ids, so even if their offsets are the same, the cache keys will be different, preventing collisions.</p><p>Simply put, ShardedLRUCache provides globally increasing IDs mainly to distinguish between different SSTable files, saving each file from having to maintain its own unique ID for cache keys.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Alright, that’s our analysis of LevelDB’s LRU Cache. We’ve seen the design philosophy and implementation details of an industrial-grade, high-performance cache. Let’s summarize the key points:</p><ol><li><strong>Interface and Implementation Separation</strong>: By using an abstract Cache interface, the cache’s users are decoupled from its concrete implementation, reflecting the Dependency Inversion Principle of object-oriented design. User code only needs to interact with the Cache interface.</li><li><strong>Carefully Designed Cache Entries</strong>: The LRUHandle struct includes metadata like reference counts and cache flags. It uses a flexible array member to store variable-length keys, reducing memory allocation overhead and improving performance.</li><li><strong>Dual Linked List Optimization</strong>: Using two doubly linked lists (in_use_ and lru_) to manage “in-use” and “evictable” cache items avoids traversing the entire list during eviction, thus improving eviction efficiency.</li><li><strong>Dummy Node Technique</strong>: Using dummy nodes simplifies linked list operations by eliminating the need to handle special cases for empty lists, making the code more concise.</li><li><strong>Sharding to Reduce Lock Contention</strong>: ShardedLRUCache divides the cache into multiple shards, each with its own lock, significantly improving concurrency performance in multi-threaded environments.</li><li><strong>Reference Counting for Memory Management</strong>: Precise reference counting ensures that cache entries are not deallocated while still referenced externally, and that memory is reclaimed promptly when they are no longer needed.</li><li><strong>Assertions for Correctness</strong>: Extensive use of assertions checks preconditions and invariants, ensuring that errors are detected early.</li></ol><p>These design ideas and implementation techniques are well worth learning from for our own projects. Especially in high-concurrency, high-performance scenarios, the optimization methods used in LevelDB can help us design more efficient cache systems.</p>]]></content:encoded>
      
      
      <category domain="https://selfboot.cn/categories/Source-Code-Analysis/">Source Code Analysis</category>
      
      
      <category domain="https://selfboot.cn/tags/C/">C++</category>
      
      <category domain="https://selfboot.cn/tags/LevelDB/">LevelDB</category>
      
      
      <comments>https://selfboot.cn/en/2025/06/13/leveldb_source_LRU_cache/#disqus_thread</comments>
      
    </item>
    
    
    
    
    
    <item>
      <title>LevelDB Explained - The Implementation Details of MemTable</title>
      <link>https://selfboot.cn/en/2025/06/11/leveldb_source_memtable/</link>
      <guid>https://selfboot.cn/en/2025/06/11/leveldb_source_memtable/</guid>
      <pubDate>Wed, 11 Jun 2025 19:47:42 GMT</pubDate>
      
      <description>This article delves into the implementation details of LevelDB&#39;s MemTable, covering its core role in managing recently written data in memory. It details MemTable&#39;s internal structure, its skiplist-based data structure, the encoding format for key-value pairs, and its memory management mechanism. By analyzing the implementation of the Add and Get methods, it shows how key-value pairs are encoded, stored, and efficiently queried. It also explains how the reference counting mechanism enables concurrent access control and how MemTable collaborates with friend classes for data traversal. This is a valuable reference for understanding LevelDB&#39;s read/write flow and performance optimization.</description>
      
      
      
      <content:encoded><![CDATA[<p>In LevelDB, all write operations are first recorded in a <a href="https://selfboot.cn/en/2024/08/14/leveldb_source_wal_log/">Write-Ahead Log (WAL)</a> to ensure durability. The data is then stored in a MemTable. The primary role of the MemTable is to <strong>store recently written data in an ordered fashion in memory</strong>. Once certain conditions are met, the data is flushed to disk in batches.</p><p>LevelDB maintains two types of MemTables in memory. One is writable and accepts new write requests. When it reaches a certain size threshold, it is converted into an immutable MemTable. A background process is then triggered to write it to disk, forming an SSTable. During this process, a new MemTable is created to accept new write operations, ensuring that write operations can continue without interruption.</p><p>When reading data, LevelDB first queries the MemTable. If the data is not found, it then queries the immutable MemTable, and finally the SSTable files on disk. In LevelDB’s implementation, both the writable MemTable and the immutable MemTable are implemented using the <code>MemTable</code> class. In this article, we will examine the implementation details of the memtable.</p><span id="more"></span><h2 id="How-to-Use-MemTable"><a href="#How-to-Use-MemTable" class="headerlink" title="How to Use MemTable"></a>How to Use MemTable</h2><p>First, let’s see where the <code>MemTable</code> class is used in LevelDB. In the core DB <a href="https://github.com/google/leveldb/blob/main/db/db_impl.h#L177">implementation class <code>DBImpl</code></a>, we can see two member pointers:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DBImpl</span> : <span class="keyword">public</span> DB &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">DBImpl</span>(<span class="type">const</span> Options&amp; options, <span class="type">const</span> std::string&amp; dbname);</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">  MemTable* mem_;</span><br><span class="line">  <span class="function">MemTable* imm_ <span class="title">GUARDED_BY</span><span class="params">(mutex_)</span></span>;  <span class="comment">// Memtable being compacted</span></span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>mem_</code> is the writable memtable, and <code>imm_</code> is the immutable memtable. These are the only two memtable objects in the database instance, used to store recently written data. Both are used when reading and writing key-values.</p><p>Let’s first look at the write process. I previously wrote about the entire process of writing key-values in <a href="https://selfboot.cn/en/2025/01/24/leveldb_source_writedb/">LevelDB Explained - Implementation and Optimization Details of Key-Value Writing</a>. During the write process, after the WAL is successfully written, the <code>MemTableInserter</code> class in <a href="https://github.com/google/leveldb/blob/main/db/write_batch.cc#L121">db&#x2F;write_batch.cc</a> is called to write to the memtable. The specific code is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// db/write_batch.cc</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MemTableInserter</span> : <span class="keyword">public</span> WriteBatch::Handler &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  SequenceNumber sequence_;</span><br><span class="line">  MemTable* mem_;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">Put</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">const</span> Slice&amp; value)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    mem_-&gt;<span class="built_in">Add</span>(sequence_, kTypeValue, key, value);</span><br><span class="line">    sequence_++;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Here, the <code>Add</code> interface is called to write a key-value pair to the memtable. <code>sequence_</code> is the write sequence number, <code>kTypeValue</code> is the write type, and <code>key</code> and <code>value</code> are the user-provided key-value pair.</p><p>Besides the write process, the <code>MemTable</code> class is also needed when reading key-value pairs. Specifically, in the Get method of the <code>DBImpl</code> class in <a href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1147">db&#x2F;db_impl.cc</a>, the memtable’s Get method is called to query for a key-value pair.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">DBImpl::Get</span><span class="params">(<span class="type">const</span> ReadOptions&amp; options, <span class="type">const</span> Slice&amp; key,</span></span></span><br><span class="line"><span class="params"><span class="function">                   std::string* value)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  MemTable* mem = mem_;</span><br><span class="line">  MemTable* imm = imm_;</span><br><span class="line">  Version* current = versions_-&gt;<span class="built_in">current</span>();</span><br><span class="line">  mem-&gt;<span class="built_in">Ref</span>();</span><br><span class="line">  <span class="keyword">if</span> (imm != <span class="literal">nullptr</span>) imm-&gt;<span class="built_in">Ref</span>();</span><br><span class="line">  <span class="comment">// Unlock while reading from files and memtables</span></span><br><span class="line">  &#123;</span><br><span class="line">    mutex_.<span class="built_in">Unlock</span>();</span><br><span class="line">    <span class="function">LookupKey <span class="title">lkey</span><span class="params">(key, snapshot)</span></span>;</span><br><span class="line">    <span class="keyword">if</span> (mem-&gt;<span class="built_in">Get</span>(lkey, value, &amp;s)) &#123;</span><br><span class="line">      <span class="comment">// Done</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (imm != <span class="literal">nullptr</span> &amp;&amp; imm-&gt;<span class="built_in">Get</span>(lkey, value, &amp;s)) &#123;</span><br><span class="line">      <span class="comment">// Done</span></span><br><span class="line">    &#125;</span><br><span class="line">    mutex_.<span class="built_in">Lock</span>();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  mem-&gt;<span class="built_in">Unref</span>();</span><br><span class="line">  <span class="keyword">if</span> (imm != <span class="literal">nullptr</span>) imm-&gt;<span class="built_in">Unref</span>();</span><br><span class="line">  <span class="comment">// ...</span></span><br></pre></td></tr></table></figure><p>Here, local pointers <code>mem</code> and <code>imm</code> are created to reference the member variables <code>mem_</code> and <code>imm_</code>, and these local pointers are then used for reading. A question arises here: <strong><span style="color:red">Why not use the member variables <code>mem_</code> and <code>imm_</code> directly for reading?</span></strong> We’ll answer this question later in the <a href="#Answering-Questions">Answering Questions</a> section.</p><p>So far, we have seen the main usage of MemTable. Now let’s look at how it’s implemented internally.</p><h2 id="MemTable-Implementation"><a href="#MemTable-Implementation" class="headerlink" title="MemTable Implementation"></a>MemTable Implementation</h2><p>Before discussing the implementation of MemTable’s external methods, it’s important to know that the data in a MemTable is actually stored in a skiplist. A skiplist provides most of the advantages of a balanced tree (such as ordering and logarithmic time complexity for insertion and search), but is simpler to implement. For a detailed implementation of the skiplist, you can refer to <a href="https://selfboot.cn/en/2024/09/09/leveldb_source_skiplist/">LevelDB Explained - How to implement SkipList</a>.</p><p>The MemTable class declares a skiplist object <code>table_</code> as a member variable. The skiplist is a template class that requires a key type and a Comparator for initialization. Here, the key in the memtable’s skiplist is of type <code>const char*</code>, and the comparator is of type KeyComparator. KeyComparator is a custom comparator used to sort the key-values in the skiplist.</p><p>KeyComparator contains a member variable comparator of type InternalKeyComparator, which is used to compare the sizes of internal keys. The operator() of the KeyComparator comparator overloads the function call operator. It first decodes the internal key from the <code>const char*</code>, and then calls the Compare method of InternalKeyComparator to compare the sizes of the internal keys. The specific implementation is in <a href="https://github.com/google/leveldb/blob/main/db/memtable.cc#L28">db&#x2F;memtable.cc</a>.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> MemTable::KeyComparator::<span class="built_in">operator</span>()(<span class="type">const</span> <span class="type">char</span>* aptr,</span><br><span class="line">                                        <span class="type">const</span> <span class="type">char</span>* bptr) <span class="type">const</span> &#123;</span><br><span class="line">  <span class="comment">// Internal keys are encoded as length-prefixed strings.</span></span><br><span class="line">  Slice a = <span class="built_in">GetLengthPrefixedSlice</span>(aptr);</span><br><span class="line">  Slice b = <span class="built_in">GetLengthPrefixedSlice</span>(bptr);</span><br><span class="line">  <span class="keyword">return</span> comparator.<span class="built_in">Compare</span>(a, b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>To add, LevelDB’s <strong>internal key</strong> is actually a concatenation of the user-provided key and an internal sequence number, plus a type identifier. This ensures that different versions of the same key are ordered, thus enabling MVCC for concurrent reads and writes. When storing to the MemTable, length information is encoded before the internal key, called a <code>memtable key</code>. This allows us to decode the internal key from the <code>const char*</code> memtable key using the length information during reads. I have analyzed this part in detail in another article: <a href="https://selfboot.cn/en/2025/06/10/leveldb_mvcc_intro/">LevelDB Explained - Understanding Multi-Version Concurrency Control (MVCC)</a>. Feel free to check it out.</p><p>MemTable uses a skiplist for storage and primarily supports Add and Get methods externally. Let’s look at the implementation details of these two functions.</p><h3 id="Add-a-Key-Value-Pair"><a href="#Add-a-Key-Value-Pair" class="headerlink" title="Add a Key-Value Pair"></a>Add a Key-Value Pair</h3><p>The Add method is used to add a key-value pair to the MemTable, where key and value are the user-provided key-value pair, SequenceNumber is the sequence number at the time of writing, and ValueType is the write type. There are two types: kTypeValue and kTypeDeletion. kTypeValue represents an insertion operation, and kTypeDeletion represents a deletion operation. In LevelDB, a deletion operation is actually an insertion of a key-value pair marked for deletion.</p><p>The implementation of Add is in <a href="https://github.com/google/leveldb/blob/main/db/memtable.cc#L76">db&#x2F;memtable.cc</a>, and the function definition is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">MemTable::Add</span><span class="params">(SequenceNumber s, ValueType type, <span class="type">const</span> Slice&amp; key,</span></span></span><br><span class="line"><span class="params"><span class="function">                   <span class="type">const</span> Slice&amp; value)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Format of an entry is concatenation of:</span></span><br><span class="line">  <span class="comment">//  key_size     : varint32 of internal_key.size()</span></span><br><span class="line">  <span class="comment">//  key bytes    : char[internal_key.size()]</span></span><br><span class="line">  <span class="comment">//  tag          : uint64((sequence &lt;&lt; 8) | type)</span></span><br><span class="line">  <span class="comment">//  value_size   : varint32 of value.size()</span></span><br><span class="line">  <span class="comment">//  value bytes  : char[value.size()]</span></span><br><span class="line">  <span class="comment">//...</span></span><br></pre></td></tr></table></figure><p>The comments here are very clear. The MemTable stores a formatted key-value pair, starting with the length of the internal key, followed by the internal key byte string (which is the tag part below, including User Key + Sequence Number + Value Type), then the length of the value, and finally the value byte string. The whole thing consists of 5 parts, with the following format:</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">+-----------+-----------+----------------------+----------+--------+</span><br><span class="line">| <span class="type">Key</span> <span class="type">Size</span>  | <span class="type">User</span> <span class="type">Key</span>  |          tag         | <span class="type">Val</span> <span class="type">Size</span> | <span class="type">Value</span>  |</span><br><span class="line">+-----------+-----------+----------------------+----------+--------+</span><br><span class="line">| varint32  | key bytes | <span class="number">64</span>-bit, last <span class="number">8</span> bits <span class="keyword">as</span> <span class="class"><span class="keyword">type</span> | varint32 | value  |</span></span><br></pre></td></tr></table></figure><p>Here, the first part, <code>keysize</code>, is the Varint-encoded length of the user key plus an 8-byte tag. The tag is a combination of the sequence number and the value type, with the high 56 bits storing the sequence number and the low 8 bits storing the value type. The other parts are simpler and will not be detailed here.</p><p>The insertion process first calculates the required memory size, allocates the memory, then writes the values of each part, and finally inserts it into the skiplist. The specific write process code is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// db/memtable.cc</span></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="type">size_t</span> key_size = key.<span class="built_in">size</span>();</span><br><span class="line">  <span class="type">size_t</span> val_size = value.<span class="built_in">size</span>();</span><br><span class="line">  <span class="type">size_t</span> internal_key_size = key_size + <span class="number">8</span>;</span><br><span class="line">  <span class="type">const</span> <span class="type">size_t</span> encoded_len = <span class="built_in">VarintLength</span>(internal_key_size) +</span><br><span class="line">                             internal_key_size + <span class="built_in">VarintLength</span>(val_size) +</span><br><span class="line">                             val_size;</span><br><span class="line">  <span class="type">char</span>* buf = arena_.<span class="built_in">Allocate</span>(encoded_len);</span><br><span class="line">  <span class="type">char</span>* p = <span class="built_in">EncodeVarint32</span>(buf, internal_key_size);</span><br><span class="line">  std::<span class="built_in">memcpy</span>(p, key.<span class="built_in">data</span>(), key_size);</span><br><span class="line">  p += key_size;</span><br><span class="line">  <span class="built_in">EncodeFixed64</span>(p, (s &lt;&lt; <span class="number">8</span>) | type);</span><br><span class="line">  p += <span class="number">8</span>;</span><br><span class="line">  p = <span class="built_in">EncodeVarint32</span>(p, val_size);</span><br><span class="line">  std::<span class="built_in">memcpy</span>(p, value.<span class="built_in">data</span>(), val_size);</span><br><span class="line">  <span class="built_in">assert</span>(p + val_size == buf + encoded_len);</span><br><span class="line">  table_.<span class="built_in">Insert</span>(buf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>EncodeVarint32</code> and <code>EncodeFixed64</code> here are encoding functions used to encode integers into a byte stream. For details, you can refer to <a href="https://selfboot.cn/en/2024/08/29/leveldb_source_utils/">LevelDB Explained - Arena, Random, CRC32, and More.</a>. Next, let’s look at the implementation of key querying.</p><h3 id="Get-a-Key-Value"><a href="#Get-a-Key-Value" class="headerlink" title="Get a Key-Value"></a>Get a Key-Value</h3><p>The definition of the query method is also quite simple, as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// If memtable contains a value for key, store it in *value and return true.</span></span><br><span class="line"><span class="comment">// If memtable contains a deletion for key, store a NotFound() error</span></span><br><span class="line"><span class="comment">// in *status and return true.</span></span><br><span class="line"><span class="comment">// Else, return false.</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Get</span><span class="params">(<span class="type">const</span> LookupKey&amp; key, std::string* value, Status* s)</span></span>;</span><br></pre></td></tr></table></figure><p>The key passed to this interface is not the user input key, but a <code>LookupKey</code> object, defined in <a href="https://github.com/google/leveldb/blob/main/db/dbformat.h#L184">db&#x2F;dbformat.h</a>. This is because in LevelDB, the same user key can have different versions. When querying, a snapshot (i.e., sequence number) must be specified to get the corresponding version. Therefore, a LookupKey class is abstracted here, which can be initialized with the user-input key and sequence number to get the required key-value format.</p><p>For the lookup process, we first get the previously mentioned memtable key using the memtable_key method of the LookupKey object, and then call the skiplist’s Seek method to perform the search. The complete implementation of the Get method in <a href="https://github.com/google/leveldb/blob/main/db/memtable.cc#L102">db&#x2F;memtable.cc</a> is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">MemTable::Get</span><span class="params">(<span class="type">const</span> LookupKey&amp; key, std::string* value, Status* s)</span> </span>&#123;</span><br><span class="line">  Slice memkey = key.<span class="built_in">memtable_key</span>();</span><br><span class="line">  <span class="function">Table::Iterator <span class="title">iter</span><span class="params">(&amp;table_)</span></span>;</span><br><span class="line">  iter.<span class="built_in">Seek</span>(memkey.<span class="built_in">data</span>());</span><br><span class="line">  <span class="keyword">if</span> (iter.<span class="built_in">Valid</span>()) &#123;</span><br><span class="line">    <span class="comment">// Check that it belongs to same user key.  We do not check the</span></span><br><span class="line">    <span class="comment">// sequence number since the Seek() call above should have skipped</span></span><br><span class="line">    <span class="comment">// all entries with overly large sequence numbers.</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* entry = iter.<span class="built_in">key</span>();</span><br><span class="line">    <span class="type">uint32_t</span> key_length;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span>* key_ptr = <span class="built_in">GetVarint32Ptr</span>(entry, entry + <span class="number">5</span>, &amp;key_length);</span><br><span class="line">    <span class="keyword">if</span> (comparator_.comparator.<span class="built_in">user_comparator</span>()-&gt;<span class="built_in">Compare</span>(</span><br><span class="line">            <span class="built_in">Slice</span>(key_ptr, key_length - <span class="number">8</span>), key.<span class="built_in">user_key</span>()) == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// Correct user key</span></span><br><span class="line">      <span class="type">const</span> <span class="type">uint64_t</span> tag = <span class="built_in">DecodeFixed64</span>(key_ptr + key_length - <span class="number">8</span>);</span><br><span class="line">      <span class="keyword">switch</span> (<span class="built_in">static_cast</span>&lt;ValueType&gt;(tag &amp; <span class="number">0xff</span>)) &#123;</span><br><span class="line">        <span class="keyword">case</span> kTypeValue: &#123;</span><br><span class="line">          Slice v = <span class="built_in">GetLengthPrefixedSlice</span>(key_ptr + key_length);</span><br><span class="line">          value-&gt;<span class="built_in">assign</span>(v.<span class="built_in">data</span>(), v.<span class="built_in">size</span>());</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">case</span> kTypeDeletion:</span><br><span class="line">          *s = Status::<span class="built_in">NotFound</span>(<span class="built_in">Slice</span>());</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>We know that the skiplist’s Seek method positions the iterator at the <strong>first position in the list that is greater than or equal to the target internal key</strong>. Therefore, we need to additionally verify that the key of this entry is consistent with the user’s query key. This is because there may be multiple keys with the same prefix, and Seek may return a different key that has the same prefix as the query key. For example, querying for “app” might return a record for “apple”.</p><p>The comments also specifically mention that we <strong>do not check the sequence number in the internal key. Why is that?</strong> As mentioned earlier, the keys in the skiplist are sorted based on the internal key comparator (<code>InternalKeyComparator</code>), which considers both the key value and the sequence number. First, <strong>it uses the user-defined comparison function (defaulting to lexicographical order) to compare the user keys</strong>, with smaller key values coming first. If the user keys are the same, it then compares the sequence numbers, with <strong>records having larger sequence numbers appearing earlier in the skiplist</strong>. This is because we usually want newer changes (i.e., records with larger sequence numbers) for the same user key to be accessed first.</p><p>For example, if there are two internal keys, Key1 &#x3D; “user_key1”, Seq &#x3D; 1002 and Key2 &#x3D; “user_key1”, Seq &#x3D; 1001. In the skiplist, the first record (Seq &#x3D; 1002) will be placed before the second record (Seq &#x3D; 1001), because 1002 &gt; 1001. When Seek is used to find &lt;Key &#x3D; user_key1, Seq &#x3D; 1001&gt;, it will naturally skip the record with Seq &#x3D; 1002.</p><p>So, after getting the internal key, there is no need to check the sequence number again. We only need to confirm that the user keys are equal, then get the 64-bit tag and extract the low 8-bit operation type using <code>0xff</code>. For a delete operation, it will return a “not found” status, indicating that the key-value has been deleted. For a value operation, it will then decode the value byte string from the end of the memtable key and assign it to the value pointer.</p><h2 id="Friend-Class-Declaration"><a href="#Friend-Class-Declaration" class="headerlink" title="Friend Class Declaration"></a>Friend Class Declaration</h2><p>In addition to the Add and Get methods, the MemTable class also declares a friend class friend class <code>MemTableBackwardIterator;</code>. As the name suggests, it is a reverse iterator. However, this class definition is not found anywhere in the entire code repository. It’s possible that this was a feature reserved during development that was never implemented, and the invalid code was forgotten to be removed. The compiler doesn’t report an error here because the <strong>C++ compiler does not require a friend class to be defined when processing a friend declaration</strong>. The compiler only checks the syntax correctness of the declaration. The lack of a definition only becomes an issue when the class is actually used (e.g., creating an instance or accessing its members).</p><p>There is also another friend, friend class <code>MemTableIterator;</code>, which implements the Iterator interface for traversing the key-value pairs in the memTable. The methods of MemTableIterator, such as key() and value(), rely on operations on the internal iterator iter_, which works directly on the memTable’s SkipList. These are all private members of memTable, so it needs to be declared as a friend class.</p><p>In db_impl.cc, when the immemtable needs to be flushed to an SST file at Level0, MemTableIterator is used to traverse the key-value pairs in the memTable. The usage part of the code is as follows, where BuildTable traverses the memTable and writes the key-value pairs to the SST file.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// db/db_impl.cc</span></span><br><span class="line"><span class="function">Status <span class="title">DBImpl::WriteLevel0Table</span><span class="params">(MemTable* mem, VersionEdit* edit,</span></span></span><br><span class="line"><span class="params"><span class="function">                                Version* base)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  Iterator* iter = mem-&gt;<span class="built_in">NewIterator</span>();</span><br><span class="line">  <span class="built_in">Log</span>(options_.info_log, <span class="string">&quot;Level-0 table #%llu: started&quot;</span>,</span><br><span class="line">      (<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span>)meta.number);</span><br><span class="line"></span><br><span class="line">  Status s;</span><br><span class="line">  &#123;</span><br><span class="line">    mutex_.<span class="built_in">Unlock</span>();</span><br><span class="line">    s = <span class="built_in">BuildTable</span>(dbname_, env_, options_, table_cache_, iter, &amp;meta);</span><br><span class="line">    mutex_.<span class="built_in">Lock</span>();</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>When traversing the memtable, a friend class is used. <strong>Why not just provide some public interfaces for traversal?</strong> One advantage of using a friend class is that the responsibilities of the classes are clearly divided. MemTableIterator is responsible for traversing the data in the memTable, while memTable is responsible for managing the storage of the data. This separation helps to clearly define the responsibilities of the classes, following the single responsibility principle, where each class handles only a specific set of tasks, making the system design more modular.</p><h2 id="Memory-Management"><a href="#Memory-Management" class="headerlink" title="Memory Management"></a>Memory Management</h2><p>Finally, let’s look at the memory management of MemTable. The MemTable class has a member variable arena_ of type Arena, which is used to manage the memory allocation for the skiplist. When a key-value pair is inserted, the encoded information is stored in the memory allocated by arena_. For information on the Arena class for memory management, you can refer to <a href="https://selfboot.cn/en/2024/08/29/leveldb_source_utils/#Memory-Management-Arena">LevelDB Explained - Arena, Random, CRC32, and More.</a>.</p><p>To release memory promptly when the MemTable is no longer in use, a <strong>reference counting</strong> mechanism is introduced to manage memory. Reference counting allows shared access to the MemTable without worrying about resource release issues. It also provides Ref and Unref methods to increase and decrease the reference count:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Increase reference count.</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Ref</span><span class="params">()</span> </span>&#123; ++refs_; &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Drop reference count.  Delete if no more references exist.</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Unref</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  --refs_;</span><br><span class="line">  <span class="built_in">assert</span>(refs_ &gt;= <span class="number">0</span>);</span><br><span class="line">  <span class="keyword">if</span> (refs_ &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">delete</span> <span class="keyword">this</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>When the reference count drops to zero, the MemTable automatically deletes itself, at which point the destructor <code>~MemTable()</code> is called to release the memory. When an object is destructed, for custom member variables, <strong>their respective destructors are called to release resources</strong>. In MemTable, a skiplist is used to store keys, and the memory for the skiplist is managed by Arena arena_. During the destruction of MemTable, the destructor of arena_ is called to release the previously allocated memory.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Arena::~<span class="built_in">Arena</span>() &#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; blocks_.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">    <span class="keyword">delete</span>[] blocks_[i];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>It is worth noting here that MemTable sets its destructor ~MemTable(); to private, forcing external code to manage the lifecycle of MemTable through the Unref() method. This ensures that the reference counting logic is executed correctly, preventing memory errors caused by improper deletion operations.</p><h3 id="Answering-Questions"><a href="#Answering-Questions" class="headerlink" title="Answering Questions"></a>Answering Questions</h3><p>Now, there is one last question, the one we left earlier. In the LevelDB Get method, why are two local pointers created to reference the member variables mem_ and imm_ instead of using them directly?</p><p><strong>What would be the problem if mem_ and imm_ were used directly?</strong> First, consider the case without locking. If a read thread is reading mem_, and another write thread happens to fill mem_, triggering the logic to switch mem_ to imm_, a new empty mem_ will be created. At this point, the memory address the read thread is reading from becomes invalid. Of course, you could add a lock to protect both reads and writes to mem_ and imm_, but this would result in poor concurrency performance, as only one read or write operation would be allowed at a time.</p><p>To support concurrency, LevelDB’s approach here is more complex. When reading, it first acquires a thread lock, copies mem_ and imm_, and increases their reference counts using Ref(). After that, the thread lock can be released, and the find operation can be performed on the copied mem and imm. This find operation does not require a thread lock, allowing multiple read threads to operate concurrently. After the read is complete, Unref() is called to decrease the reference count. If the reference count becomes zero, the object is destroyed.</p><p><strong>Consider multiple read threads reading mem_ while one write thread is writing to mem_</strong>. Each read thread will first get its own reference to mem_, then release the lock and start the find operation. The write thread can continue to write content to it, or create a new mem_ after it is full. As long as any read thread is still searching, the reference count of the original mem_ will not be zero, and the memory address will remain valid. Only after all read threads have finished, and the write thread has filled mem_, converted it to imm_, and written it to an SST file, will the reference count of the original mem_ become zero. At this point, the destruction operation is triggered, and the address can be reclaimed.</p><p>The text might be a bit confusing, so I had an AI generate a mermaid flowchart to help understand:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250611_leveldb_source_memtable_life_mermaid_en.webp" alt="LevelDB MemTable Lifecycle Diagram"></p><p>The mermaid source code can be found <a href="/downloads/mermaid_leveldb_source_memtable_en.txt">here</a>.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In the entire LevelDB architecture, MemTable plays a pivotal role, connecting the upper and lower layers. It receives write requests from the upper layer, accumulates them in memory to a certain amount, then transforms into an immutable Immutable MemTable, which is eventually written to disk as an SST file by a background thread. At the same time, it is the highest priority component in the read path, ensuring that the most recently written data can be read immediately.</p><p>In this article, we have analyzed the implementation principles and working mechanism of MemTable in LevelDB in detail. Finally, let’s briefly summarize the core design of MemTable:</p><ol><li><strong>Skiplist-based implementation</strong>: MemTable uses a skiplist internally to store data. This data structure provides most of the advantages of a balanced tree, while being simpler to implement and efficiently supporting find and insert operations.</li><li><strong>Memory management mechanism</strong>: MemTable uses the Arena memory allocator to manage memory, allocating and deallocating uniformly to avoid memory fragmentation and improve memory utilization.</li><li><strong>Reference counting mechanism</strong>: The <code>Ref()</code> and <code>Unref()</code> methods implement reference counting to support concurrent access while ensuring that resources are released promptly when no longer in use.</li><li><strong>Specific key-value encoding format</strong>: The key-value pairs stored in MemTable use a specific encoding format, including key length, user key, sequence number and type identifier, value length, and the value itself, which supports LevelDB’s multi-version concurrency control (MVCC).</li><li><strong>Friend class collaboration</strong>: The friend class <code>MemTableIterator</code> is used to traverse the data in MemTable, implementing the principle of separation of concerns.</li></ol><p>Through meticulous memory management and a reference counting mechanism, MemTable solves the problem of concurrent access. With its skiplist data structure, it achieves efficient queries and insertions. Through a specific key-value encoding format, it supports multi-version concurrency control. These design choices together form the foundation of LevelDB’s high performance and reliability.</p>]]></content:encoded>
      
      
      <category domain="https://selfboot.cn/categories/Source-Code-Analysis/">Source Code Analysis</category>
      
      
      <category domain="https://selfboot.cn/tags/C/">C++</category>
      
      <category domain="https://selfboot.cn/tags/LevelDB/">LevelDB</category>
      
      
      <comments>https://selfboot.cn/en/2025/06/11/leveldb_source_memtable/#disqus_thread</comments>
      
    </item>
    
    
    
    
    
    <item>
      <title>LevelDB Explained - Understanding Multi-Version Concurrency Control (MVCC)</title>
      <link>https://selfboot.cn/en/2025/06/10/leveldb_mvcc_intro/</link>
      <guid>https://selfboot.cn/en/2025/06/10/leveldb_mvcc_intro/</guid>
      <pubDate>Tue, 10 Jun 2025 17:47:42 GMT</pubDate>
      
      <description>This article provides an in-depth analysis of how LevelDB implements concurrency control through MVCC, with detailed explanations of versioned key data structure design, sorting rules, and implementation details of read/write processes. It explains how LevelDB embeds sequence numbers and type information in keys to achieve multi-version data management, allowing read operations to obtain consistent views without locking, while write operations create new versions rather than overwriting existing data. Through analysis of actual code, the article showcases internal key sorting methods, the working principles of the Snapshot mechanism, and version control logic in key-value read/write processes, helping readers understand the practical engineering implementation of concurrency control in modern database systems.</description>
      
      
      
      <content:encoded><![CDATA[<p>In database systems, concurrent access is a common scenario. When multiple users read and write to a database simultaneously, ensuring the correctness of each person’s read and write results becomes a challenge that concurrency control mechanisms need to address.</p><p>Consider a simple money transfer scenario: account A initially has <code>$1000</code> and needs to transfer <code>$800</code> to account B. The transfer process includes two steps: deducting money from account A and adding money to account B. If someone queries the balances of accounts A and B between these two steps, what would they see?</p><p>Without any concurrency control, the query would reveal an anomaly: account A has been debited by <code>$800</code>, leaving only <code>$200</code>, while account B hasn’t yet received the transfer, still showing its original amount! This is a typical data inconsistency problem. To solve this issue, database systems <strong>need some form of concurrency control mechanism</strong>.</p><span id="more"></span><p>The most intuitive solution is locking – when someone is performing a write operation (such as a transfer), others’ read operations must wait. Returning to our example, only after both steps of the transfer are completed can users query the correct account balances. However, locking mechanisms have obvious drawbacks: whenever a key is being written, all read operations on that key must wait in line, limiting concurrency and resulting in poor performance.</p><p>Modern database systems widely adopt MVCC for concurrency control, and LevelDB is no exception. Let’s examine LevelDB’s MVCC implementation through its source code.</p><h2 id="Concurrency-Control-Through-MVCC"><a href="#Concurrency-Control-Through-MVCC" class="headerlink" title="Concurrency Control Through MVCC"></a>Concurrency Control Through MVCC</h2><p>MVCC (<a href="https://en.wikipedia.org/wiki/Multiversion_concurrency_control">Multi-Version Concurrency Control</a>) is a concurrency control mechanism that enables concurrent access by maintaining multiple versions of data. Simply put, LevelDB’s MVCC implementation has <strong>several key points</strong>:</p><ul><li><strong>Each key can have multiple versions</strong>, each with its own sequence number;</li><li><strong>Write operations create new versions instead of directly modifying existing data</strong>. Different writes need to be mutually exclusive with locks to ensure each write gets an incremental version number;</li><li><strong>Different read operations can run concurrently without locks</strong>. Multiple read operations can also run concurrently with write operations without locks;</li><li>Isolation between reads and writes, or between different reads, is achieved through snapshots, with read operations always seeing data versions from a specific point in time.</li></ul><p>This is the core idea of MVCC. Let’s understand how MVCC works through a specific operation sequence. Assume we have the following operations:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Time T1: <span class="attribute">sequence</span>=100, write <span class="attribute">key</span>=A, <span class="attribute">value</span>=1</span><br><span class="line">Time T2: <span class="attribute">sequence</span>=101, write <span class="attribute">key</span>=A, <span class="attribute">value</span>=2</span><br><span class="line">Time T3: Reader1 gets <span class="attribute">snapshot</span>=101</span><br><span class="line">Time T4: <span class="attribute">sequence</span>=102, write <span class="attribute">key</span>=A, <span class="attribute">value</span>=3</span><br><span class="line">Time T5: Reader2 gets <span class="attribute">snapshot</span>=102</span><br></pre></td></tr></table></figure><p>Regardless of whether Reader1 or Reader2 reads first, Reader1 reading key&#x3D;A will always get value&#x3D;2 (sequence&#x3D;101), while Reader2 reading key&#x3D;A will get value&#x3D;3 (sequence&#x3D;102). If there are subsequent reads without specifying a snapshot, they will get the latest data. The sequence diagram below makes this easier to understand; the mermaid source code is <a href="/downloads/mermaid_leveldb_mvcc_en.txt">here</a>:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250610_leveldb_mvcc_intro_r_w_en.webp" alt="LevelDB Read&#x2F;Write MVCC Operation Sequence Diagram"></p><p>The overall effect of MVCC is as shown above, which is relatively straightforward. Now let’s look at how MVCC is implemented in LevelDB.</p><h2 id="LevelDB’s-Versioned-Key-Format"><a href="#LevelDB’s-Versioned-Key-Format" class="headerlink" title="LevelDB’s Versioned Key Format"></a>LevelDB’s Versioned Key Format</h2><p>A prerequisite for implementing MVCC is that <strong>each key maintains multiple versions</strong>. Therefore, we need to design a data structure that associates keys with version numbers. LevelDB’s key format is as follows:</p><blockquote><p>[key][sequence&lt;&lt;8|type]</p></blockquote><p>LevelDB’s approach is quite easy to understand – it appends version information to the original key. This version information is a 64-bit unsigned integer, with the high 56 bits storing the sequence and the low 8 bits storing the operation type. Currently, there are only two operation types, corresponding to write and delete operations.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Value types encoded as the last component of internal keys.</span></span><br><span class="line"><span class="comment">// DO NOT CHANGE THESE ENUM VALUES: they are embedded in the on-disk</span></span><br><span class="line"><span class="comment">// data structures.</span></span><br><span class="line"><span class="keyword">enum</span> <span class="title class_">ValueType</span> &#123; kTypeDeletion = <span class="number">0x0</span>, kTypeValue = <span class="number">0x1</span> &#125;;</span><br></pre></td></tr></table></figure><p>Since the sequence number is only 56 bits, it can support at most $ 2^{56} $ writes. Could this be a problem? Would it fail if I want to <span style="color:red">write more keys</span>? Theoretically yes, but let’s analyze from a practical usage perspective. Assuming 1 million writes per second (which is already a very high write QPS), the duration this system could sustain writes would be:</p><p>$$ 2^{56} &#x2F; 1000000 &#x2F; 3600 &#x2F; 24 &#x2F; 365 &#x3D; 2284 $$ </p><p>Well… it can handle writes for over 2000 years, so this sequence number is sufficient, and there’s no need to worry about depletion. Although the data format design is quite simple, it has several advantages:</p><ol><li><strong>The same key supports different versions</strong> – when the same key is written multiple times, the most recent write will have a higher sequence number. Concurrent reads of older versions of this key are supported during writes.</li><li>The type field distinguishes between normal writes and deletions, so deletion doesn’t actually remove data but writes a deletion marker, with actual deletion occurring only during compaction.</li></ol><p>We know that keys in LevelDB are stored in sequence. When querying a single key, binary search can quickly locate it. When obtaining a series of consecutive keys, binary search can quickly locate the starting point of the range, followed by sequential scanning. But now that we’ve added version numbers to the keys, the question arises: <strong>how do we sort keys with version numbers</strong>?</p><h3 id="Internal-Key-Sorting-Method"><a href="#Internal-Key-Sorting-Method" class="headerlink" title="Internal Key Sorting Method"></a>Internal Key Sorting Method</h3><p>LevelDB’s approach is relatively simple and effective, with the following sorting rules:</p><ol><li>First, sort by key in ascending order, using lexicographical ordering of strings</li><li>Then, sort by sequence number in descending order, with larger sequence numbers coming first</li><li>Finally, sort by type in descending order, with write types coming before deletion types</li></ol><p>To implement these sorting rules, LevelDB created its own comparator in <a href="https://github.com/google/leveldb/blob/main/db/dbformat.cc#L47">db&#x2F;dbformat.cc</a>, with the following code:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">InternalKeyComparator::Compare</span><span class="params">(<span class="type">const</span> Slice&amp; akey, <span class="type">const</span> Slice&amp; bkey)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Order by:</span></span><br><span class="line">  <span class="comment">//    increasing user key (according to user-supplied comparator)</span></span><br><span class="line">  <span class="comment">//    decreasing sequence number</span></span><br><span class="line">  <span class="comment">//    decreasing type (though sequence# should be enough to disambiguate)</span></span><br><span class="line">  <span class="type">int</span> r = user_comparator_-&gt;<span class="built_in">Compare</span>(<span class="built_in">ExtractUserKey</span>(akey), <span class="built_in">ExtractUserKey</span>(bkey));</span><br><span class="line">  <span class="keyword">if</span> (r == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">uint64_t</span> anum = <span class="built_in">DecodeFixed64</span>(akey.<span class="built_in">data</span>() + akey.<span class="built_in">size</span>() - <span class="number">8</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">uint64_t</span> bnum = <span class="built_in">DecodeFixed64</span>(bkey.<span class="built_in">data</span>() + bkey.<span class="built_in">size</span>() - <span class="number">8</span>);</span><br><span class="line">    <span class="keyword">if</span> (anum &gt; bnum) &#123;</span><br><span class="line">      r = <span class="number">-1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (anum &lt; bnum) &#123;</span><br><span class="line">      r = <span class="number">+1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>We can see that it first removes the last 8 bits from the versioned key to get the actual user key, and then compares according to the user key’s sorting rules. Additionally, LevelDB provides a default user key comparator, <code>leveldb.BytewiseComparator</code>, which compares keys based entirely on their byte sequence. The comparator implementation code is in <a href="https://github.com/google/leveldb/blob/main/util/comparator.cc#L21">util&#x2F;comparator.cc</a>:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BytewiseComparatorImpl</span> : <span class="keyword">public</span> Comparator &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">BytewiseComparatorImpl</span>() = <span class="keyword">default</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">const</span> <span class="type">char</span>* <span class="title">Name</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">override</span> </span>&#123; <span class="keyword">return</span> <span class="string">&quot;leveldb.BytewiseComparator&quot;</span>; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">Compare</span><span class="params">(<span class="type">const</span> Slice&amp; a, <span class="type">const</span> Slice&amp; b)</span> <span class="type">const</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a.<span class="built_in">compare</span>(b);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// ... </span></span><br></pre></td></tr></table></figure><p>Here, Slice is a string class defined in LevelDB to represent a string, and its compare method performs a byte-by-byte comparison. In fact, LevelDB also supports user-defined comparators – users just need to implement the Comparator interface. When using comparators, BytewiseComparator is wrapped in a singleton, with code that might be a bit difficult to understand:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">const</span> Comparator* <span class="title">BytewiseComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="type">static</span> NoDestructor&lt;BytewiseComparatorImpl&gt; singleton;</span><br><span class="line">  <span class="keyword">return</span> singleton.<span class="built_in">get</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>I previously wrote an article specifically explaining the NoDestructor template class, for those interested: <a href="https://selfboot.cn/en/2024/07/22/leveldb_source_nodestructor/">LevelDB Explained - Preventing C++ Object Destruction</a>.</p><p>The benefits of this sorting approach are evident: first, sorting user keys in ascending order makes range queries highly efficient. When users need to retrieve a series of consecutive keys, they can use binary search to quickly locate the starting point of the range, followed by sequential scanning. Additionally, multiple versions of the same user key are sorted by sequence number in descending order, meaning the newest version comes first, facilitating quick access to the current value. During a query, it only needs to find the first version with a sequence number less than or equal to the current snapshot’s, <strong>without having to scan all versions completely</strong>.</p><p>That’s enough about sorting. Now, let’s look at how keys are assembled during read and write operations.</p><h2 id="Writing-Versioned-Keys"><a href="#Writing-Versioned-Keys" class="headerlink" title="Writing Versioned Keys"></a>Writing Versioned Keys</h2><p>The process of writing key-value pairs in LevelDB is quite complex; you can refer to my previous article: <a href="https://selfboot.cn/en/2025/01/24/leveldb_source_writedb/">LevelDB Explained - Implementation and Optimization Details of Key-Value Writing</a>. Simply put, data is first written to the memtable, then to the immutable memtable, and finally gradually settled (compacted) into SST files at different levels. The first step of the entire process is writing to the memtable, so at the beginning of writing to the memtable, the key is tagged with a version and type, assembled into the versioned internal key format we mentioned earlier.</p><p>The code for assembling the key is in the <code>MemTable::Add</code> function in <a href="https://github.com/google/leveldb/blob/main/db/memtable.cc#L76">db&#x2F;memtable.c</a>. In addition to assembling the key, it also concatenates the value part. The implementation is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">MemTable::Add</span><span class="params">(SequenceNumber s, ValueType type, <span class="type">const</span> Slice&amp; key,</span></span></span><br><span class="line"><span class="params"><span class="function">                   <span class="type">const</span> Slice&amp; value)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Format of an entry is concatenation of:</span></span><br><span class="line">  <span class="comment">//  key_size     : varint32 of internal_key.size()</span></span><br><span class="line">  <span class="comment">//  key bytes    : char[internal_key.size()]</span></span><br><span class="line">  <span class="comment">//  tag          : uint64((sequence &lt;&lt; 8) | type)</span></span><br><span class="line">  <span class="comment">//  value_size   : varint32 of value.size()</span></span><br><span class="line">  <span class="comment">//  value bytes  : char[value.size()]</span></span><br><span class="line">  <span class="type">size_t</span> key_size = key.<span class="built_in">size</span>();</span><br><span class="line">  <span class="type">size_t</span> val_size = value.<span class="built_in">size</span>();</span><br><span class="line">  <span class="type">size_t</span> internal_key_size = key_size + <span class="number">8</span>;</span><br><span class="line">  <span class="type">const</span> <span class="type">size_t</span> encoded_len = <span class="built_in">VarintLength</span>(internal_key_size) +</span><br><span class="line">                             internal_key_size + <span class="built_in">VarintLength</span>(val_size) +</span><br><span class="line">                             val_size;</span><br><span class="line">  <span class="type">char</span>* buf = arena_.<span class="built_in">Allocate</span>(encoded_len);</span><br><span class="line">  <span class="type">char</span>* p = <span class="built_in">EncodeVarint32</span>(buf, internal_key_size);</span><br><span class="line">  std::<span class="built_in">memcpy</span>(p, key.<span class="built_in">data</span>(), key_size);</span><br><span class="line">  p += key_size;</span><br><span class="line">  <span class="built_in">EncodeFixed64</span>(p, (s &lt;&lt; <span class="number">8</span>) | type);</span><br><span class="line">  p += <span class="number">8</span>;</span><br><span class="line">  p = <span class="built_in">EncodeVarint32</span>(p, val_size);</span><br><span class="line">  std::<span class="built_in">memcpy</span>(p, value.<span class="built_in">data</span>(), val_size);</span><br><span class="line">  <span class="built_in">assert</span>(p + val_size == buf + encoded_len);</span><br><span class="line">  table_.<span class="built_in">Insert</span>(buf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here we can see that multiple writes of the same user key produce multiple versions, each with a unique sequence number. Once a user key is converted to an internal key, all subsequent processing is based on this internal key, including MemTable conversion to Immutable MemTable, SST file writing, SST file merging, and so on.</p><p>In the Add function, the length of the internal key is also stored before the internal key itself, and the length and internal key are concatenated and inserted into the MemTable together. This key is actually a memtable_key, which is also used for searching in the memtable during reading.</p><p><strong>Why do we need to store the length here?</strong> We know that the SkipList in Memtable uses const char* pointers as the key type, but these pointers are just raw pointers to certain positions in memory. When the skiplist’s comparator needs to compare two keys, it needs to know the exact range of each key – that is, the start and end positions. If we directly use the internal key, there’s no clear way to know the exact boundaries of an internal key in memory. With length information added, we can quickly locate the boundaries of each key, allowing for correct comparison.</p><h2 id="Key-Value-Reading-Process"><a href="#Key-Value-Reading-Process" class="headerlink" title="Key-Value Reading Process"></a>Key-Value Reading Process</h2><p>Next, let’s look at the process of reading key-values. When reading key-values, the user key is first converted to an internal key, and then a search is performed. However, the first issue here is which sequence number to use. Before answering this question, let’s look at the common method for reading keys:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">std::string newValue;</span><br><span class="line">status = db-&gt;<span class="built_in">Get</span>(leveldb::<span class="built_in">ReadOptions</span>(), <span class="string">&quot;key500&quot;</span>, &amp;newValue);</span><br></pre></td></tr></table></figure><p>There’s a ReadOptions parameter here, which encapsulates a Snapshot object. You can understand this snapshot as the state of the database at a particular point in time, containing all the data before that point but not including writes after that point.</p><p>The core implementation of the snapshot is actually saving the maximum sequence number at a certain point in time. When reading, this sequence number is used to assemble the internal key. There are two scenarios during reading: if no snapshot is specified, the latest sequence number is used; if a previously saved snapshot is used, the sequence number of that snapshot is used.</p><p>Then, based on the snapshot sequence number and user key, assembly occurs. Here, a LookupKey object is first defined to encapsulate some common operations when using internal keys for lookups. The code is in <a href="https://github.com/google/leveldb/blob/main/db/dbformat.h#L184">db&#x2F;dbformat.h</a>:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A helper class useful for DBImpl::Get()</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LookupKey</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">// Initialize *this for looking up user_key at a snapshot with</span></span><br><span class="line">  <span class="comment">// the specified sequence number.</span></span><br><span class="line">  <span class="built_in">LookupKey</span>(<span class="type">const</span> Slice&amp; user_key, SequenceNumber sequence);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">LookupKey</span>(<span class="type">const</span> LookupKey&amp;) = <span class="keyword">delete</span>;</span><br><span class="line">  LookupKey&amp; <span class="keyword">operator</span>=(<span class="type">const</span> LookupKey&amp;) = <span class="keyword">delete</span>;</span><br><span class="line"></span><br><span class="line">  ~<span class="built_in">LookupKey</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Return a key suitable for lookup in a MemTable.</span></span><br><span class="line">  <span class="function">Slice <span class="title">memtable_key</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> <span class="built_in">Slice</span>(start_, end_ - start_); &#125;</span><br><span class="line">  <span class="comment">// Return an internal key (suitable for passing to an internal iterator)</span></span><br><span class="line">  <span class="function">Slice <span class="title">internal_key</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> <span class="built_in">Slice</span>(kstart_, end_ - kstart_); &#125;</span><br><span class="line">  <span class="comment">// Return the user key</span></span><br><span class="line">  <span class="function">Slice <span class="title">user_key</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> <span class="built_in">Slice</span>(kstart_, end_ - kstart_ - <span class="number">8</span>); &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In the LookupKey constructor, the internal key is assembled based on the passed user_key and sequence, with the specific code in <a href="https://github.com/google/leveldb/blob/main/db/dbformat.cc#L117">db&#x2F;dbformat.cc</a>. When searching in the memtable, memtable_key is used, and when searching in the SST, internal_key is used. The memtable_key here is what we mentioned earlier – adding length information before the internal_key to facilitate quick location of each key’s boundaries in the SkipList.</p><p>If the key is not found in the memtable and immutable memtable, it will be searched for in the SST. Searching in the SST is considerably more complex, involving the management of multi-version data. I will write a dedicated article to explain this reading process in the future.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>This article’s explanation of MVCC is still relatively basic, introducing the general concept and focusing on how sequence numbers are processed during read and write operations. It hasn’t delved into multi-version data management or the process of cleaning up old version data. We’ll explore these topics in future articles.</p><p>In summary, LevelDB implements multi-version concurrency control by introducing version numbers into key-values. It achieves read isolation through snapshots, with writes always creating new versions. For read operations, no locks are needed, allowing concurrent reading. For write operations, locks are required to ensure the order of writes.</p><p>This design provides excellent concurrent performance, ensures read consistency, and reduces lock contention. However, the trade-offs are additional storage space overhead and the code complexity that comes with maintaining multiple versions.</p>]]></content:encoded>
      
      
      <category domain="https://selfboot.cn/categories/Source-Code-Analysis/">Source Code Analysis</category>
      
      
      <category domain="https://selfboot.cn/tags/C/">C++</category>
      
      <category domain="https://selfboot.cn/tags/LevelDB/">LevelDB</category>
      
      
      <comments>https://selfboot.cn/en/2025/06/10/leveldb_mvcc_intro/#disqus_thread</comments>
      
    </item>
    
    
    
    
    
    <item>
      <title>In-depth Experience with 3 MCP Servers via Cursor: Impressive but Not Yet Practical?</title>
      <link>https://selfboot.cn/en/2025/05/23/mcp_user_report/</link>
      <guid>https://selfboot.cn/en/2025/05/23/mcp_user_report/</guid>
      <pubDate>Fri, 23 May 2025 11:39:14 GMT</pubDate>
      
      <description>This article details the background and origins of MCP, its relationship with OpenAI Function Calling, and how to configure and use MCP Servers in Cursor. It explores practical scenarios like web automation, GitHub repository analysis, and chart generation. While MCP shows impressive capabilities in GitHub code analysis, web manipulation still has limitations. The article also highlights MCP&#39;s core constraint: its reliance on the LLM&#39;s function calling ability, which is affected by token limits. Although MCP offers a standardized solution for AI tool invocation, its practical value is currently limited, positioning it more as an experimental technology with hopes for significant improvements as model capabilities advance.</description>
      
      
      
      <content:encoded><![CDATA[<p>When Large Language Models (LLMs) first emerged, they primarily generated responses based on pre-trained data. These early models had two main drawbacks:</p><ol><li>They lacked knowledge of recent events. For instance, a model trained in March 2024 wouldn’t know about events in May 2024.</li><li>They couldn’t utilize external tools. “Tools” here can be understood as function calls. For example, if I had a tool function to publish an article, I couldn’t use natural language to make the LLM call this function.</li></ol><p>To address these issues, OpenAI was the first to introduce <code>function calling</code> capabilities in their models, as detailed in their blog post: <a href="https://openai.com/index/function-calling-and-other-api-updates/">Function calling and other API updates</a>.</p><h2 id="Background-Understanding-Function-Calling"><a href="#Background-Understanding-Function-Calling" class="headerlink" title="Background: Understanding Function Calling"></a>Background: Understanding Function Calling</h2><p>At this point, we can inform the model about the tools we have, what parameters each tool requires, what it can do, and what its output will be. When the model receives a specific task, it helps us select the appropriate tool and parse the parameters. We can then execute the corresponding tool and get the result. This process can be iterated, allowing the AI to decide the next steps based on the tool’s output.</p><p>I found an animated GIF online that illustrates the workflow after function calling was introduced:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250522_mcp_user_report_functioncalling.webp" alt="Understanding the function calling process"></p><span id="more"></span><p>Of course, the LLM here only selects the appropriate tool from the list we provide and parses the parameters. It cannot directly call the tools; we need to <strong>program the tool-calling part ourselves</strong>. You can refer to OpenAI’s <a href="https://platform.openai.com/docs/guides/function-calling?api-mode=responses">Function calling documentation</a>.</p><h2 id="Why-Introduce-MCP"><a href="#Why-Introduce-MCP" class="headerlink" title="Why Introduce MCP?"></a>Why Introduce MCP?</h2><p>Function calling alone can already do many things, leading to several interesting projects like <a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a>, arguably the earliest Agent.</p><p>However, there’s a problem: different vendors implement function calling differently, requiring developers to adapt their code for each platform. Developers also need to write code to parse the output of function calling and invoke the corresponding tools. This involves a lot of engineering work, such as handling retries, timeouts, and result parsing.</p><p>In the computer science field, <strong>there’s nothing that can’t be solved by adding an intermediate layer</strong>. After more than a year, with improvements in model capabilities and the enrichment of various external tools, <a href="https://www.anthropic.com/news/model-context-protocol">Anthropic launched the MCP protocol on November 25, 2024</a>. It introduced the MCP Client and MCP Server as an intermediate layer to address the communication issues between LLM applications and external data sources and tools.</p><p>Of course, there have been other solutions to empower models to call external tools, such as OpenAI’s <a href="https://openai.com/index/introducing-the-gpt-store/">ChatGPT Store</a> and the once-popular <a href="https://chatgpt.com/gpts">GPTs</a>, though they seem to be rarely used now.</p><p>Currently, MCP is quite popular. Here’s a diagram to help you understand:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250522_mcp_user_report_whatismcp.webp" alt="Understanding what MCP is"></p><p>This article mainly focuses on practical experience, so we’ll keep the background introduction brief. If you’re interested in MCP development, you can refer to the <a href="https://modelcontextprotocol.io/introduction">official documentation</a>, which is quite detailed.</p><h2 id="How-to-Use-Cursor-MCP"><a href="#How-to-Use-Cursor-MCP" class="headerlink" title="How to Use Cursor MCP"></a>How to Use Cursor MCP</h2><p>Before we start, I’ll briefly introduce how to use MCP with Cursor. Integrating MCP with Cursor is quite convenient. For most MCP Servers that don’t require API keys, a simple configuration is usually enough. MCP is evolving rapidly, so it’s recommended to check <a href="https://docs.cursor.com/context/model-context-protocol">Cursor’s official documentation</a> for the latest information. Many online articles that teach you how to configure them are actually outdated.</p><p>Here, I’ll explain the overall configuration concept to help you understand the documentation. Cursor acts as the AI application here, with a built-in MCP Client, so you don’t need to worry about the Client part. You just need to tell it which MCP Servers you have, and Cursor will automatically call the tools and use their results during chat sessions.</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250522_mcp_user_report_understand.webp" alt="MCP Overall Understanding Diagram"></p><p>Let’s use the diagram above for better understanding. Currently, most AI applications use JSON files to configure MCP Servers, like Claude Desktop. Cursor is no exception; it supports global configuration (<code>~/cursor/mcp.json</code>) and project-specific configuration (<code>.cursor/mcp.json</code>).</p><p>Cursor currently supports two types of MCP Servers: local MCP CLI Stdio Server and remote MCP SSE Server. For SSE, you can refer to my previous article <a href="https://selfboot.cn/en/2024/05/19/stream_sse_chunk/">Understanding Several Stream Output Implementation Methods with Examples</a>. The local CLI method essentially starts a server process on the local machine, and Cursor interacts with this local process via standard input&#x2F;output.</p><p>The local server supports Python, Node services, and Docker containers. However, the prerequisite is that <strong>the corresponding language environment is already installed on the local machine, and the startup commands can be executed</strong>. I’ve provided an example for these three methods:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npx @browsermcp/mcp@latest</span><br><span class="line">uvx mcp-server-browser-use@latest</span><br><span class="line">docker run -i --<span class="built_in">rm</span> -e GITHUB_PERSONAL_ACCESS_TOKEN ghcr.io/github/github-mcp-server</span><br></pre></td></tr></table></figure><p>Sometimes, MCP Servers require additional configurations, such as GitHub API keys. In such cases, you’ll need to configure them manually. Remember to set your keys as environment variables and <strong>never upload your keys to a code repository</strong>.</p><p>For a specific MCP Server, you can refer to its documentation for configuration instructions; it shouldn’t be too difficult. Once the JSON is configured, Cursor will automatically recognize it. You can enable the MCP Server, and if you see a green mark and the listed tools after a while, it means the configuration was successful.</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250522_mcp_user_report_configsucc.webp" alt="Cursor MCP Server Configuration Successful"></p><p>Now, let’s get to the main event: experiencing the practical effects of MCP.</p><h2 id="MCP-Experience-1-Web-Automation"><a href="#MCP-Experience-1-Web-Automation" class="headerlink" title="MCP Experience 1: Web Automation"></a>MCP Experience 1: Web Automation</h2><p>I’ve often thought about using AI to automate web tasks, such as automatically searching a specific site, browsing content on a designated topic, posting replies, or exporting meaningful content. So, I tried Microsoft’s headless Playwright MCP Server and gave it a relatively complex task:</p><blockquote><p>Help me open Taobao, search for Apple iPhones, find the latest model, list the top few stores with the highest sales volume, and export the sales volume and price for each store.</p></blockquote><p>It did open Taobao as expected and found the search box, entering “Apple iPhone,” as shown:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250522_mcp_user_report_taobaocase.webp" alt="MCP Experience 1: Web Automation"></p><p>However, a login pop-up appeared, and Claude 3.7 got stuck here for a long time without resolving it. After running for a while, the final output was:</p><blockquote><p>I noticed we encountered a login pop-up, which might be because the Taobao website requires users to log in to view detailed search results. This is a common security measure on Taobao, especially for non-logged-in users accessing via automated tools.<br>Let me try to cancel the login pop-up by clicking the close button:<br>We see that after clicking the close button, the Taobao website still requires login to view search results. Taobao has strict security restrictions for non-logged-in users, especially when accessed by automated tools.<br>Since logging in is required to view detailed iPhone information, I can try other ways to obtain information about Apple iPhones. For example, I can use web search to find publicly available data on the latest iPhone models, their sales, and prices:</p></blockquote><p>Ultimately, it started searching the web, which was far from my original intention. Handling login states is indeed tricky. I found another <a href="https://browsermcp.io/">MCP Server</a> that supports running in the local browser, allowing AI to operate after logging in. This tool works by installing a browser extension, and the MCP Server controls the local browser.</p><p>After using it, the login state is no longer an issue, but some websites have anti-scraping features that protect page elements. This MCP Server cannot perfectly control them yet, for example, it fails to click buttons, and scrolling is very laggy.</p><p>Actually, LLMs still face considerable difficulties in browser control. A recently popular project, <a href="https://browser-use.com/">Browser Use</a>, attempts to control browsers using not only HTML elements but also visual elements. The overall prospect seems better, and I’ll try a deep dive into it once I have tokens.</p><h2 id="MCP-Experience-2-GitHub-Repository-Analysis"><a href="#MCP-Experience-2-GitHub-Repository-Analysis" class="headerlink" title="MCP Experience 2: GitHub Repository Analysis"></a>MCP Experience 2: GitHub Repository Analysis</h2><p>Next, let’s try the <a href="https://github.com/github/github-mcp-server">GitHub MCP Server</a> from Cursor’s official examples. It supports searching repositories, code, issues, creating PRs, etc. I thought of a scenario where, upon encountering a popular project, AI could first summarize currently hot PRs or Issues, and then see if there are opportunities to contribute. Of course, if the AI could find valuable Issues, then analyze the code, provide solutions, and automatically submit the code, that would be even more valuable.</p><p>But first, let’s break down the problem and start with a low-difficulty information gathering task:</p><blockquote><p>In the LevelDB project, what are the highly discussed pull requests that haven’t been merged yet?</p></blockquote><p>Using Claude 3.7 here, it surprisingly got into a bit of an infinite loop, repeatedly calling the <code>list_pull_requests</code> tool with almost identical parameters:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;owner&quot;</span><span class="punctuation">:</span> <span class="string">&quot;google&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;repo&quot;</span><span class="punctuation">:</span> <span class="string">&quot;leveldb&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;state&quot;</span><span class="punctuation">:</span> <span class="string">&quot;open&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;sort&quot;</span><span class="punctuation">:</span> <span class="string">&quot;updated&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;direction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;desc&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;perPage&quot;</span><span class="punctuation">:</span> <span class="number">30</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>It checked over 10 times without automatically terminating. Since PR checking was unsuccessful, I switched to checking Issues. This time, it worked well, using the <code>list_issues</code> tool. It checked 3 pages, with parameters similar to this:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;owner&quot;</span><span class="punctuation">:</span> <span class="string">&quot;google&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;repo&quot;</span><span class="punctuation">:</span> <span class="string">&quot;leveldb&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;state&quot;</span><span class="punctuation">:</span> <span class="string">&quot;open&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;sort&quot;</span><span class="punctuation">:</span> <span class="string">&quot;comments&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;direction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;desc&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;perPage&quot;</span><span class="punctuation">:</span> <span class="number">10</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;page&quot;</span><span class="punctuation">:</span> <span class="number">3</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>Finally, it provided some conclusions, as shown:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250522_mcp_user_report_leveldbcase.webp" alt="GitHub MCP Issue Information Analysis"></p><p>I checked a few, and there were no major problems. I was quite satisfied with this. For a large project, being able to quickly find highly discussed Issues and then analyze them can indeed be helpful. However, I suspect it didn’t find all of them; there were only 3 pages, but in reality, there are over 200 Issues.</p><p>Then, I continued to focus on one <a href="https://github.com/google/leveldb/pull/917">PR #917</a> and asked it to analyze it for me. Coincidentally, Claude launched the Sonnet 4 model today, so I used this new model for the analysis. I have to say, for such well-defined small problems, AI analysis is very powerful.</p><p>First, it collected the comments on this PR, the code changes in the PR, and also pulled two other Issues mentioned in this PR. <strong>After synthesizing all this information, it provided a detailed analysis</strong>. The analysis was very impressive: it started with a problem description, background, and manifestations, followed by the proposed solution, community discussion points about this solution (such as performance impact and author responses). Finally, it also gave the current status of the PR: submitted in June 2021 and still not merged. This analysis was amazing; it seems I can use this for open-source project issues in the future.</p><p>Here’s a screenshot:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250523_mcp_user_report_githubissue.webp" alt="GitHub MCP PR Information Analysis"></p><p>Of course, after looking at the GitHub MCP Server documentation, I found that it not only provides the ability to read repositories and Issues but also to modify repositories. This includes submitting PRs, creating Issues, creating comments, creating labels, and creating new branches. I haven’t had a chance to deeply explore these features that modify repositories yet; I’ll try them when I get the chance.</p><h2 id="MCP-Experience-3-Chart-Generation"><a href="#MCP-Experience-3-Chart-Generation" class="headerlink" title="MCP Experience 3: Chart Generation"></a>MCP Experience 3: Chart Generation</h2><p>Sometimes, I frequently need to generate good-looking reports from data. Previously, AI even wrote a tool to <a href="https://gallery.selfboot.cn/en/tools/chartrace">generate dynamic bar charts</a>. Now with MCP, I can try letting AI generate charts. There are many cool chart generation libraries, like ECharts. I checked and found no official chart library, but I found an <a href="https://github.com/antvis/mcp-server-chart?tab=readme-ov-file">mcp-server-chart</a> that supports generating ECharts charts.</p><p>Here’s a <a href="https://gallery.selfboot.cn/en/tools/chartrace/dynamic/china_population">dynamic racing bar chart of population changes in Chinese provinces over the last 10 years</a>. I exported some data and then tried generating a chart with the MCP Server.</p><p>I directly gave it a file and prompted:</p><blockquote><p>@china_population.csv Using this Chinese population change data, generate a bar chart of the population of each province in 2022 and 2023.</p></blockquote><p>I used the Claude 4 Sonnet model here, and it successfully called the <code>generate_column_chart</code> tool of <code>mcp-server-chart</code> to generate the chart. However, this tool returns an image URL, which needs to be copied from the output and opened to view. Actually, Cursor supports outputting images as Base64 encoding, so they can be loaded in the chat. The image URL returned by the tool is <a href="https://mdn.alipayobjects.com/one_clip/afts/img/w099SKFp0AMAAAAAAAAAAAAAoEACAQFr/original">here</a>, and the effect is as follows:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250523_mcp_user_report_echart.webp" alt="MCP Generated Bar Chart"></p><p>Then I discovered that this tool supports other types of charts, such as line charts, scatter plots, pie charts, etc. There was one chart whose type I didn’t know, but it looked quite good, so I took a screenshot and gave it to Claude, prompting:</p><blockquote><p>Referring to this image, generate a chart of the population of each province in 2023.</p></blockquote><p>It first analyzed that it was a treemap, then helped me generate the result and explained it. It explained that the largest rectangular block represents Guangdong Province, occupying the largest area, reflecting its status as the most populous province. The generated chart <a href="https://mdn.alipayobjects.com/one_clip/afts/img/6CQ6TKSrI_sAAAAAAAAAAAAAoEACAQFr/original">URL is here</a>, and I’ll display it here as well:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250523_mcp_user_report_echart_treemap.webp" alt="MCP Generated Population Treemap"></p><p>The effect is quite good. Currently, this tool has one Tool per chart type, and the supported chart types are still limited.</p><h2 id="MCP-Usage-Limitations"><a href="#MCP-Usage-Limitations" class="headerlink" title="MCP Usage Limitations"></a>MCP Usage Limitations</h2><p>Current MCP still has some limitations. First, we need to be clear that <strong>the MCP protocol only adds an intermediate layer of Server and Client; it still relies on the LLM’s function calling capability</strong>. And function calling is subject to the LLM’s context length limit; tool descriptions, parameters, etc., all consume tokens.</p><p>When the number of tools is too large or their descriptions are too complex, it might lead to insufficient tokens. Furthermore, even if there are enough tokens, providing too many tool descriptions can degrade the model’s performance. <a href="https://platform.openai.com/docs/guides/function-calling?api-mode=responses#token-usage">OpenAI’s documentation</a> also mentions:</p><blockquote><p>Under the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model’s context limit and are billed as input tokens. If you run into token limits, we suggest limiting the number of functions or the length of the descriptions you provide for function parameters.</p></blockquote><p>Since MCP is based on function calling capabilities, it shares the same limitations. If an MCP server provides too many tools, or if the tool descriptions are too complex, it will affect the actual performance.</p><p>For example, Cursor recommends that enabled MCP Servers provide a maximum of 40 tools. Too many tools can lead to poor model performance, and some models do not support more than 40 tools.</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250522_mcp_user_report_cursor_limit.webp" alt="Cursor MCP Tool Limitation"></p><h2 id="The-Practical-Value-of-MCP"><a href="#The-Practical-Value-of-MCP" class="headerlink" title="The Practical Value of MCP?"></a>The Practical Value of MCP?</h2><p>Alright, now that we’ve introduced the background, usage methods, and limitations of MCP, let’s finally discuss its practical value. There are many MCP Servers on the market. Cursor has an <a href="https://cursor.directory/mcp">MCP Server list page</a>; you can look for them there if needed.</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250522_mcp_user_report_cursor_allmcps.webp" alt="MCP Server List"></p><p>After a quick look, I feel that some MCP Servers might be worth trying out more in the future.</p><ul><li><a href="https://github.com/mendableai/firecrawl-mcp-server">firecrawl-mcp-server</a>: This tool can search web pages and export their content. It also supports searching, in-depth research, and batch crawling. I feel it could be used to gather reference materials when writing articles in the future. The need for web crawling will persist, and there are many similar MCP Servers that can be explored later.</li><li><a href="https://github.com/MiniMax-AI/MiniMax-MCP">MiniMax-MCP</a>: Recently, MiniMax’s speech synthesis topped the charts, and my experience with it was indeed very good. It offers dozens of voice timbres, each very distinctive and sounding almost human. This MCP Server supports calling MiniMax’s synthesis API, which can be used to generate some voice content, even just for novelty.</li><li><a href="https://github.com/ClickHouse/mcp-clickhouse">mcp-clickhouse</a>: If these DB operation-type MCP Servers are powerful enough, they would be great. You could query data just by chatting, which is sufficient for ordinary users. Combined with chart-generating MCP Servers, you could truly visualize data with a single sentence. It’s not just ClickHouse; Mysql, Sqlite, and Redis also have MCP Servers that can be tried later.</li></ul><p>Among the few I’ve tried, some do have nice highlight features, but none have made me feel they offer particularly great value. After the initial novelty, they were shelved. Only the GitHub MCP Server made me think I might use it in the future.</p><p>However, before this article was even finished, the Claude Sonnet 4 model was released, touted as the world’s most powerful programming model. Its reasoning ability has also significantly improved. I’ll need to use it for a while longer to get a real feel for it. Perhaps as model capabilities improve and various MCP Servers continue to be optimized, they will one day become indispensable tools for everyone.</p><p>Do any of you have good use cases for MCP Servers? Feel free to leave comments and discuss.</p>]]></content:encoded>
      
      
      <category domain="https://selfboot.cn/categories/Artificial-Intelligence/">Artificial Intelligence</category>
      
      
      <category domain="https://selfboot.cn/tags/LLM/">LLM</category>
      
      
      <comments>https://selfboot.cn/en/2025/05/23/mcp_user_report/#disqus_thread</comments>
      
    </item>
    
    
    
    
    
    <item>
      <title>LevelDB Explained - Implementation and Optimization Details of Key-Value Writing</title>
      <link>https://selfboot.cn/en/2025/01/24/leveldb_source_writedb/</link>
      <guid>https://selfboot.cn/en/2025/01/24/leveldb_source_writedb/</guid>
      <pubDate>Fri, 24 Jan 2025 18:00:00 GMT</pubDate>
      
      <description>This article provides an in-depth analysis of LevelDB&#39;s write mechanism, detailing the complete process from the Put interface to WAL logging and MemTable persistence. Through source code analysis, it reveals how LevelDB achieves 400,000 writes per second throughput through core technologies like WriteBatch merging strategy, dual MemTable memory management, WAL sequential write optimization, and dynamic Level0 file throttling. It also explores engineering details such as mixed sync write handling, small key-value merge optimization, and data consistency in exceptional scenarios, helping you master the design essence and implementation strategies of LevelDB&#39;s high-performance writing.</description>
      
      
      
      <content:encoded><![CDATA[<p>LevelDB provides a Put interface for writing key-value pairs, which is one of the most important operations in a KV database. The usage is straightforward:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">leveldb::Status status = leveldb::DB::<span class="built_in">Open</span>(options, <span class="string">&quot;./db&quot;</span>, &amp;db);</span><br><span class="line">status = db-&gt;<span class="built_in">Put</span>(leveldb::<span class="built_in">WriteOptions</span>(), key, value);</span><br></pre></td></tr></table></figure><p>One of LevelDB’s greatest advantages is its <strong>extremely fast write speed, supporting high concurrent random writes</strong>. The official <a href="https://github.com/google/leveldb/tree/main?tab=readme-ov-file#write-performance">write performance benchmark</a> shows:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fillseq      :       1.765 micros/op;   62.7 MB/s</span><br><span class="line">fillsync     :     268.409 micros/op;    0.4 MB/s (10000 ops)</span><br><span class="line">fillrandom   :       2.460 micros/op;   45.0 MB/s</span><br><span class="line">overwrite    :       2.380 micros/op;   46.5 MB/s</span><br></pre></td></tr></table></figure><p>As we can see, without forced disk synchronization, random write speed reaches 45.0 MB&#x2F;s, supporting about 400,000 writes per second. With forced disk synchronization, although the write speed decreases significantly, it still maintains around 0.4 MB&#x2F;s, supporting about 3,700 writes per second.</p><p>What exactly happens behind the Put interface? How is data written? What optimizations does LevelDB implement? Let’s explore these questions together.  Before we begin, let’s look at an overview flowchart:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250124_leveldb_source_writedb_flow_en.png" alt="LevelDB Write Process Overview"></p><span id="more"></span><h2 id="Two-Ways-to-Write-Keys-in-LevelDB"><a href="#Two-Ways-to-Write-Keys-in-LevelDB" class="headerlink" title="Two Ways to Write Keys in LevelDB"></a>Two Ways to Write Keys in LevelDB</h2><p>LevelDB supports both single key-value pair writes and batch writes. Internally, both are handled through <a href="https://selfboot.cn/en/2025/01/13/leveldb_source_write_batch/">WriteBatch</a>, regardless of whether it’s a single or batch write.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">DB::Put</span><span class="params">(<span class="type">const</span> WriteOptions&amp; opt, <span class="type">const</span> Slice&amp; key, <span class="type">const</span> Slice&amp; value)</span> </span>&#123;</span><br><span class="line">  WriteBatch batch;</span><br><span class="line">  batch.<span class="built_in">Put</span>(key, value);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">Write</span>(opt, &amp;batch);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>We can choose to aggregate write operations at the application layer when calling LevelDB interfaces to achieve batch writes and improve write throughput. For example, we can design a buffer mechanism at the application layer to collect write requests over a period and then submit them together in a WriteBatch. This approach reduces disk write frequency and context switches, thereby improving performance.</p><p>Alternatively, we can write single key-value pairs each time, and LevelDB will handle them internally through WriteBatch. In high-concurrency situations, multiple write operations might be merged internally before being written to WAL and updated to the memtable.</p><p>The overall write process is quite complex. In this article, we’ll focus on the process of writing to WAL and memtable.</p><h2 id="Detailed-Write-Steps-in-LevelDB"><a href="#Detailed-Write-Steps-in-LevelDB" class="headerlink" title="Detailed Write Steps in LevelDB"></a>Detailed Write Steps in LevelDB</h2><p>The complete write implementation is in the <a href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1205">DBImpl::Write method in leveldb&#x2F;db&#x2F;db_impl.cc</a>. Let’s break it down step by step.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">DBImpl::Write</span><span class="params">(<span class="type">const</span> WriteOptions&amp; options, WriteBatch* updates)</span> </span>&#123;</span><br><span class="line">  <span class="function">Writer <span class="title">w</span><span class="params">(&amp;mutex_)</span></span>;</span><br><span class="line">  w.batch = updates;</span><br><span class="line">  w.sync = options.sync;</span><br><span class="line">  w.done = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function">MutexLock <span class="title">l</span><span class="params">(&amp;mutex_)</span></span>;</span><br><span class="line">  writers_.<span class="built_in">push_back</span>(&amp;w);</span><br><span class="line">  <span class="keyword">while</span> (!w.done &amp;&amp; &amp;w != writers_.<span class="built_in">front</span>()) &#123;</span><br><span class="line">    w.cv.<span class="built_in">Wait</span>();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (w.done) &#123;</span><br><span class="line">    <span class="keyword">return</span> w.status;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The initial part assigns the WriteBatch and sync parameters to the Writer structure and manages multiple Writer structures through a writers_ queue. These two structures and the queue play crucial roles in the entire write process, so let’s examine them first.</p><h3 id="Writer-Structure-and-Processing-Queue"><a href="#Writer-Structure-and-Processing-Queue" class="headerlink" title="Writer Structure and Processing Queue"></a>Writer Structure and Processing Queue</h3><p>Here, <a href="https://github.com/google/leveldb/blob/main/db/db_impl.h#L186">writers_</a> is a queue of type <code>std::deque&lt;Writer*&gt;</code>, used to manage multiple Writer structures.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::deque&lt;Writer*&gt; writers_ <span class="title">GUARDED_BY</span><span class="params">(mutex_)</span></span>;</span><br></pre></td></tr></table></figure><p>The queue is decorated with <code>GUARDED_BY(mutex_)</code>, indicating that access to the queue needs to be protected by the <code>mutex_</code> mutex lock. This uses Clang’s static thread safety analysis feature, which you can learn more about in my previous article <a href="https://selfboot.cn/en/2025/01/02/leveldb_source_thread_anno/">LevelDB Explained - Static Thread Safety Analysis with Clang</a>.</p><p>The Writer structure is defined as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">DBImpl</span>::Writer &#123;</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">Writer</span><span class="params">(port::Mutex* mu)</span></span></span><br><span class="line"><span class="function">      : batch(nullptr), sync(false), done(false), cv(mu) &#123;</span>&#125;</span><br><span class="line"></span><br><span class="line">  Status status;</span><br><span class="line">  WriteBatch* batch;</span><br><span class="line">  <span class="type">bool</span> sync;</span><br><span class="line">  <span class="type">bool</span> done;</span><br><span class="line">  port::CondVar cv;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>The Writer structure encapsulates several parameters, with the most important being a WriteBatch pointer that records the data for each WriteBatch write request. A status field records any error states for each WriteBatch write request.</p><p>Additionally, a sync flag <strong>indicates whether each WriteBatch write request needs to be immediately flushed to disk</strong>. By default, it’s false, meaning no forced disk flush. In this case, if the system crashes, some data that hasn’t been written to disk might be lost. If the sync option is enabled, each write will be immediately flushed to disk. While this increases overall write latency, it ensures that once a write succeeds, the data won’t be lost. For more details about flushing files to disk, you can refer to my previous article <a href="https://selfboot.cn/en/2024/08/02/leveldb_source_env_posixfile/">LevelDB Explained - Posix File Operation Details</a>.</p><p>The <strong>done flag marks whether each WriteBatch write request is completed</strong>. Since multiple WriteBatches might be merged internally, when a write request is merged into another batch, it’s marked as complete to avoid duplicate execution. This improves concurrent write efficiency.</p><p>To <strong>implement waiting and notification, there’s also a condition variable cv, which supports batch processing of multiple write requests and synchronization between them</strong>. During writes, multiple threads can submit write requests simultaneously, with each request being placed in the write queue. <strong>The actual write process is serialized, with only one batch of writes executing at a time</strong>. Each time, the front request from the queue is taken, and if there are other waiting tasks in the queue, they will be merged into one batch for processing. During the processing of the current batch, subsequent requests entering the queue need to wait. When the current batch is completed, waiting write requests in the queue are notified.</p><p>With this introduction, you should understand the meaning of the initial code in the Write method. For each write request, a Writer structure is created and placed in the writers_ queue. Then, in the while loop, it checks if the current write request is complete, returning the write status result if it is. If the current write request isn’t at the front of the queue, it needs to wait on the cv condition variable.</p><p>If the current write request is at the front of the queue, then the actual write operation needs to be executed. What does this specific write process look like?</p><h3 id="Pre-allocating-Space"><a href="#Pre-allocating-Space" class="headerlink" title="Pre-allocating Space"></a>Pre-allocating Space</h3><p>Before the actual write, we need to ensure there’s enough space for the data. This is handled by the <a href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1330">MakeRoomForWrite</a> method, which ensures sufficient resources and space are available before processing new write requests. It manages memtable usage, controls Level 0 file count, and triggers background compaction when needed.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// REQUIRES: this thread is currently at the front of the writer queue</span></span><br><span class="line"><span class="function">Status <span class="title">DBImpl::MakeRoomForWrite</span><span class="params">(<span class="type">bool</span> force)</span> </span>&#123;</span><br><span class="line">  mutex_.<span class="built_in">AssertHeld</span>();</span><br><span class="line">  <span class="built_in">assert</span>(!writers_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="type">bool</span> allow_delay = !force;</span><br><span class="line">  Status s;</span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!bg_error_.<span class="built_in">ok</span>()) &#123;</span><br><span class="line">      <span class="comment">// Yield previous error</span></span><br><span class="line">      s = bg_error_;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The initial part includes some validation: AssertHeld verifies that the current thread holds the mutex_ lock, and the writers_ queue must not be empty. Then it checks if bg_error_ is empty; if not, it returns the bg_error_ status. As we’ll see later, if writing to WAL fails during disk flush, bg_error_ will be set, causing subsequent writes to fail directly.</p><p>In the while loop, there are several if branches handling different situations:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (allow_delay &amp;&amp; versions_-&gt;<span class="built_in">NumLevelFiles</span>(<span class="number">0</span>) &gt;=</span><br><span class="line">                                  config::kL0_SlowdownWritesTrigger) &#123;</span><br><span class="line">      <span class="comment">// We are getting close to hitting a hard limit on the number of</span></span><br><span class="line">      <span class="comment">// L0 files.  Rather than delaying a single write by several</span></span><br><span class="line">      <span class="comment">// seconds when we hit the hard limit, start delaying each</span></span><br><span class="line">      <span class="comment">// individual write by 1ms to reduce latency variance.  Also,</span></span><br><span class="line">      <span class="comment">// this delay hands over some CPU to the compaction thread in</span></span><br><span class="line">      <span class="comment">// case it is sharing the same core as the writer.</span></span><br><span class="line">      mutex_.<span class="built_in">Unlock</span>();</span><br><span class="line">      env_-&gt;<span class="built_in">SleepForMicroseconds</span>(<span class="number">1000</span>);</span><br><span class="line">      allow_delay = <span class="literal">false</span>;  <span class="comment">// Do not delay a single write more than once</span></span><br><span class="line">      mutex_.<span class="built_in">Lock</span>();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>First, when the number of Level 0 files approaches the kL0_SlowdownWritesTrigger&#x3D;8 threshold, it <strong>temporarily releases the lock and delays for 1 millisecond to slow down the write speed</strong>. However, this is only allowed once to avoid blocking a single write for too long. This small Level 0 file count threshold is set to prevent writes from being blocked for too long when the system reaches its bottleneck. Before reaching the bottleneck, it starts distributing the delay across each request to reduce pressure. The comments explain this clearly.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (!force &amp;&amp;</span><br><span class="line">           (mem_-&gt;<span class="built_in">ApproximateMemoryUsage</span>() &lt;= options_.write_buffer_size)) &#123;</span><br><span class="line">  <span class="comment">// There is room in current memtable</span></span><br><span class="line">  <span class="keyword">break</span>;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><p>Next, if the current memtable’s usage hasn’t exceeded its maximum capacity, it returns directly. Here, write_buffer_size is the maximum capacity of the memtable, defaulting to 4MB. This can be configured - a larger value will cache more data in memory, improving write performance, but will use more memory and take longer to recover when reopening the db.</p><p>The next two situations require waiting because there’s no place to write:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (imm_ != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">  <span class="comment">// We have filled up the current memtable, but the previous</span></span><br><span class="line">  <span class="comment">// one is still being compacted, so we wait.</span></span><br><span class="line">  <span class="built_in">Log</span>(options_.info_log, <span class="string">&quot;Current memtable full; waiting...\n&quot;</span>);</span><br><span class="line">  background_work_finished_signal_.<span class="built_in">Wait</span>();</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (versions_-&gt;<span class="built_in">NumLevelFiles</span>(<span class="number">0</span>) &gt;= config::kL0_StopWritesTrigger) &#123;</span><br><span class="line">  <span class="comment">// There are too many level-0 files.</span></span><br><span class="line">  <span class="built_in">Log</span>(options_.info_log, <span class="string">&quot;Too many L0 files; waiting...\n&quot;</span>);</span><br><span class="line">  background_work_finished_signal_.<span class="built_in">Wait</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The first case is when the immutable memtable is still being written, so we need to wait for it to complete. LevelDB maintains two memtables: one current writable memtable (mem_) and one immutable memtable (imm_). When mem_ is full, it becomes imm_ and flushes data to disk. If imm_ hasn’t finished flushing to disk, we must wait before converting the current mem_ to a new imm_.</p><p>The second case is when there are too many Level 0 files, requiring waiting for compaction to complete. LevelDB configures a threshold kL0_StopWritesTrigger for Level 0 file count, defaulting to 12. When exceeded, current write requests must wait. This is because Level 0 files don’t have global sorting guarantees, and multiple Level 0 files might contain overlapping key ranges. For reads, queries need to search all L0 files, and too many files increase read latency. For writes, more files mean more background compaction work, affecting overall system performance. Therefore, Level 0 file count is strictly controlled, blocking writes when the threshold is reached.</p><p>When both imm_ is empty and mem_ doesn’t have enough space, there are several tasks to be done:</p><ol><li><strong>Create new log file</strong>: Generate a new log file number and try to create a new writable file as WAL (Write-Ahead Log). If it fails, reuse the file number and exit the loop, returning an error status.</li><li><strong>Close old log file</strong>: Close the current log file. If closing fails, record the background error to prevent subsequent write operations.</li><li><strong>Update log file pointer</strong>: Set the new log file pointer, update the log number, and create a new log::Writer for writing.</li><li><strong>Convert memtable</strong>: Convert the current memtable to an immutable memtable (imm_), and create a new memtable for writing. Mark the existence of an immutable memtable through has_imm_.store(true, std::memory_order_release).</li><li>Trigger background compaction: Call MaybeScheduleCompaction() to trigger background compaction tasks to process the immutable memtable.</li></ol><p>Here we can see that <strong>memtables and WAL files have a one-to-one correspondence, with each memtable corresponding to a WAL file. The WAL file records all operations written to the memtable, and when the memtable is full, the WAL file is switched simultaneously</strong>. At the same time, the foreground memtable and new WAL log file handle new requests, while the background imm_ and old WAL file handle compaction tasks. Once compaction is complete, the old WAL file can be deleted.</p><h3 id="Merging-Write-Tasks"><a href="#Merging-Write-Tasks" class="headerlink" title="Merging Write Tasks"></a>Merging Write Tasks</h3><p>Next is the logic for merging writes. Here’s the <a href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1224">core code</a>:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">uint64_t</span> last_sequence = versions_-&gt;<span class="built_in">LastSequence</span>();</span><br><span class="line">Writer* last_writer = &amp;w;</span><br><span class="line"><span class="keyword">if</span> (status.<span class="built_in">ok</span>() &amp;&amp; updates != <span class="literal">nullptr</span>) &#123;  <span class="comment">// nullptr batch is for compactions</span></span><br><span class="line">  WriteBatch* write_batch = <span class="built_in">BuildBatchGroup</span>(&amp;last_writer);</span><br><span class="line">  WriteBatchInternal::<span class="built_in">SetSequence</span>(write_batch, last_sequence + <span class="number">1</span>);</span><br><span class="line">  last_sequence += WriteBatchInternal::<span class="built_in">Count</span>(write_batch);</span><br><span class="line"></span><br><span class="line">  &#123;</span><br><span class="line">   <span class="comment">// ... specific writing to WAL and memtable </span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (write_batch == tmp_batch_) tmp_batch_-&gt;<span class="built_in">Clear</span>();</span><br><span class="line"></span><br><span class="line">  versions_-&gt;<span class="built_in">SetLastSequence</span>(last_sequence);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>First, it gets the current global sequence value. Here, <strong>sequence is used to record the version number of written key-value pairs, which increases monotonically globally</strong>. Each write request is assigned a unique sequence value, implementing features like MVCC through the version number mechanism. When writing the current batch of key-value pairs, it first sets the sequence value, and after successful writing, it updates the last_sequence value.</p><p>To <strong>improve write concurrency performance, each write not only needs to write the front task but also attempts to merge subsequent write tasks in the queue</strong>. The merging logic is placed in <a href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1280">BuildBatchGroup</a>, which mainly traverses the entire write queue, <strong>continuously merging subsequent write tasks into the front write task while controlling the overall batch size and ensuring the disk flush level</strong>. The overall constructed write batch is placed in a temporary object tmp_batch_, which is cleared after the complete write operation is finished.</p><p>We mentioned that each write task is actually encapsulated as a WriteBatch object, whose implementation supports merging different write tasks and getting task sizes. For detailed implementation, you can refer to my previous article <a href="https://selfboot.cn/en/2025/01/13/leveldb_source_write_batch/">LevelDB Explained - Elegant Merging of Write and Delete Operations</a>.</p><p>The code above actually omitted the core logic of writing to WAL and memtable, let’s look at this part’s implementation.</p><h3 id="Writing-to-WAL-and-MemTable"><a href="#Writing-to-WAL-and-MemTable" class="headerlink" title="Writing to WAL and MemTable"></a>Writing to WAL and MemTable</h3><p>In LevelDB, when writing key-value pairs, it first writes to the WAL log, then writes to the memtable. The WAL log is key to implementing data recovery in LevelDB, while the memtable is key to implementing memory caching and fast queries. Here’s the critical write code:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Add to log and apply to memtable.  We can release the lock</span></span><br><span class="line"><span class="comment">// during this phase since &amp;w is currently responsible for logging</span></span><br><span class="line"><span class="comment">// and protects against concurrent loggers and concurrent writes</span></span><br><span class="line"><span class="comment">// into mem_.</span></span><br><span class="line">&#123;</span><br><span class="line">  mutex_.<span class="built_in">Unlock</span>();</span><br><span class="line">  status = log_-&gt;<span class="built_in">AddRecord</span>(WriteBatchInternal::<span class="built_in">Contents</span>(write_batch));</span><br><span class="line">  <span class="type">bool</span> sync_error = <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">if</span> (status.<span class="built_in">ok</span>() &amp;&amp; options.sync) &#123;</span><br><span class="line">    status = logfile_-&gt;<span class="built_in">Sync</span>();</span><br><span class="line">    <span class="keyword">if</span> (!status.<span class="built_in">ok</span>()) &#123;</span><br><span class="line">      sync_error = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (status.<span class="built_in">ok</span>()) &#123;</span><br><span class="line">    status = WriteBatchInternal::<span class="built_in">InsertInto</span>(write_batch, mem_);</span><br><span class="line">  &#125;</span><br><span class="line">  mutex_.<span class="built_in">Lock</span>();</span><br><span class="line">  <span class="keyword">if</span> (sync_error) &#123;</span><br><span class="line">    <span class="comment">// The state of the log file is indeterminate: the log record we</span></span><br><span class="line">    <span class="comment">// just added may or may not show up when the DB is re-opened.</span></span><br><span class="line">    <span class="comment">// So we force the DB into a mode where all future writes fail.</span></span><br><span class="line">    <span class="built_in">RecordBackgroundError</span>(status);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, <strong>when writing to WAL and memtable, the mutex_ lock is first released, and then reacquired after completion</strong>. The comments specifically explain that while the current front <code>&amp;w</code> is responsible for writing to WAL and memtable, subsequent write calls can acquire the mutex_ lock to complete queue entry operations. However, since they’re not at the front, they need to wait on the condition variable, and only when the current task completes do they have a chance to execute. Therefore, <strong>although the lock is released during the process of writing to WAL and memtable, the overall write is still serialized</strong>. WAL and memtable themselves don’t need to ensure thread safety.</p><p>However, since writing to WAL and memtable is relatively time-consuming, after releasing the lock, other operations that need mutex_ can acquire it and continue executing, improving overall system concurrency.</p><p>WAL (Write-Ahead Logging) is a logging mechanism that allows recording logs before data is written to disk. <strong>WAL logs are written sequentially, and disk sequential IO performance is better than random IO performance, so sequential writes are generally more efficient</strong>. After successfully writing to WAL, data is placed in the memtable, which is a memory structure with high write efficiency. When enough data accumulates in memory, it’s written to disk. If the system crashes and restarts, data in the memtable may be lost, but through WAL logs, write operations can be replayed to restore the data state, ensuring data integrity.</p><p>The specific write here simply calls the AddRecord method of the log::Writer object log_ to write WriteBatch data. log::Writer will organize this data and write it to disk at appropriate times. For detailed implementation, you can refer to my previous article <a href="https://selfboot.cn/en/2024/08/14/leveldb_source_wal_log/">LevelDB Explained - How To Read and Write WAL Logs</a>.</p><p>Of course, if the write comes with sync&#x3D;true, after successfully writing to WAL, the logfile_-&gt;Sync() method will be called to force disk flush. To clarify, <strong>writing content to files is done through the system call <code>write</code>, but success of this system call doesn’t guarantee the data has been written to disk. File systems generally put data in a buffer first, then choose appropriate times to flush to disk based on circumstances</strong>. To ensure data is written to disk, additional system calls are needed, with different platforms having different interfaces. For details, refer to my previous article <a href="https://selfboot.cn/en/2024/08/02/leveldb_source_env_posixfile/">LevelDB Explained - Posix File Operation Details</a>.</p><p>If an error occurs during forced disk flush, the RecordBackgroundError method is called to record the error status in bg_error_, causing all subsequent write operations to fail directly.</p><p>After successfully writing to WAL, we can write to the memtable. Here, the WriteBatchInternal::InsertInto method is called to insert WriteBatch data into the memtable. I’ll cover the implementation of memtable in detail in a future article.</p><h3 id="Updating-Batch-Write-Task-Status"><a href="#Updating-Batch-Write-Task-Status" class="headerlink" title="Updating Batch Write Task Status"></a>Updating Batch Write Task Status</h3><p>After completing the batch write, we need to update the status of batch write tasks, taking the Writer object from the front of the writers_ queue, then iterating until the last write task in the batch. Here we update the status of all completed tasks and wake up all waiting write tasks. The <a href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1259">core implementation</a> is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">  Writer* ready = writers_.<span class="built_in">front</span>();</span><br><span class="line">  writers_.<span class="built_in">pop_front</span>();</span><br><span class="line">  <span class="keyword">if</span> (ready != &amp;w) &#123;</span><br><span class="line">    ready-&gt;status = status;</span><br><span class="line">    ready-&gt;done = <span class="literal">true</span>;</span><br><span class="line">    ready-&gt;cv.<span class="built_in">Signal</span>();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (ready == last_writer) <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Notify new head of write queue</span></span><br><span class="line"><span class="keyword">if</span> (!writers_.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">  writers_.<span class="built_in">front</span>()-&gt;cv.<span class="built_in">Signal</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Finally, if there are still write tasks in the queue, we need to wake up the front write task to continue processing. At this point, the entire write process is complete, and we can return the write result to the caller.</p><h2 id="Other-Engineering-Implementation-Details"><a href="#Other-Engineering-Implementation-Details" class="headerlink" title="Other Engineering Implementation Details"></a>Other Engineering Implementation Details</h2><p>While we’ve analyzed the complete write process, there are some engineering implementation details worth examining.</p><h3 id="Handling-Mixed-Sync-and-Non-sync-Writes"><a href="#Handling-Mixed-Sync-and-Non-sync-Writes" class="headerlink" title="Handling Mixed Sync and Non-sync Writes"></a>Handling Mixed Sync and Non-sync Writes</h3><p>How does LevelDB internally handle a batch of write requests that includes both sync and non-sync writes?</p><p>From our previous analysis, we can see that after taking the front write task from the queue, it attempts to merge subsequent write tasks in the queue. Since each write task can either force sync disk flush or not, how are write tasks with different sync configurations handled during merging?</p><p>Here, when <strong>sync&#x3D;true is configured, writes will force disk flush. For merged batch writes, the sync setting of the front task is used</strong>. The <a href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1237">core code</a> is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">DBImpl::Write</span><span class="params">(<span class="type">const</span> WriteOptions&amp; options, WriteBatch* updates)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">  <span class="keyword">if</span> (status.<span class="built_in">ok</span>() &amp;&amp; updates != <span class="literal">nullptr</span>) &#123;  <span class="comment">// nullptr batch is for compactions</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    &#123;</span><br><span class="line">      mutex_.<span class="built_in">Unlock</span>();</span><br><span class="line">      status = log_-&gt;<span class="built_in">AddRecord</span>(WriteBatchInternal::<span class="built_in">Contents</span>(write_batch));</span><br><span class="line">      <span class="type">bool</span> sync_error = <span class="literal">false</span>;</span><br><span class="line">      <span class="keyword">if</span> (status.<span class="built_in">ok</span>() &amp;&amp; options.sync) &#123;</span><br><span class="line">        status = logfile_-&gt;<span class="built_in">Sync</span>();</span><br><span class="line">        <span class="keyword">if</span> (!status.<span class="built_in">ok</span>()) &#123;</span><br><span class="line">          sync_error = <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Therefore, if the front task doesn’t require disk flush, then during merging, it cannot merge write tasks with sync&#x3D;true. The <a href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1302">core implementation code</a> is as follows:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (; iter != writers_.<span class="built_in">end</span>(); ++iter) &#123;</span><br><span class="line">  Writer* w = *iter;</span><br><span class="line">  <span class="keyword">if</span> (w-&gt;sync &amp;&amp; !first-&gt;sync) &#123;</span><br><span class="line">    <span class="comment">// Do not include a sync write into a batch handled by a non-sync write.</span></span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>However, if the front task has sync&#x3D;true, then during merging, we don’t need to consider the sync settings of the write tasks being merged. This is because the entire merged batch will be forced to flush to disk. This design <strong>ensures that the write durability guarantee level isn’t reduced while potentially improving it</strong>. Of course, improving the write durability guarantee level here doesn’t actually increase overall latency, because the front task must flush to disk anyway, and including additional write tasks that don’t require disk flush won’t increase the latency.</p><h3 id="Optimizing-Large-Batch-Small-Key-Write-Latency"><a href="#Optimizing-Large-Batch-Small-Key-Write-Latency" class="headerlink" title="Optimizing Large Batch Small Key Write Latency"></a>Optimizing Large Batch Small Key Write Latency</h3><p>As we can see from the above implementation, during large-scale concurrent writes, write requests are first placed in a queue and then written serially. If the keys being written are relatively small, then after taking a write task from the front of the queue, it’s merged with other writes in the current queue into a batch. When merging, a max_size needs to be set to limit the number of merged keys. What’s a reasonable value for this max_size?</p><p>LevelDB provides an empirical value, defaulting to 1 &lt;&lt; 20 bytes. However, consider a scenario where all keys being written are relatively small - during merging, many keys might be merged, leading to longer write latency. <strong>Since these are small key writes, long write latency doesn’t provide a good user experience</strong>.</p><p>Therefore, a small optimization was added: if the overall size of the current front write task is less than 128 &lt;&lt; 10 bytes, then max_size will be much smaller. Of course, this value seems to be empirical as well; I haven’t found official documentation explaining it. The relevant code is in <a href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1289">BuildBatchGroup</a>:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Allow the group to grow up to a maximum size, but if the</span></span><br><span class="line"><span class="comment">// original write is small, limit the growth so we do not slow</span></span><br><span class="line"><span class="comment">// down the small write too much.</span></span><br><span class="line"><span class="type">size_t</span> max_size = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line"><span class="keyword">if</span> (size &lt;= (<span class="number">128</span> &lt;&lt; <span class="number">10</span>)) &#123;</span><br><span class="line">  max_size = size + (<span class="number">128</span> &lt;&lt; <span class="number">10</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="WAL-Success-but-MemTable-Failure"><a href="#WAL-Success-but-MemTable-Failure" class="headerlink" title="WAL Success but MemTable Failure"></a>WAL Success but MemTable Failure</h3><p>During the write process, WAL is written first, and only after successful WAL writing is the memtable written. These two writes aren’t atomic. If WAL writing fails, we don’t write to the memtable and block all subsequent writes, which is reasonable and won’t cause issues.</p><p>However, if WAL writing succeeds but writing to the memtable fails, the status returned to the caller indicates write failure, and the caller might consider the write failed. But since WAL was successfully written and other processes executed normally, including updating the global sequence, <strong>if a failure occurs at this point, during recovery from WAL, the failed memtable writes will also be recovered, resulting in data that the caller thought failed actually being successfully written</strong>.</p><p>I haven’t fully understood this issue yet - it might be a potential bug in LevelDB, or I might not have fully grasped the overall code. I tried analyzing this with GPT O1 and Deepseek’s R1 deep thinking, and both lean towards this being a bug. Deepseek’s thought process was quite interesting:</p><blockquote><p>I need to confirm how likely this situation is in actual LevelDB operation. What typically causes MemTable write failures? Memory shortage or other unrecoverable errors? For temporary errors, does LevelDB have a retry mechanism?</p><p>Also, checking LevelDB’s official documentation or related discussions, is there any mention of how to handle this situation? For example, does LevelDB actively delete or invalidate corresponding WAL records when MemTable writes fail?</p><p>We might also need to consider the database recovery process. When LevelDB restarts, it reads WAL and rebuilds MemTable. If a WAL record corresponds to a failed MemTable write but WAL exists, recovery will attempt to write to MemTable again. If conditions allow (e.g., sufficient memory), the write might succeed, causing inconsistency. However, if the original failure was persistent (like data structure error), recovery might fail again, preventing the database from opening and thus avoiding inconsistency.</p><p>Additionally, does the user’s question assume certain specific scenarios? For example, MemTable write fails but the database doesn’t crash, then closes normally, and WAL isn’t cleared, so it’s reapplied during recovery. Is this scenario possible?</p></blockquote><p>AI’s deep thinking and logical reasoning capabilities are quite strong now, considering issues quite comprehensively. Readers are welcome to discuss this issue in the comments.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>The article is quite long, so let’s summarize. LevelDB’s write operation design fully considers high concurrency and performance optimization, implementing efficient key-value pair writing through a series of ingenious mechanisms. Here are some design aspects worth learning from:</p><ol><li><p><strong>Batch Write Merging</strong>: LevelDB merges multiple write requests through a Writer queue, avoiding frequent disk IO. Each write request is placed in the queue, with the queue head responsible for merging subsequent requests into a large WriteBatch. This design significantly improves throughput, especially suitable for high-concurrency small key-value pair write scenarios.</p></li><li><p><strong>WAL Log Crash Recovery</strong>: Write-Ahead Logging (WAL): All write operations are first sequentially written to WAL logs, ensuring data durability. Only after writing to WAL is the MemTable in memory updated. This “log first, memory second” design is the cornerstone of LevelDB’s crash recovery.</p></li><li><p><strong>Memory Double Buffering</strong>: When MemTable is full, it converts to Immutable MemTable and triggers background compaction while creating a new MemTable and WAL file. This <strong>double buffering mechanism avoids write blocking and achieves smooth memory-to-disk data transfer</strong>.</p></li><li><p><strong>Write Throttling and Adaptive Delay</strong>: Through kL0_SlowdownWritesTrigger and kL0_StopWritesTrigger thresholds, actively introducing write delays or pausing writes when there are too many Level 0 files. This “soft throttling” strategy prevents system avalanche effects after overload.</p></li><li><p><strong>Dynamic Batch Merging</strong>: Dynamically adjusting maximum batch size based on current queue head request size (e.g., 128KB for small requests, 1MB for large requests), balancing throughput and latency.</p></li><li><p><strong>Condition Variable Wake-up</strong>: Implementing efficient thread wait-notify through CondVar, ensuring merged writes don’t block subsequent requests for too long.</p></li><li><p><strong>Mixed Sync Handling</strong>: Supporting simultaneous handling of requests requiring forced disk flush (sync&#x3D;true) and non-forced flush, prioritizing the persistence level of the queue head request without compromising data safety.</p></li><li><p><strong>Error Isolation</strong>: WAL write failures mark global error state bg_error_, directly rejecting all subsequent write requests to prevent data inconsistency.</p></li></ol><p>Finally, welcome to discuss in the comments and learn LevelDB’s implementation details together.</p>]]></content:encoded>
      
      
      <category domain="https://selfboot.cn/categories/Source-Code-Analysis/">Source Code Analysis</category>
      
      
      <category domain="https://selfboot.cn/tags/C/">C++</category>
      
      <category domain="https://selfboot.cn/tags/LevelDB/">LevelDB</category>
      
      
      <comments>https://selfboot.cn/en/2025/01/24/leveldb_source_writedb/#disqus_thread</comments>
      
    </item>
    
    
    
    
    
    <item>
      <title>LevelDB Explained - Elegant Merging of Write and Delete Operations</title>
      <link>https://selfboot.cn/en/2025/01/13/leveldb_source_write_batch/</link>
      <guid>https://selfboot.cn/en/2025/01/13/leveldb_source_write_batch/</guid>
      <pubDate>Mon, 13 Jan 2025 22:00:00 GMT</pubDate>
      
      <description>This article provides an in-depth analysis of WriteBatch design and implementation in LevelDB, detailing how it improves performance through batch write and delete operations. The article explores WriteBatch&#39;s interface design, sequence number mechanism, operation record storage format, and other aspects, examining core functionalities like global sequence number increment, operation counting, and data format validation through source code analysis. Additionally, it demonstrates practical usage scenarios through test cases, making it valuable reading for developers interested in LevelDB or storage system design.</description>
      
      
      
      <content:encoded><![CDATA[<p>LevelDB supports both single key-value writes and batch writes. These two types of operations are essentially handled the same way - they’re both encapsulated in a WriteBatch object, which helps improve write operation efficiency.</p><p>In LevelDB, WriteBatch is implemented using a simple data structure that contains a series of write operations. These operations are serialized (converted to byte streams) and stored in an internal string. Each operation includes an operation type (such as insert or delete), key, and value (for insert operations).</p><p>When a WriteBatch is committed to the database, its contents are parsed and applied to both the WAL log and memtable. Regardless of how many operations a WriteBatch contains, they are processed and logged as a single unit.</p><span id="more"></span><p>WriteBatch’s implementation primarily involves 4 files, let’s examine them:</p><ol><li><a href="https://github.com/google/leveldb/blob/main/include/leveldb/write_batch.h">include&#x2F;leveldb&#x2F;write_batch.h</a>: The public interface file defining the WriteBatch class interface.</li><li><a href="https://github.com/google/leveldb/blob/main/db/write_batch_internal.h">db&#x2F;write_batch_internal.h</a>: Internal implementation file defining the WriteBatchInternal class, providing methods to manipulate WriteBatch.</li><li><a href="https://github.com/google/leveldb/blob/main/db/write_batch.cc">db&#x2F;write_batch.cc</a>: The implementation file for the WriteBatch class.</li><li><a href="https://github.com/google/leveldb/blob/main/db/write_batch_test.cc">db&#x2F;write_batch_test.cc</a>: Test file for WriteBatch functionality.</li></ol><h2 id="WriteBatch-Interface-Design"><a href="#WriteBatch-Interface-Design" class="headerlink" title="WriteBatch Interface Design"></a>WriteBatch Interface Design</h2><p>Let’s first look at write_batch.h, which defines the public interfaces of the WriteBatch class. While LevelDB’s code comments are very clear, we’ll skip them for now:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LEVELDB_EXPORT</span> WriteBatch &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="keyword">class</span> <span class="title class_">LEVELDB_EXPORT</span> Handler &#123;</span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">virtual</span> ~<span class="built_in">Handler</span>();</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">Put</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">const</span> Slice&amp; value)</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">Delete</span><span class="params">(<span class="type">const</span> Slice&amp; key)</span> </span>= <span class="number">0</span>;</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">WriteBatch</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Intentionally copyable.</span></span><br><span class="line">  <span class="built_in">WriteBatch</span>(<span class="type">const</span> WriteBatch&amp;) = <span class="keyword">default</span>;</span><br><span class="line">  WriteBatch&amp; <span class="keyword">operator</span>=(<span class="type">const</span> WriteBatch&amp;) = <span class="keyword">default</span>;</span><br><span class="line"></span><br><span class="line">  ~<span class="built_in">WriteBatch</span>();</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">Put</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">const</span> Slice&amp; value)</span></span>;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">Delete</span><span class="params">(<span class="type">const</span> Slice&amp; key)</span></span>;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">Clear</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function"><span class="type">size_t</span> <span class="title">ApproximateSize</span><span class="params">()</span> <span class="type">const</span></span>;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">Append</span><span class="params">(<span class="type">const</span> WriteBatch&amp; source)</span></span>;</span><br><span class="line">  <span class="function">Status <span class="title">Iterate</span><span class="params">(Handler* handler)</span> <span class="type">const</span></span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="keyword">friend</span> <span class="keyword">class</span> <span class="title class_">WriteBatchInternal</span>;</span><br><span class="line"></span><br><span class="line">  std::string rep_;  <span class="comment">// See comment in write_batch.cc for the format of rep_</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>The <a href="https://github.com/google/leveldb/blob/main/include/leveldb/write_batch.h#L35">WriteBatch::Handler</a> is an abstract base class that defines interfaces for handling key-value operations, containing only Put and Delete methods. This design allows the WriteBatch class implementation to be decoupled from <strong>specific storage operations</strong>, meaning WriteBatch doesn’t need to know directly how to apply operations to underlying storage (like MemTable).</p><p><strong>By inheriting from the Handler class, various handlers can be created that implement these methods differently</strong>. For example:</p><ol><li>MemTableInserter: Defined in db&#x2F;write_batch.cc, stores key-value operations in MemTable.</li><li>WriteBatchItemPrinter: Defined in db&#x2F;dumpfile.cc, prints key-value operations to a file for testing.</li></ol><p>Additionally, there’s a <code>friend class WriteBatchInternal</code> that can access WriteBatch’s private and protected members. <strong>WriteBatchInternal mainly encapsulates internal operations that don’t need to be exposed publicly and are only used internally. By hiding internal operation methods in WriteBatchInternal, the object’s interface remains clean, and internal implementations can be modified freely without affecting code that uses these objects</strong>.</p><h3 id="WriteBatch-Usage"><a href="#WriteBatch-Usage" class="headerlink" title="WriteBatch Usage"></a>WriteBatch Usage</h3><p>At the application level, we can use WriteBatch to write multiple key-value pairs in batch, then write the WriteBatch to the database using the <code>DB::Write</code> method.</p><p>WriteBatch supports Put and Delete operations and can merge multiple WriteBatches. Here’s a usage example:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">WriteBatch batch;</span><br><span class="line">batch.<span class="built_in">Put</span>(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line">batch.<span class="built_in">Put</span>(<span class="string">&quot;key2&quot;</span>, <span class="string">&quot;value2&quot;</span>);</span><br><span class="line">batch.<span class="built_in">Delete</span>(<span class="string">&quot;key3&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Merge another batch</span></span><br><span class="line">WriteBatch another_batch;</span><br><span class="line">another_batch.<span class="built_in">Put</span>(<span class="string">&quot;key4&quot;</span>, <span class="string">&quot;value4&quot;</span>);</span><br><span class="line">batch.<span class="built_in">Append</span>(another_batch);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Write to database</span></span><br><span class="line">db-&gt;<span class="built_in">Write</span>(writeOptions, &amp;batch);</span><br></pre></td></tr></table></figure><h2 id="WriteBatch-Implementation-Details"><a href="#WriteBatch-Implementation-Details" class="headerlink" title="WriteBatch Implementation Details"></a>WriteBatch Implementation Details</h2><p>So how is WriteBatch implemented? The key lies in <a href="https://github.com/google/leveldb/blob/main/db/write_batch.cc">db&#x2F;write_batch.cc</a>, where the class has a private member <code>std::string rep_</code> to store serialized key-value operations. Let’s first look at the storage data protocol:</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">+---------------+---------------+----------------------------------------+</span><br><span class="line">|<span class="string">   Sequence    </span>|<span class="string">     Count     </span>|<span class="string">                Data                    </span>|</span><br><span class="line">|<span class="string">  (8 bytes)    </span>|<span class="string">   (4 bytes)   </span>|<span class="string">                                        </span>|</span><br><span class="line">+---------------+---------------+----------------------------------------+</span><br><span class="line">                                   |<span class="string">                 </span>|<span class="string">                   </span>|</span><br><span class="line">                                   v                 v                   v</span><br><span class="line">                               +-------+         +-------+          +-------+</span><br><span class="line">                               |<span class="string">Record1</span>|<span class="string">         </span>|<span class="string">Record2</span>|<span class="string">   ...    </span>|<span class="string">RecordN</span>|</span><br><span class="line">                               +-------+         +-------+          +-------+</span><br><span class="line">                                  |<span class="string">                 </span>|</span><br><span class="line">                                  v                 v</span><br><span class="line">                        +-----------------+ +-----------------+</span><br><span class="line">                        |<span class="string"> kTypeValue      </span>|<span class="string"> </span>|<span class="string"> kTypeDeletion   </span>|</span><br><span class="line">                        |<span class="string"> Varstring Key   </span>|<span class="string"> </span>|<span class="string"> Varstring Key   </span>|</span><br><span class="line">                        |<span class="string"> Varstring Value </span>|<span class="string"> </span>|<span class="string">                 </span>|</span><br><span class="line">                        +-----------------+ +-----------------+</span><br><span class="line">                        </span><br><span class="line">Varstring (variable-length string):</span><br><span class="line">+-------------+-----------------------+</span><br><span class="line">|<span class="string"> Length (varint32) </span>|<span class="string"> Data (uint8[])  </span>|</span><br><span class="line">+-------------+-----------------------+</span><br></pre></td></tr></table></figure><p>The first 12 bytes of this string are header metadata, including 8 bytes for sequence number and 4 bytes for count. Following that are one or more operation records, each containing an operation type and key-value pair. The operation type is a single byte, which can be either Put or Delete. Keys and values are variable-length strings in varstring format.</p><h3 id="LevelDB’s-Sequence-Number-Mechanism"><a href="#LevelDB’s-Sequence-Number-Mechanism" class="headerlink" title="LevelDB’s Sequence Number Mechanism"></a>LevelDB’s Sequence Number Mechanism</h3><p>The first 8 bytes of rep_ represent a 64-bit sequence number. The WriteBatchInternal friend class provides two methods to get and set the sequence number, internally using <a href="https://selfboot.cn/en/2024/08/29/leveldb_source_utils/#Integer-Encoding-and-Decoding">EncodeFixed64 and DecodeFixed64</a> methods to encode and decode the 64-bit sequence number.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">SequenceNumber <span class="title">WriteBatchInternal::Sequence</span><span class="params">(<span class="type">const</span> WriteBatch* b)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">SequenceNumber</span>(<span class="built_in">DecodeFixed64</span>(b-&gt;rep_.<span class="built_in">data</span>()));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">WriteBatchInternal::SetSequence</span><span class="params">(WriteBatch* b, SequenceNumber seq)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">EncodeFixed64</span>(&amp;b-&gt;rep_[<span class="number">0</span>], seq);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Sequence numbers are globally incrementing identifiers in LevelDB, used for version control and operation ordering</strong>. Each WriteBatch receives a consecutive range of sequence numbers during execution, with each operation (Put&#x2F;Delete) within the batch being assigned one of these numbers. Sequence numbers serve three core purposes in LevelDB:</p><ol><li><p><strong>Version Control</strong>: Each key in LevelDB can have multiple versions, each corresponding to a sequence number. When reading, sequence numbers are compared to determine which version to return. Higher sequence numbers indicate newer versions.</p></li><li><p><strong>Multi-Version Concurrency Control (MVCC)</strong>: Write operations get new sequence numbers and create new versions of keys. Read operations can specify a sequence number to access data snapshots at that point in time. This mechanism allows concurrent execution of read and write operations without blocking each other.</p></li><li><p><strong>Crash Recovery</strong>: WAL (Write-Ahead Log) records operation sequence numbers. During system restart, sequence numbers help accurately rebuild the data state at the time of crash, avoiding duplicate application of already persisted operations.</p></li></ol><p>This design allows LevelDB to maintain data consistency while implementing efficient concurrency control.</p><p>The sequence number setting logic is in the <a href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1222">DBImpl::Write</a> method, which first gets the current maximum sequence number, then allocates a new sequence number for the WriteBatch.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">DBImpl::Write</span><span class="params">(<span class="type">const</span> WriteOptions&amp; options, WriteBatch* updates)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="type">uint64_t</span> last_sequence = versions_-&gt;<span class="built_in">LastSequence</span>();</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="keyword">if</span> (status.<span class="built_in">ok</span>() &amp;&amp; updates != <span class="literal">nullptr</span>) &#123;  <span class="comment">// nullptr batch is for compactions</span></span><br><span class="line">    WriteBatch* write_batch = <span class="built_in">BuildBatchGroup</span>(&amp;last_writer);</span><br><span class="line">    WriteBatchInternal::<span class="built_in">SetSequence</span>(write_batch, last_sequence + <span class="number">1</span>);</span><br><span class="line">    last_sequence += WriteBatchInternal::<span class="built_in">Count</span>(write_batch);</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>If a WriteBatch contains multiple operations, these operations are assigned sequence numbers consecutively. When writing to the WAL log, the WriteBatch’s sequence number is written to the log, allowing operations to be recovered in order during recovery. After writing to the memtable, the current maximum sequence number is updated for the next allocation.</p><h3 id="Count-for-Operation-Tracking"><a href="#Count-for-Operation-Tracking" class="headerlink" title="Count for Operation Tracking"></a>Count for Operation Tracking</h3><p>The header also includes 4 bytes for count, which records the number of operations in the WriteBatch. The count is incremented for each put or delete operation. For example:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">WriteBatch batch;</span><br><span class="line">batch.<span class="built_in">Put</span>(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);  <span class="comment">// count = 1</span></span><br><span class="line">batch.<span class="built_in">Put</span>(<span class="string">&quot;key2&quot;</span>, <span class="string">&quot;value2&quot;</span>);  <span class="comment">// count = 2</span></span><br><span class="line">batch.<span class="built_in">Delete</span>(<span class="string">&quot;key3&quot;</span>);         <span class="comment">// count = 3</span></span><br><span class="line"><span class="type">int</span> num_ops = WriteBatchInternal::<span class="built_in">Count</span>(&amp;batch);  <span class="comment">// = 3</span></span><br></pre></td></tr></table></figure><p>When merging two WriteBatches, their counts are also accumulated, as shown in the <a href="https://github.com/google/leveldb/blob/main/db/write_batch.cc#L144">WriteBatchInternal::Append</a> method:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">WriteBatchInternal::Append</span><span class="params">(WriteBatch* dst, <span class="type">const</span> WriteBatch* src)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">SetCount</span>(dst, <span class="built_in">Count</span>(dst) + <span class="built_in">Count</span>(src));</span><br><span class="line">  <span class="built_in">assert</span>(src-&gt;rep_.<span class="built_in">size</span>() &gt;= kHeader);</span><br><span class="line">  dst-&gt;rep_.<span class="built_in">append</span>(src-&gt;rep_.<span class="built_in">data</span>() + kHeader, src-&gt;rep_.<span class="built_in">size</span>() - kHeader);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The count is used primarily in two places. First, when iterating through each record, it’s used for <a href="https://github.com/google/leveldb/blob/main/db/write_batch.cc#L75">integrity checking</a> to ensure no operations are missed.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">WriteBatch::Iterate</span><span class="params">(Handler* handler)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">  <span class="function">Slice <span class="title">input</span><span class="params">(rep_)</span></span>;</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">if</span> (found != WriteBatchInternal::<span class="built_in">Count</span>(<span class="keyword">this</span>)) &#123;</span><br><span class="line">    <span class="keyword">return</span> Status::<span class="built_in">Corruption</span>(<span class="string">&quot;WriteBatch has wrong count&quot;</span>);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> Status::<span class="built_in">OK</span>();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Second, during database writes, the count helps pre-determine how many sequence numbers need to be allocated, ensuring sequence number continuity. As shown in <a href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L449">DBImpl::Write</a>:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WriteBatchInternal::<span class="built_in">SetSequence</span>(write_batch, last_sequence + <span class="number">1</span>);</span><br><span class="line">last_sequence += WriteBatchInternal::<span class="built_in">Count</span>(write_batch);</span><br></pre></td></tr></table></figure><h3 id="Supported-Operations"><a href="#Supported-Operations" class="headerlink" title="Supported Operations"></a>Supported Operations</h3><p>After the header’s sequence and count, rep_ contains a series of records, each including an operation type and key-value pair. Records can be added through Put and Delete methods. Here’s the implementation of Put:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">WriteBatch::Put</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">const</span> Slice&amp; value)</span> </span>&#123;</span><br><span class="line">  WriteBatchInternal::<span class="built_in">SetCount</span>(<span class="keyword">this</span>, WriteBatchInternal::<span class="built_in">Count</span>(<span class="keyword">this</span>) + <span class="number">1</span>);</span><br><span class="line">  rep_.<span class="built_in">push_back</span>(<span class="built_in">static_cast</span>&lt;<span class="type">char</span>&gt;(kTypeValue));</span><br><span class="line">  <span class="built_in">PutLengthPrefixedSlice</span>(&amp;rep_, key);</span><br><span class="line">  <span class="built_in">PutLengthPrefixedSlice</span>(&amp;rep_, value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This updates the count, adds the kTypeValue operation type, then adds the key and value. The Delete operation is similar - it increments the count, uses kTypeDeletion as the operation type, and only needs to add the key.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">WriteBatch::Delete</span><span class="params">(<span class="type">const</span> Slice&amp; key)</span> </span>&#123;</span><br><span class="line">  WriteBatchInternal::<span class="built_in">SetCount</span>(<span class="keyword">this</span>, WriteBatchInternal::<span class="built_in">Count</span>(<span class="keyword">this</span>) + <span class="number">1</span>);</span><br><span class="line">  rep_.<span class="built_in">push_back</span>(<span class="built_in">static_cast</span>&lt;<span class="type">char</span>&gt;(kTypeDeletion));</span><br><span class="line">  <span class="built_in">PutLengthPrefixedSlice</span>(&amp;rep_, key);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Above shows how records are added to rep_, but how are these records parsed from rep_? The WriteBatch class provides an <a href="https://github.com/google/leveldb/blob/main/db/write_batch.cc#L42">Iterate</a> method that traverses each record in rep_ and flexibly handles these records through the passed Handler interface.</p><p>Additionally, the implementation includes <strong>data format validation, checking header size, operation type, and operation count matching</strong>. It can return Corruption errors indicating incorrect data format. Here’s the core code of Iterate:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">WriteBatch::Iterate</span><span class="params">(Handler* handler)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">  <span class="function">Slice <span class="title">input</span><span class="params">(rep_)</span></span>;</span><br><span class="line">  <span class="keyword">if</span> (input.<span class="built_in">size</span>() &lt; kHeader) &#123;</span><br><span class="line">    <span class="keyword">return</span> Status::<span class="built_in">Corruption</span>(<span class="string">&quot;malformed WriteBatch (too small)&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  input.<span class="built_in">remove_prefix</span>(kHeader);</span><br><span class="line">  Slice key, value;</span><br><span class="line">  <span class="type">int</span> found = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> (!input.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">    found++;</span><br><span class="line">    <span class="type">char</span> tag = input[<span class="number">0</span>];</span><br><span class="line">    input.<span class="built_in">remove_prefix</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">switch</span> (tag) &#123;</span><br><span class="line">      <span class="keyword">case</span> kTypeValue:</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">GetLengthPrefixedSlice</span>(&amp;input, &amp;key) &amp;&amp;</span><br><span class="line">            <span class="built_in">GetLengthPrefixedSlice</span>(&amp;input, &amp;value)) &#123;</span><br><span class="line">          handler-&gt;<span class="built_in">Put</span>(key, value);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">return</span> Status::<span class="built_in">Corruption</span>(<span class="string">&quot;bad WriteBatch Put&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> kTypeDeletion:</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">GetLengthPrefixedSlice</span>(&amp;input, &amp;key)) &#123;</span><br><span class="line">          handler-&gt;<span class="built_in">Delete</span>(key);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">return</span> Status::<span class="built_in">Corruption</span>(<span class="string">&quot;bad WriteBatch Delete&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">return</span> Status::<span class="built_in">Corruption</span>(<span class="string">&quot;unknown WriteBatch tag&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>As mentioned earlier, Handler is WriteBatch’s abstract base class and can accept different implementations. When writing data in LevelDB, the MemTableInserter class is passed in, which stores operation data in MemTable. Here’s the specific implementation:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">WriteBatchInternal::InsertInto</span><span class="params">(<span class="type">const</span> WriteBatch* b, MemTable* memtable)</span> </span>&#123;</span><br><span class="line">  MemTableInserter inserter;</span><br><span class="line">  inserter.sequence_ = WriteBatchInternal::<span class="built_in">Sequence</span>(b);</span><br><span class="line">  inserter.mem_ = memtable;</span><br><span class="line">  <span class="keyword">return</span> b-&gt;<span class="built_in">Iterate</span>(&amp;inserter);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Overall, WriteBatch is responsible for storing key-value operation data and handling encoding&#x2F;decoding, while Handler is responsible for processing each piece of data specifically. This allows WriteBatch operations to be flexibly applied to different scenarios, facilitating extension.</p><h2 id="Test-Case-Analysis"><a href="#Test-Case-Analysis" class="headerlink" title="Test Case Analysis"></a>Test Case Analysis</h2><p>Finally, let’s look at <a href="https://github.com/google/leveldb/blob/main/db/write_batch_test.cc">write_batch_test.cc</a>, which provides test cases for WriteBatch functionality.</p><p>First, it defines a PrintContents function to output all operation records in WriteBatch. It uses MemTableInserter to store WriteBatch operation records in MemTable, then traverses all records using MemTable’s iterator and saves them to a string.</p><p>The test cases cover the following scenarios:</p><ol><li>Empty: Tests if an empty WriteBatch works normally</li><li>Multiple: Tests multiple Put and Delete operations</li><li>Corruption: Writes data then deliberately truncates some records to test reading as many normal records as possible</li><li>Append: Tests merging two WriteBatches, including sequence numbers and empty WriteBatch cases</li><li>ApproximateSize: Tests the ApproximateSize method for calculating approximate WriteBatch size</li></ol><p>Through these test cases, we can understand how to use WriteBatch. Interestingly, when looking at the Append code earlier, we didn’t notice whose sequence number is used after merging. Looking at the test cases, we discover it uses the target WriteBatch’s sequence number:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TEST</span>(WriteBatchTest, Append) &#123;</span><br><span class="line">  WriteBatch b1, b2;</span><br><span class="line">  WriteBatchInternal::<span class="built_in">SetSequence</span>(&amp;b1, <span class="number">200</span>);</span><br><span class="line">  WriteBatchInternal::<span class="built_in">SetSequence</span>(&amp;b2, <span class="number">300</span>);</span><br><span class="line">  b<span class="number">1.</span><span class="built_in">Append</span>(b2);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(<span class="string">&quot;&quot;</span>, <span class="built_in">PrintContents</span>(&amp;b1));</span><br><span class="line">  b<span class="number">2.</span><span class="built_in">Put</span>(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;va&quot;</span>);</span><br><span class="line">  b<span class="number">1.</span><span class="built_in">Append</span>(b2);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(<span class="string">&quot;Put(a, va)@200&quot;</span>, <span class="built_in">PrintContents</span>(&amp;b1));</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>By delving into the implementation of LevelDB’s WriteBatch, we can clearly see the ingenuity of its design. WriteBatch improves the efficiency of write operations and simplifies the implementation of concurrency control and fault recovery by encapsulating multiple write and delete operations together. Several highlights are worth noting:</p><ol><li><strong>Batch Operations</strong>: WriteBatch allows combining multiple Put and Delete operations into a single batch, reducing frequent I&#x2F;O operations and enhancing write performance.</li><li><strong>Sequence Number Mechanism</strong>: Through globally incrementing sequence numbers, LevelDB achieves Multi-Version Concurrency Control (MVCC), ensuring consistency in read and write operations.</li><li><strong>Handler Abstraction</strong>: The Handler interface decouples the specific implementation of operations from storage logic, making the code more flexible and extensible.</li><li><strong>Data Format Validation</strong>: When parsing WriteBatch, LevelDB performs strict data format validation to ensure data integrity and correctness.</li></ol><p>Of course, this article only analyzes the implementation of WriteBatch and does not cover the entire write process of LevelDB. In future articles, we will continue to explore the complete flow of writing a key.</p>]]></content:encoded>
      
      
      <category domain="https://selfboot.cn/categories/Source-Code-Analysis/">Source Code Analysis</category>
      
      
      <category domain="https://selfboot.cn/tags/C/">C++</category>
      
      <category domain="https://selfboot.cn/tags/LevelDB/">LevelDB</category>
      
      
      <comments>https://selfboot.cn/en/2025/01/13/leveldb_source_write_batch/#disqus_thread</comments>
      
    </item>
    
    
    
    
    
    <item>
      <title>5 Real-world Cases of C++ Process Crashes from Production</title>
      <link>https://selfboot.cn/en/2025/01/10/c++_crash_cases/</link>
      <guid>https://selfboot.cn/en/2025/01/10/c++_crash_cases/</guid>
      <pubDate>Fri, 10 Jan 2025 21:00:00 GMT</pubDate>
      
      <description>This article analyzes 5 common types of process crashes in C++ development, including uncaught exceptions, array bounds violations, iterator invalidation, concurrent data races, and stack overflow issues. Through real-world code examples, it explains the root causes, debugging methods, and solutions for each problem. The article also covers how to analyze coredump issues using tools like GDB and Valgrind, serving as a practical C++ reference guide.</description>
      
      
      
      <content:encoded><![CDATA[<p>If you’ve worked on any non-trivial C++ project, you’ve likely encountered process coredumps. A coredump is a mechanism where the operating system records the current memory state of a program when it encounters a severe error during execution.</p><p>There are many reasons why a C++ process might coredump, including:</p><ol><li><strong>Illegal Memory Access</strong>: This includes dereferencing null pointers, accessing freed memory, array bounds violations, etc.</li><li><strong>Stack Overflow</strong>: Caused by infinite recursion or large arrays allocated on the stack</li><li><strong>Segmentation Fault</strong>: Attempting to write to read-only memory or accessing unmapped memory regions</li><li><strong>Uncaught Exceptions</strong>: Program termination due to unhandled exceptions</li></ol><p>When encountering a coredump, we typically need to examine the core file for problem analysis and debugging. Analyzing core files can be challenging as it requires a deep understanding of C++’s memory model, exception handling mechanisms, and system calls.</p><p>Rather than focusing on core file analysis methods, this article will present several real-world cases to help developers proactively avoid these errors in their code.</p><span id="more"></span><h2 id="Uncaught-Exceptions"><a href="#Uncaught-Exceptions" class="headerlink" title="Uncaught Exceptions"></a>Uncaught Exceptions</h2><p>One of the most common causes of process crashes in production code is throwing exceptions without proper catch handlers. For example, when using <a href="https://cplusplus.com/reference/string/stoi/">std::stoi</a> to convert a string to an integer, if the string cannot be converted to a number, it throws a <code>std::invalid_argument</code> exception. If neither the framework nor the caller catches this exception, the process will crash.</p><p>The C++ standard library has quite a few functions that may throw exceptions. Common examples include:</p><ul><li>std::vector::at(): Throws <code>std::out_of_range</code> for out-of-bounds access</li><li>std::vector::push_back(): Throws <code>std::bad_alloc</code> if memory allocation fails</li><li>std::map::at(): Throws <code>std::out_of_range</code> if the key doesn’t exist</li></ul><p>When using these potentially throwing functions from the standard library, proper exception handling is crucial. <strong>For custom classes, it’s recommended to use error codes rather than exceptions for error handling. While there’s ongoing debate about exceptions versus error codes, you should follow what you’re comfortable with or your project’s conventions</strong>. For functions that are guaranteed not to throw, you can mark them with noexcept to inform both the compiler and users.</p><p>It’s worth noting that some function calls won’t throw exceptions but may lead to <a href="https://selfboot.cn/en/2016/09/18/c++_undefined_behaviours/">undefined behavior</a>, which can also cause process crashes. For example, the <a href="https://cplusplus.com/reference/cstdlib/atoi/?kw=atoi">atoi function</a> exhibits undefined behavior if the string can’t be converted to a number, potentially leading to crashes in certain scenarios.</p><p>When using basic functions, if you’re unsure about their behavior, consult the <a href="https://cplusplus.com/">cplusplus</a> documentation to determine whether they might throw exceptions or lead to undefined behavior. For example, regarding vector:</p><blockquote><p>std::vector::front()<br> Calling this function on an empty container causes undefined behavior.</p><p>std::vector::push_back()<br> If a reallocation happens, the storage is allocated using the container’s allocator, which may throw exceptions on failure (for the default allocator, bad_alloc is thrown if the allocation request does not succeed).</p></blockquote><h2 id="Array-Bounds-Violations"><a href="#Array-Bounds-Violations" class="headerlink" title="Array Bounds Violations"></a>Array Bounds Violations</h2><p>Beyond exceptions, another common issue is array bounds violations. We all know that in C++, accessing an array with an out-of-bounds index leads to illegal memory access, potentially causing a process crash. You might think, “How could array access go out of bounds? I just need to check the length when iterating!”</p><p>Let’s look at a real example from production code. For demonstration purposes, I’ve simplified the actual business logic to show just the core issue:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; src = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>&#125;;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; dest;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; src.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        <span class="comment">// Filtering logic possibly added later</span></span><br><span class="line">        <span class="keyword">if</span>(src[i] == <span class="number">8</span>) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        dest.<span class="built_in">push_back</span>(src[i] * <span class="number">100</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// ... Continue processing based on src content</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; src.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        <span class="comment">// Other processing of src</span></span><br><span class="line">        <span class="comment">// This usage is problematic, but might not crash as memory is on heap</span></span><br><span class="line">        <span class="comment">// dest[i] -= 5; </span></span><br><span class="line">        dest.<span class="built_in">at</span>(i) -= <span class="number">5</span>; <span class="comment">// This will crash</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Initially, the first loop initializes dest from src. After some intervening code, another loop processes dest based on src’s content.</p><p>This worked fine at first, but then a requirement was added to filter out certain values from src, leading to the addition of the if statement to skip some elements. The person making this change might not have noticed the subsequent iteration over src and dest, not realizing that filtering would cause dest’s length to differ from src’s.</p><p>This scenario can be tricky to trigger as a coredump, as it only occurs in rare cases where filtering actually results in different lengths. Moreover, even when accessing an out-of-bounds index in the second loop, using [] might not cause a crash. The example code deliberately uses at() to force a crash on out-of-bounds access.</p><h2 id="Iterator-Invalidation"><a href="#Iterator-Invalidation" class="headerlink" title="Iterator Invalidation"></a>Iterator Invalidation</h2><p>Beyond array bounds violations, another common issue is iterator invalidation. The iterator pattern provides a way to access elements in a container without exposing its internal representation. In C++, iterators are a crucial concept, serving as a bridge between containers and algorithms.</p><p>Many containers in the C++ standard library provide iterators, such as vector, list, and map. <strong>When accessing these container iterators, if the iterator has been invalidated, it leads to undefined behavior and potentially causes a process crash</strong>.</p><p>There are several ways iterators can become invalid. For example, when a vector reallocates memory during expansion, all previous iterators become invalid. A classic example is trying to remove even-numbered elements from a vector, where beginners might write something like this:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> it = numbers.<span class="built_in">begin</span>(); it != numbers.<span class="built_in">end</span>(); ++it) &#123;</span><br><span class="line">    <span class="keyword">if</span> (*it % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">        numbers.<span class="built_in">erase</span>(it);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, when <code>erase</code> is called to remove an element, it <strong>invalidates all iterators at and after the deletion point</strong>. So continuing to use <code>it</code> in the loop leads to undefined behavior. The correct approach is to use the return value from erase to update the iterator, or use remove_if and erase to remove elements.</p><p>While this example is relatively simple, we’ve encountered more subtle iterator invalidation issues in production. In one case, we had a batch processing task using a coroutine pool to handle IO-intensive tasks and write results back to a vector. Here’s a simplified version of the code:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Simulated async processing function</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AsyncProcess</span><span class="params">(<span class="type">int</span>&amp; value)</span> </span>&#123;</span><br><span class="line">    std::this_thread::<span class="built_in">sleep_for</span>(std::chrono::<span class="built_in">milliseconds</span>(<span class="number">100</span>));</span><br><span class="line">    value += <span class="number">1</span>;  <span class="comment">// Might access an invalid reference</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; values;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; results;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        values.<span class="built_in">push_back</span>(i);</span><br><span class="line">        results.<span class="built_in">push_back</span>(<span class="number">-1</span>);</span><br><span class="line">        <span class="type">int</span>&amp; result = results.<span class="built_in">back</span>();</span><br><span class="line"></span><br><span class="line">        <span class="function">std::thread <span class="title">t</span><span class="params">([&amp;result]() &#123;</span></span></span><br><span class="line"><span class="params"><span class="function">            AsyncProcess(result);  <span class="comment">// Using reference in async task</span></span></span></span><br><span class="line"><span class="params"><span class="function">        &#125;)</span></span>;</span><br><span class="line">        t.<span class="built_in">detach</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Wait for tasks to complete</span></span><br><span class="line">    std::this_thread::<span class="built_in">sleep_for</span>(std::chrono::<span class="built_in">seconds</span>(<span class="number">1</span>));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, we store a reference to <code>results.back()</code> and use it in an async task. While the async task is executing, the results vector continues to add new elements. When the vector needs to reallocate, the original memory is freed and new memory is allocated. At this point, the reference held by the async task becomes dangling, and accessing it leads to undefined behavior.</p><p>The correct approach would be to either use <code>reserve</code> to pre-allocate space to avoid reallocation, or store indices instead of references.</p><h2 id="Data-Races-in-Concurrent-Code"><a href="#Data-Races-in-Concurrent-Code" class="headerlink" title="Data Races in Concurrent Code"></a>Data Races in Concurrent Code</h2><p>Another category of crashes comes from data races in concurrent code. A common scenario is having a background thread in a service that pulls configuration updates from a configuration center and updates them locally. Meanwhile, multiple business threads concurrently read this configuration.</p><p>Since this is a classic read-heavy scenario, it’s typically implemented using a read-write lock. Multiple reader threads can hold the read lock simultaneously, while the writer thread needs exclusive access, ensuring no other read or write operations during writing. New read operations must wait during write operations. A possible execution sequence might look like this:</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Time ──────────────────────────────────────────────────────▶</span></span><br><span class="line"><span class="attribute">Reader 1</span><span class="punctuation">:</span> <span class="string">     RRRR      RRRR      </span></span><br><span class="line"><span class="attribute">Reader 2</span><span class="punctuation">:</span> <span class="string">        RRRR        RRRR</span></span><br><span class="line"><span class="attribute">Reader 3</span><span class="punctuation">:</span> <span class="string">           RRRR         RRRR</span></span><br><span class="line"><span class="attribute">Writer A</span><span class="punctuation">:</span> <span class="string"> W                 W</span></span><br></pre></td></tr></table></figure><p>Here, W represents a write operation, and R represents a read operation. As you can see, new read operations must wait during write operations. We encountered a crash in production due to incorrect use of read-write locks. While the actual scenario was more complex, here’s the core code simplified:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DataManager</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    std::shared_mutex mutex_;</span><br><span class="line">    std::unordered_map&lt;std::string, std::string&gt; m_data;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> num_keys = <span class="number">100</span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">loadData</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        std::unordered_map&lt;std::string, std::string&gt; localdata;</span><br><span class="line">        std::vector&lt;std::string&gt; keys;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_keys; i++) &#123;</span><br><span class="line">            keys.<span class="built_in">push_back</span>(<span class="string">&quot;test&quot;</span> + std::<span class="built_in">to_string</span>(i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_keys; i++) &#123;</span><br><span class="line">            localdata[keys[i]] = <span class="string">&quot;test&quot;</span> + std::<span class="built_in">to_string</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="function">std::unique_lock&lt;std::shared_mutex&gt; <span class="title">lock</span><span class="params">(mutex_)</span></span>;</span><br><span class="line">            m_data.<span class="built_in">swap</span>(localdata);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">std::string <span class="title">readData</span><span class="params">(<span class="type">const</span> std::string&amp; key)</span> </span>&#123;</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="function">std::shared_lock&lt;std::shared_mutex&gt; <span class="title">lock</span><span class="params">(mutex_)</span></span>;</span><br><span class="line">            <span class="keyword">return</span> m_data[key];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>The complete demonstration code is available in <a href="https://gist.github.com/selfboot/dc0d9450ded391bc28a43aecd1045694">core_share.cpp</a>. In loadData, we prepare the configuration data and then use a write lock to update it. In readData, we use a read lock to read the configuration.</p><p>Seems fine, right? The crash was very intermittent, and the code hadn’t been modified for a long time. We could only analyze it using the core file. Surprisingly, the core stack showed the crash occurring during the <strong>destruction of localdata</strong> in the loadData method. localdata is a local variable that swaps with m_data before destruction. This suggested that m_data’s memory layout was problematic, and <strong>m_data is only written here, with all other access being “read”</strong>.</p><p>Upon closer inspection of the code, we discovered that m_data was being read using the [] operator for unordered_map. <strong>For unordered_map, if a key doesn’t exist, [] will insert a default value</strong>. Aha! While we intended to protect read-only operations with a read lock, we accidentally performed write operations. As we know, concurrent writes to unordered_map cause data races, which explains the crashes.</p><p>Of course, the core stack might not always show the destruction point. For example, in our demonstration code, the stack shows the crash in the reader thread’s readData, as shown in this image:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250110_c++_crash_cases_mapread.png" alt="Reader Thread Crash Stack"></p><h2 id="Catastrophic-Backtracking-Leading-to-Stack-Overflow"><a href="#Catastrophic-Backtracking-Leading-to-Stack-Overflow" class="headerlink" title="Catastrophic Backtracking Leading to Stack Overflow"></a>Catastrophic Backtracking Leading to Stack Overflow</h2><p>The examples above are relatively easy to avoid with proper attention. However, the following issue is less well-known and easier to overlook.</p><p>We needed to check if a string contained a pair of parentheses, so we used C++ regular expressions. Here’s a simplified version of the code:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;regex&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::string problematic = <span class="string">&quot;((((&quot;</span>;</span><br><span class="line">    problematic += std::<span class="built_in">string</span>(<span class="number">100000</span>, <span class="string">&#x27;a&#x27;</span>);</span><br><span class="line">    problematic += <span class="string">&quot;))))&quot;</span>;</span><br><span class="line">    <span class="function">std::regex <span class="title">re</span><span class="params">(<span class="string">R&quot;(\([^\)]+\))&quot;</span>)</span></span>;</span><br><span class="line">    std::smatch matches;</span><br><span class="line">    <span class="type">bool</span> found = std::<span class="built_in">regex_search</span>(problematic, matches, re);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In this code, we construct a very long string and use a regular expression to match it. When compiled with g++ and executed, the program crashes. Looking at the stack trace in gdb shows:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250107_c++_crash_cases_regex.png" alt="Stack Overflow Due to Catastrophic Backtracking"></p><p>This happens because the regex engine performs extensive backtracking, creating new stack frames for each backtrack operation. Eventually, the stack depth exceeds the stack size limit, causing the process to crash.</p><p>This is known as <strong>Catastrophic Backtracking</strong>. In real-world development, it’s best to limit input length when dealing with complex text processing. If possible, use loops or other non-recursive solutions instead of regular expressions. If regular expressions are necessary, limit repetition counts (use {n,m} instead of + or *) and avoid nested repetitions (like (.+)+).</p><p>The regular expression could be modified to:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::regex <span class="title">re</span><span class="params">(<span class="string">R&quot;(\([^\)]&#123;1,100&#125;\))&quot;</span>)</span></span>;</span><br></pre></td></tr></table></figure><p>Of course, beyond recursive backtracking, other scenarios can cause stack overflow, such as infinite recursion or large arrays allocated on the stack. Fortunately, stack overflow issues are relatively easy to diagnose when you have a core file.</p><h2 id="Analyzing-Coredump-Issues"><a href="#Analyzing-Coredump-Issues" class="headerlink" title="Analyzing Coredump Issues"></a>Analyzing Coredump Issues</h2><p>When encountering crashes, we typically need to examine the core file. In production environments, if the business process uses significant memory, saving a core file after a crash might take considerable time. Production environments usually have watchdog processes that periodically check business processes, and if a process becomes unresponsive, some watchdogs use <code>kill -9</code> to terminate and restart the process. <strong>In such cases, we might get an incomplete core file that was only partially written</strong>. The solution is to modify the watchdog process to wait for the core file to complete before restarting the process.</p><p>Once we have the core file, we analyze it using gdb. If the stack trace is clear, we can usually identify the issue quickly. However, often the stack trace might be incomplete, showing a series of ??. For example, with the iterator invalidation issue above, running with gdb shows this stack trace:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250109_c++_crash_cases_gdb_iter.png" alt="Iterator Invalidation Stack Trace"></p><p>This stack trace provides little useful information and is difficult to analyze. For issues that can be reliably reproduced like our example, using <a href="https://en.wikipedia.org/wiki/Valgrind">Valgrind</a> for analysis can make it easier to locate the problem. The analysis results for the above code show:</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20250109_c++_crash_cases_valgrind.png" alt="Valgrind Analysis of Iterator Invalidation"></p><p>The analysis results show two main issues: Invalid read and Invalid write. The problematic code lines are clearly indicated, making it easy to locate the issue.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>This article has covered five classic cases of process crashes that I’ve encountered:</p><ol><li><p><strong>Uncaught Exceptions</strong>: When using standard library functions, be aware of potential exceptions. Handle exceptions properly for functions that might throw them. For custom classes, consider using error codes instead of exceptions.</p></li><li><p><strong>Array Bounds Violations</strong>: When working with arrays or containers, carefully check index validity. Especially when iterating over the same container multiple times, ensure its size hasn’t changed. Consider using the <code>at()</code> method for bounds-checked access.</p></li><li><p><strong>Iterator Invalidation</strong>: When using iterators, be aware that container operations (like deletion or insertion) can invalidate them. For vectors, reallocation invalidates all iterators; understand the invalidation rules for other containers as well.</p></li><li><p><strong>Data Races in Concurrent Code</strong>: In multi-threaded environments, pay special attention to concurrent data access. Even seemingly read-only operations (like map’s [] operator) might modify container contents. Use appropriate synchronization mechanisms (like mutexes or read-write locks) to protect shared data.</p></li><li><p><strong>Stack Overflow from Catastrophic Backtracking</strong>: When using regular expressions or other potentially recursive operations, be mindful of input limitations. For complex text processing, prefer non-recursive solutions or limit recursion depth.</p></li></ol><p>Of course, there are other less common causes of crashes, such as the one I previously encountered: <a href="https://selfboot.cn/en/2024/03/15/object_memory_coredump/">C++ Process Coredump Analysis Due to Missing Bazel Dependencies</a>. If you’ve encountered any memorable crash cases, feel free to share them in the comments.</p>]]></content:encoded>
      
      
      <category domain="https://selfboot.cn/categories/Programming/">Programming</category>
      
      
      <category domain="https://selfboot.cn/tags/C/">C++</category>
      
      
      <comments>https://selfboot.cn/en/2025/01/10/c++_crash_cases/#disqus_thread</comments>
      
    </item>
    
    
    
    
    
    <item>
      <title>LevelDB Explained - Static Thread Safety Analysis with Clang</title>
      <link>https://selfboot.cn/en/2025/01/02/leveldb_source_thread_anno/</link>
      <guid>https://selfboot.cn/en/2025/01/02/leveldb_source_thread_anno/</guid>
      <pubDate>Thu, 02 Jan 2025 22:00:00 GMT</pubDate>
      
      <description>This article introduces how LevelDB uses Clang&#39;s static thread safety analysis tool, which supports detecting potential thread safety issues at compile time through macro annotations in the code.</description>
      
      
      
      <content:encoded><![CDATA[<p>LevelDB has some interesting macros that I rarely use in my daily coding. These macros are defined in <a href="https://github.com/google/leveldb/blob/main/port/thread_annotations.h">thread_annotations.h</a> and can be used to <strong>detect potential thread safety issues at compile time using Clang’s thread safety analysis tool</strong>.</p><p><img src="https://slefboot-1251736664.file.myqcloud.com/20241227_leveldb_source_thread_anno_code.png" alt="Clang&#39;s Thread Safety Analysis Tool"></p><span id="more"></span><p>What exactly do macros like these do? Let’s take a look together.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">GUARDED_BY</span>(x)          <span class="comment">// Indicates that a variable must be accessed while holding lock x</span></span><br><span class="line"><span class="built_in">PT_GUARDED_BY</span>(x)       <span class="comment">// GUARDED_BY for pointer types</span></span><br><span class="line"><span class="built_in">ACQUIRED_AFTER</span>(...)    <span class="comment">// Specifies the lock acquisition order to prevent deadlocks</span></span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure><h2 id="GUARDED-BY-Lock-Protection"><a href="#GUARDED-BY-Lock-Protection" class="headerlink" title="GUARDED_BY Lock Protection"></a>GUARDED_BY Lock Protection</h2><p>You’ll often see annotations like <code>GUARDED_BY(mutex_)</code> in class member variable definitions. What’s their purpose? Take a look at the LRU Cache definition:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LRUCache</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line"> <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="keyword">mutable</span> port::Mutex mutex_;</span><br><span class="line">  <span class="function"><span class="type">size_t</span> usage_ <span class="title">GUARDED_BY</span><span class="params">(mutex_)</span></span>;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="function">HandleTable table_ <span class="title">GUARDED_BY</span><span class="params">(mutex_)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>This is Clang’s thread safety annotation. During compilation, Clang checks that all accesses to <code>usage_</code> and <code>table_</code> occur while holding the <code>mutex_</code> lock. Additionally, the compiler checks whether all locks that should be released are actually released at the end of functions or code blocks, helping prevent resource leaks or deadlocks due to forgotten lock releases.</p><p>In contrast, when writing business code, we rarely use these thread safety annotations. At most, we might add comments indicating that something isn’t thread-safe and needs lock protection, relying entirely on developer discipline. Unsurprisingly, this leads to various strange multi-threading data race issues in business code.</p><p>LevelDB’s implementation includes many such thread safety annotations, which <strong>not only explicitly tell other developers that a variable needs lock protection but also help detect potential thread safety issues at compile time, reducing race conditions, deadlocks, and other problems that might occur in multi-threaded environments</strong>.</p><h3 id="Thread-Annotation-Example-with-Lock-Protection"><a href="#Thread-Annotation-Example-with-Lock-Protection" class="headerlink" title="Thread Annotation Example with Lock Protection"></a>Thread Annotation Example with Lock Protection</h3><p>Let’s look at a complete example to see how Clang’s thread safety annotations work. In this SharedData class, the <code>counter_</code> variable needs lock protection, and <code>mutex_</code> is our wrapped lock implementation.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// guard.cpp</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mutex&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">__attribute__</span>((<span class="built_in">capability</span>(<span class="string">&quot;mutex&quot;</span>))) Mutex &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123; mutex_.<span class="built_in">lock</span>(); &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">unlock</span><span class="params">()</span> </span>&#123; mutex_.<span class="built_in">unlock</span>(); &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    std::mutex mutex_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SharedData</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">Increment</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        mutex_.<span class="built_in">lock</span>();</span><br><span class="line">        counter_++;</span><br><span class="line">        mutex_.<span class="built_in">unlock</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Wrong case: Accessing shared variable without holding the lock</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">UnsafeIncrement</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        counter_++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">UnsafeIncrement2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        mutex_.<span class="built_in">lock</span>();</span><br><span class="line">        counter_++;</span><br><span class="line">        <span class="comment">// Forgot to unlock, will trigger warning</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    Mutex mutex_;</span><br><span class="line">    <span class="type">int</span> counter_ __attribute__((<span class="built_in">guarded_by</span>(mutex_)));</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    SharedData data;</span><br><span class="line">    data.<span class="built_in">Increment</span>();</span><br><span class="line">    data.<span class="built_in">UnsafeIncrement</span>();</span><br><span class="line">    data.<span class="built_in">UnsafeIncrement2</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>For this test code to run directly, we didn’t use the GUARDED_BY macro defined in LevelDB. The <code>__attribute__((guarded_by(mutex_)))</code> here expands to the same result as the macro.</p><p>When compiling this code with Clang, you’ll see warning messages:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">clang++ -pthread -Wthread-safety -std=c++17 guard.cpp -o guard</span></span><br><span class="line">guard.cpp:16:9: warning: writing variable &#x27;counter_&#x27; requires holding mutex &#x27;mutex_&#x27; exclusively [-Wthread-safety-analysis]</span><br><span class="line">        counter_++;</span><br><span class="line">        ^</span><br><span class="line">guard.cpp:22:9: warning: writing variable &#x27;counter_&#x27; requires holding mutex &#x27;mutex_&#x27; exclusively [-Wthread-safety-analysis]</span><br><span class="line">        counter_++;</span><br><span class="line">        ^</span><br><span class="line">guard.cpp:27:9: warning: writing variable &#x27;counter_&#x27; requires holding mutex &#x27;mutex_&#x27; exclusively [-Wthread-safety-analysis]</span><br><span class="line">        counter_++;</span><br><span class="line">        ^</span><br><span class="line">3 warnings generated</span><br></pre></td></tr></table></figure><p>As you can see, the compiler detects at compile time when the <code>counter_</code> variable is accessed without holding the <code>mutex_</code> lock and issues warnings.</p><h3 id="PT-GUARDED-BY-Pointer-Protection"><a href="#PT-GUARDED-BY-Pointer-Protection" class="headerlink" title="PT_GUARDED_BY Pointer Protection"></a>PT_GUARDED_BY Pointer Protection</h3><p>While GUARDED_BY is typically used on non-pointer members to protect the member variable itself, <strong>PT_GUARDED_BY is used on pointer and smart pointer members to protect the data being pointed to</strong>. Note that PT_GUARDED_BY <strong>only protects the data being pointed to, not the pointer itself</strong>. Here’s an example:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Mutex mu;</span><br><span class="line"><span class="function"><span class="type">int</span> *p1             <span class="title">GUARDED_BY</span><span class="params">(mu)</span></span>;</span><br><span class="line"><span class="function"><span class="type">int</span> *p2             <span class="title">PT_GUARDED_BY</span><span class="params">(mu)</span></span>;</span><br><span class="line"><span class="function">unique_ptr&lt;<span class="type">int</span>&gt; p3  <span class="title">PT_GUARDED_BY</span><span class="params">(mu)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">test</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  p1 = <span class="number">0</span>;             <span class="comment">// Warning!</span></span><br><span class="line"></span><br><span class="line">  *p2 = <span class="number">42</span>;           <span class="comment">// Warning!</span></span><br><span class="line">  p2 = <span class="keyword">new</span> <span class="type">int</span>;       <span class="comment">// OK.</span></span><br><span class="line"></span><br><span class="line">  *p3 = <span class="number">42</span>;           <span class="comment">// Warning!</span></span><br><span class="line">  p<span class="number">3.</span><span class="built_in">reset</span>(<span class="keyword">new</span> <span class="type">int</span>);  <span class="comment">// OK.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Capability-Attribute-Annotation"><a href="#Capability-Attribute-Annotation" class="headerlink" title="Capability Attribute Annotation"></a>Capability Attribute Annotation</h2><p>In the example above, we didn’t use the standard library’s mutex directly, but instead wrapped it in a simple <code>Mutex</code> class. The class definition uses the <code>__attribute__((capability(&quot;mutex&quot;)))</code> annotation.</p><p>This is because Clang’s thread safety analysis needs to <strong>know which types are locks and track their acquisition and release states</strong>. Standard library types don’t have these annotations and can’t be used directly with Clang’s thread safety analysis. Here, we use Clang’s <code>capability(&quot;mutex&quot;)</code> attribute to specify that the class has lock characteristics.</p><p>LevelDB’s lock definition code also uses annotations, though slightly differently, using <code>LOCKABLE</code>:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LOCKABLE</span> Mutex &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">Mutex</span>() = <span class="keyword">default</span>;</span><br><span class="line">  ~<span class="built_in">Mutex</span>() = <span class="keyword">default</span>;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">Mutex</span>(<span class="type">const</span> Mutex&amp;) = <span class="keyword">delete</span>;</span><br><span class="line">  Mutex&amp; <span class="keyword">operator</span>=(<span class="type">const</span> Mutex&amp;) = <span class="keyword">delete</span>;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><p>This is because earlier versions of Clang used the lockable attribute, while the more general capability attribute was introduced later. For backward compatibility, lockable was kept as an alias for capability(“mutex”). So, they are equivalent.</p><h2 id="Thread-Safety-Analysis-Capabilities"><a href="#Thread-Safety-Analysis-Capabilities" class="headerlink" title="Thread Safety Analysis Capabilities"></a>Thread Safety Analysis Capabilities</h2><p>The example above is a bit simple. At its core, Clang’s static thread safety analysis aims to <strong>provide a way to protect resources at compile time</strong>. These resources can be data members like <code>counter_</code> from earlier, or functions&#x2F;methods that provide access to certain underlying resources. Clang can ensure at compile time that unless a thread has the capability to access a resource, it cannot access that resource.</p><p>Here, thread safety analysis <strong>uses attributes to declare resource constraints</strong>, which can be attached to classes, methods, and data members. Clang officially provides a series of attribute definition macros that can be used directly. LevelDB defines its own macros, which can also be referenced.</p><p>In the previous examples, annotations were mainly used on data members, but they can also be used on functions. For example, the Mutex object defined in LevelDB uses these annotations on member functions:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LOCKABLE</span> Mutex &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">Lock</span><span class="params">()</span> <span class="title">EXCLUSIVE_LOCK_FUNCTION</span><span class="params">()</span> </span>&#123; mu_.<span class="built_in">lock</span>(); &#125;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">Unlock</span><span class="params">()</span> <span class="title">UNLOCK_FUNCTION</span><span class="params">()</span> </span>&#123; mu_.<span class="built_in">unlock</span>(); &#125;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">AssertHeld</span><span class="params">()</span> <span class="title">ASSERT_EXCLUSIVE_LOCK</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>These annotations are primarily used to mark lock object member functions, telling the compiler how these functions will change the lock’s state:</p><ul><li><strong>EXCLUSIVE_LOCK_FUNCTION</strong>: Indicates that the function will acquire exclusive access to the mutex. The lock must be unheld before calling, and will be exclusively held by the current thread after calling;</li><li><strong>UNLOCK_FUNCTION</strong>: Indicates that the function will release the lock. The lock must be held before calling (either exclusively or shared), and will be released after calling;</li><li><strong>ASSERT_EXCLUSIVE_LOCK</strong>: Used to assert that the current thread holds exclusive ownership of the lock, typically used in debug code to ensure code runs in the correct locking state.</li></ul><p>Of course, these are Clang’s early thread safety annotations, mainly named for locks. The above can now be replaced with <a href="https://clang.llvm.org/docs/ThreadSafetyAnalysis.html#acquire-acquire-shared-release-release-shared-release-generic">ACQUIRE(…), ACQUIRE_SHARED(…), RELEASE(…), RELEASE_SHARED(…)</a>.</p><p>For more details about other annotations, you can refer to Clang’s official documentation on <a href="https://clang.llvm.org/docs/ThreadSafetyAnalysis.html">Thread Safety Analysis</a>.</p>]]></content:encoded>
      
      
      <category domain="https://selfboot.cn/categories/Source-Code-Analysis/">Source Code Analysis</category>
      
      
      <category domain="https://selfboot.cn/tags/C/">C++</category>
      
      <category domain="https://selfboot.cn/tags/LevelDB/">LevelDB</category>
      
      
      <comments>https://selfboot.cn/en/2025/01/02/leveldb_source_thread_anno/#disqus_thread</comments>
      
    </item>
    
    
    
    
  </channel>
</rss>