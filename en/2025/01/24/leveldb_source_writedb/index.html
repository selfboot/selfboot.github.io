<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="baidu-site-verification" content="codeva-NxrUEx2EFN"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="This article provides an in-depth analysis of LevelDB's write mechanism, detailing the complete process from the Put interface to WAL logging and MemTable persistence. Through source code analysis, it reveals how LevelDB achieves 400,000 writes per second throughput through core technologies like WriteBatch merging strategy, dual MemTable memory management, WAL sequential write optimization, and dynamic Level0 file throttling. It also explores engineering details such as mixed sync write handling, small key-value merge optimization, and data consistency in exceptional scenarios, helping you master the design essence and implementation strategies of LevelDB's high-performance writing."><title>LevelDB Explained - Implementation and Optimization Details of Key-Value Writing</title><link rel="stylesheet" type="text/css" href="/css/normalize.min.css"><link rel="stylesheet" type="text/css" href="/css/pure-min.min.css"><link rel="stylesheet" type="text/css" href="/css/grids-responsive-min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"><script type="text/javascript" src="/js/jquery.min.js"></script><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml,en/atom.xml"><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;973c5bad683a49b8af76df256779f523&quot;}"></script>
<script defer="" src="https://cloud.umami.is/script.js" data-website-id="3578d9f2-9ea0-49d4-8781-e8d1217ab924" data-domains="selfboot.cn"></script><script async="" src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="referrer" content="no-referrer-when-downgrade"><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QNFB9JLSPV"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QNFB9JLSPV');
</script><script async="" type="text/javascript" src="/js/clipboard.min.js"></script><script async="" type="text/javascript" src="/js/toastr.min.js"></script><link rel="stylesheet" href="/css/toastr.min.css"><script>function switchLanguage(lang) {
  var currentPath = window.location.pathname;
  var newPath;
  if (lang === 'en') {
    newPath = '/en' + currentPath.replace(/^\/(zh-CN\/)?/, '/');
  } else {
    newPath = currentPath.replace(/^\/en/, '');
  }
  window.location.href = newPath;
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><a id="logo" href="/.">Just For Fun</a><div class="lang-select-wrapper"><i class="fas fa-globe"></i><select id="lang-select" onchange="switchLanguage(this.value)"><option value="zh-CN">中文</option><option value="en" selected="">English</option></select></div><p class="description">Know what it is, and know why it is so. The breadth of knowledge is a byproduct of its depth!</p></div><div id="nav-menu"><a href="/en/."><i class="fa fa-home"></i><span> Home</span></a><a href="/en/archives/"><i class="fa fa-archive"></i><span> Archive</span></a><a href="/en/aboutme.html"><i class="fa fa-user"></i><span> About</span></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">LevelDB Explained - Implementation and Optimization Details of Key-Value Writing</h1><div class="post-meta">2025/01/24<span> | </span><span class="category"><a href="/categories/Source-Code-Analysis/">Source Code Analysis</a></span><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a class="disqus-comment-count" data-disqus-identifier="en/2025/01/24/leveldb_source_writedb/" href="/en/2025/01/24/leveldb_source_writedb/#disqus_thread"></a><div class="post-content"><p>LevelDB provides a Put interface for writing key-value pairs, which is one of the most important operations in a KV database. The usage is straightforward:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">leveldb::Status status = leveldb::DB::<span class="built_in">Open</span>(options, <span class="string">"./db"</span>, &amp;db);</span><br><span class="line">status = db-&gt;<span class="built_in">Put</span>(leveldb::<span class="built_in">WriteOptions</span>(), key, value);</span><br></pre></td></tr></tbody></table></figure>

<p>One of LevelDB’s greatest advantages is its <strong>extremely fast write speed, supporting high concurrent random writes</strong>. The official <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/tree/main?tab=readme-ov-file#write-performance">write performance benchmark</a> shows:</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fillseq      :       1.765 micros/op;   62.7 MB/s</span><br><span class="line">fillsync     :     268.409 micros/op;    0.4 MB/s (10000 ops)</span><br><span class="line">fillrandom   :       2.460 micros/op;   45.0 MB/s</span><br><span class="line">overwrite    :       2.380 micros/op;   46.5 MB/s</span><br></pre></td></tr></tbody></table></figure>

<p>As we can see, without forced disk synchronization, random write speed reaches 45.0 MB/s, supporting about 400,000 writes per second. With forced disk synchronization, although the write speed decreases significantly, it still maintains around 0.4 MB/s, supporting about 3,700 writes per second.</p>
<p>What exactly happens behind the Put interface? How is data written? What optimizations does LevelDB implement? Let’s explore these questions together.  Before we begin, let’s look at an overview flowchart:</p>
<p><img src="https://slefboot-1251736664.file.myqcloud.com/20250124_leveldb_source_writedb_flow_en.png/webp" alt="LevelDB Write Process Overview" srcset="https://slefboot-1251736664.file.myqcloud.com/20250124_leveldb_source_writedb_flow_en.png/webp 2390w, https://slefboot-1251736664.file.myqcloud.com/20250124_leveldb_source_writedb_flow_en.png/webp400 400w, https://slefboot-1251736664.file.myqcloud.com/20250124_leveldb_source_writedb_flow_en.png/webp800 800w, https://slefboot-1251736664.file.myqcloud.com/20250124_leveldb_source_writedb_flow_en.png/webp1600 1600w" sizes="(min-width: 1150px) 723px, (min-width: 48em) calc((100vw - 120px) * 3 / 4 - 50px), (min-width: 35.5em) calc((100vw - 75px), calc(100vw - 40px)" width="2390" height="650"></p>
<span id="more"></span>

<h2 id="Two-Ways-to-Write-Keys-in-LevelDB"><a href="#Two-Ways-to-Write-Keys-in-LevelDB" class="headerlink" title="Two Ways to Write Keys in LevelDB"></a>Two Ways to Write Keys in LevelDB</h2><p>LevelDB supports both single key-value pair writes and batch writes. Internally, both are handled through <a href="https://selfboot.cn/en/2025/01/13/leveldb_source_write_batch/">WriteBatch</a>, regardless of whether it’s a single or batch write.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">DB::Put</span><span class="params">(<span class="type">const</span> WriteOptions&amp; opt, <span class="type">const</span> Slice&amp; key, <span class="type">const</span> Slice&amp; value)</span> </span>{</span><br><span class="line">  WriteBatch batch;</span><br><span class="line">  batch.<span class="built_in">Put</span>(key, value);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">Write</span>(opt, &amp;batch);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>We can choose to aggregate write operations at the application layer when calling LevelDB interfaces to achieve batch writes and improve write throughput. For example, we can design a buffer mechanism at the application layer to collect write requests over a period and then submit them together in a WriteBatch. This approach reduces disk write frequency and context switches, thereby improving performance.</p>
<p>Alternatively, we can write single key-value pairs each time, and LevelDB will handle them internally through WriteBatch. In high-concurrency situations, multiple write operations might be merged internally before being written to WAL and updated to the memtable.</p>
<p>The overall write process is quite complex. In this article, we’ll focus on the process of writing to WAL and memtable.</p>
<h2 id="Detailed-Write-Steps-in-LevelDB"><a href="#Detailed-Write-Steps-in-LevelDB" class="headerlink" title="Detailed Write Steps in LevelDB"></a>Detailed Write Steps in LevelDB</h2><p>The complete write implementation is in the <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1205">DBImpl::Write method in leveldb/db/db_impl.cc</a>. Let’s break it down step by step.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">DBImpl::Write</span><span class="params">(<span class="type">const</span> WriteOptions&amp; options, WriteBatch* updates)</span> </span>{</span><br><span class="line">  <span class="function">Writer <span class="title">w</span><span class="params">(&amp;mutex_)</span></span>;</span><br><span class="line">  w.batch = updates;</span><br><span class="line">  w.sync = options.sync;</span><br><span class="line">  w.done = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function">MutexLock <span class="title">l</span><span class="params">(&amp;mutex_)</span></span>;</span><br><span class="line">  writers_.<span class="built_in">push_back</span>(&amp;w);</span><br><span class="line">  <span class="keyword">while</span> (!w.done &amp;&amp; &amp;w != writers_.<span class="built_in">front</span>()) {</span><br><span class="line">    w.cv.<span class="built_in">Wait</span>();</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">if</span> (w.done) {</span><br><span class="line">    <span class="keyword">return</span> w.status;</span><br><span class="line">  }</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>The initial part assigns the WriteBatch and sync parameters to the Writer structure and manages multiple Writer structures through a writers_ queue. These two structures and the queue play crucial roles in the entire write process, so let’s examine them first.</p>
<h3 id="Writer-Structure-and-Processing-Queue"><a href="#Writer-Structure-and-Processing-Queue" class="headerlink" title="Writer Structure and Processing Queue"></a>Writer Structure and Processing Queue</h3><p>Here, <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/db/db_impl.h#L186">writers_</a> is a queue of type <code>std::deque&lt;Writer*&gt;</code>, used to manage multiple Writer structures.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::deque&lt;Writer*&gt; writers_ <span class="title">GUARDED_BY</span><span class="params">(mutex_)</span></span>;</span><br></pre></td></tr></tbody></table></figure>

<p>The queue is decorated with <code>GUARDED_BY(mutex_)</code>, indicating that access to the queue needs to be protected by the <code>mutex_</code> mutex lock. This uses Clang’s static thread safety analysis feature, which you can learn more about in my previous article <a href="https://selfboot.cn/en/2025/01/02/leveldb_source_thread_anno/">LevelDB Explained - Static Thread Safety Analysis with Clang</a>.</p>
<p>The Writer structure is defined as follows:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">DBImpl</span>::Writer {</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">Writer</span><span class="params">(port::Mutex* mu)</span></span></span><br><span class="line"><span class="function">      : batch(nullptr), sync(false), done(false), cv(mu) {</span>}</span><br><span class="line"></span><br><span class="line">  Status status;</span><br><span class="line">  WriteBatch* batch;</span><br><span class="line">  <span class="type">bool</span> sync;</span><br><span class="line">  <span class="type">bool</span> done;</span><br><span class="line">  port::CondVar cv;</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure>

<p>The Writer structure encapsulates several parameters, with the most important being a WriteBatch pointer that records the data for each WriteBatch write request. A status field records any error states for each WriteBatch write request.</p>
<p>Additionally, a sync flag <strong>indicates whether each WriteBatch write request needs to be immediately flushed to disk</strong>. By default, it’s false, meaning no forced disk flush. In this case, if the system crashes, some data that hasn’t been written to disk might be lost. If the sync option is enabled, each write will be immediately flushed to disk. While this increases overall write latency, it ensures that once a write succeeds, the data won’t be lost. For more details about flushing files to disk, you can refer to my previous article <a href="https://selfboot.cn/en/2024/08/02/leveldb_source_env_posixfile/">LevelDB Explained - Posix File Operation Details</a>.</p>
<p>The <strong>done flag marks whether each WriteBatch write request is completed</strong>. Since multiple WriteBatches might be merged internally, when a write request is merged into another batch, it’s marked as complete to avoid duplicate execution. This improves concurrent write efficiency.</p>
<p>To <strong>implement waiting and notification, there’s also a condition variable cv, which supports batch processing of multiple write requests and synchronization between them</strong>. During writes, multiple threads can submit write requests simultaneously, with each request being placed in the write queue. <strong>The actual write process is serialized, with only one batch of writes executing at a time</strong>. Each time, the front request from the queue is taken, and if there are other waiting tasks in the queue, they will be merged into one batch for processing. During the processing of the current batch, subsequent requests entering the queue need to wait. When the current batch is completed, waiting write requests in the queue are notified.</p>
<p>With this introduction, you should understand the meaning of the initial code in the Write method. For each write request, a Writer structure is created and placed in the writers_ queue. Then, in the while loop, it checks if the current write request is complete, returning the write status result if it is. If the current write request isn’t at the front of the queue, it needs to wait on the cv condition variable.</p>
<p>If the current write request is at the front of the queue, then the actual write operation needs to be executed. What does this specific write process look like?</p>
<h3 id="Pre-allocating-Space"><a href="#Pre-allocating-Space" class="headerlink" title="Pre-allocating Space"></a>Pre-allocating Space</h3><p>Before the actual write, we need to ensure there’s enough space for the data. This is handled by the <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1330">MakeRoomForWrite</a> method, which ensures sufficient resources and space are available before processing new write requests. It manages memtable usage, controls Level 0 file count, and triggers background compaction when needed.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// REQUIRES: this thread is currently at the front of the writer queue</span></span><br><span class="line"><span class="function">Status <span class="title">DBImpl::MakeRoomForWrite</span><span class="params">(<span class="type">bool</span> force)</span> </span>{</span><br><span class="line">  mutex_.<span class="built_in">AssertHeld</span>();</span><br><span class="line">  <span class="built_in">assert</span>(!writers_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="type">bool</span> allow_delay = !force;</span><br><span class="line">  Status s;</span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) {</span><br><span class="line">    <span class="keyword">if</span> (!bg_error_.<span class="built_in">ok</span>()) {</span><br><span class="line">      <span class="comment">// Yield previous error</span></span><br><span class="line">      s = bg_error_;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>The initial part includes some validation: AssertHeld verifies that the current thread holds the mutex_ lock, and the writers_ queue must not be empty. Then it checks if bg_error_ is empty; if not, it returns the bg_error_ status. As we’ll see later, if writing to WAL fails during disk flush, bg_error_ will be set, causing subsequent writes to fail directly.</p>
<p>In the while loop, there are several if branches handling different situations:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (allow_delay &amp;&amp; versions_-&gt;<span class="built_in">NumLevelFiles</span>(<span class="number">0</span>) &gt;=</span><br><span class="line">                                  config::kL0_SlowdownWritesTrigger) {</span><br><span class="line">      <span class="comment">// We are getting close to hitting a hard limit on the number of</span></span><br><span class="line">      <span class="comment">// L0 files.  Rather than delaying a single write by several</span></span><br><span class="line">      <span class="comment">// seconds when we hit the hard limit, start delaying each</span></span><br><span class="line">      <span class="comment">// individual write by 1ms to reduce latency variance.  Also,</span></span><br><span class="line">      <span class="comment">// this delay hands over some CPU to the compaction thread in</span></span><br><span class="line">      <span class="comment">// case it is sharing the same core as the writer.</span></span><br><span class="line">      mutex_.<span class="built_in">Unlock</span>();</span><br><span class="line">      env_-&gt;<span class="built_in">SleepForMicroseconds</span>(<span class="number">1000</span>);</span><br><span class="line">      allow_delay = <span class="literal">false</span>;  <span class="comment">// Do not delay a single write more than once</span></span><br><span class="line">      mutex_.<span class="built_in">Lock</span>();</span><br><span class="line">    }</span><br></pre></td></tr></tbody></table></figure>

<p>First, when the number of Level 0 files approaches the kL0_SlowdownWritesTrigger=8 threshold, it <strong>temporarily releases the lock and delays for 1 millisecond to slow down the write speed</strong>. However, this is only allowed once to avoid blocking a single write for too long. This small Level 0 file count threshold is set to prevent writes from being blocked for too long when the system reaches its bottleneck. Before reaching the bottleneck, it starts distributing the delay across each request to reduce pressure. The comments explain this clearly.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (!force &amp;&amp;</span><br><span class="line">           (mem_-&gt;<span class="built_in">ApproximateMemoryUsage</span>() &lt;= options_.write_buffer_size)) {</span><br><span class="line">  <span class="comment">// There is room in current memtable</span></span><br><span class="line">  <span class="keyword">break</span>;</span><br><span class="line">} </span><br></pre></td></tr></tbody></table></figure>

<p>Next, if the current memtable’s usage hasn’t exceeded its maximum capacity, it returns directly. Here, write_buffer_size is the maximum capacity of the memtable, defaulting to 4MB. This can be configured - a larger value will cache more data in memory, improving write performance, but will use more memory and take longer to recover when reopening the db.</p>
<p>The next two situations require waiting because there’s no place to write:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (imm_ != <span class="literal">nullptr</span>) {</span><br><span class="line">  <span class="comment">// We have filled up the current memtable, but the previous</span></span><br><span class="line">  <span class="comment">// one is still being compacted, so we wait.</span></span><br><span class="line">  <span class="built_in">Log</span>(options_.info_log, <span class="string">"Current memtable full; waiting...\n"</span>);</span><br><span class="line">  background_work_finished_signal_.<span class="built_in">Wait</span>();</span><br><span class="line">} <span class="keyword">else</span> <span class="keyword">if</span> (versions_-&gt;<span class="built_in">NumLevelFiles</span>(<span class="number">0</span>) &gt;= config::kL0_StopWritesTrigger) {</span><br><span class="line">  <span class="comment">// There are too many level-0 files.</span></span><br><span class="line">  <span class="built_in">Log</span>(options_.info_log, <span class="string">"Too many L0 files; waiting...\n"</span>);</span><br><span class="line">  background_work_finished_signal_.<span class="built_in">Wait</span>();</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>The first case is when the immutable memtable is still being written, so we need to wait for it to complete. LevelDB maintains two memtables: one current writable memtable (mem_) and one immutable memtable (imm_). When mem_ is full, it becomes imm_ and flushes data to disk. If imm_ hasn’t finished flushing to disk, we must wait before converting the current mem_ to a new imm_.</p>
<p>The second case is when there are too many Level 0 files, requiring waiting for compaction to complete. LevelDB configures a threshold kL0_StopWritesTrigger for Level 0 file count, defaulting to 12. When exceeded, current write requests must wait. This is because Level 0 files don’t have global sorting guarantees, and multiple Level 0 files might contain overlapping key ranges. For reads, queries need to search all L0 files, and too many files increase read latency. For writes, more files mean more background compaction work, affecting overall system performance. Therefore, Level 0 file count is strictly controlled, blocking writes when the threshold is reached.</p>
<p>When both imm_ is empty and mem_ doesn’t have enough space, there are several tasks to be done:</p>
<ol>
<li><strong>Create new log file</strong>: Generate a new log file number and try to create a new writable file as WAL (Write-Ahead Log). If it fails, reuse the file number and exit the loop, returning an error status.</li>
<li><strong>Close old log file</strong>: Close the current log file. If closing fails, record the background error to prevent subsequent write operations.</li>
<li><strong>Update log file pointer</strong>: Set the new log file pointer, update the log number, and create a new log::Writer for writing.</li>
<li><strong>Convert memtable</strong>: Convert the current memtable to an immutable memtable (imm_), and create a new memtable for writing. Mark the existence of an immutable memtable through has_imm_.store(true, std::memory_order_release).</li>
<li>Trigger background compaction: Call MaybeScheduleCompaction() to trigger background compaction tasks to process the immutable memtable.</li>
</ol>
<p>Here we can see that <strong>memtables and WAL files have a one-to-one correspondence, with each memtable corresponding to a WAL file. The WAL file records all operations written to the memtable, and when the memtable is full, the WAL file is switched simultaneously</strong>. At the same time, the foreground memtable and new WAL log file handle new requests, while the background imm_ and old WAL file handle compaction tasks. Once compaction is complete, the old WAL file can be deleted.</p>
<h3 id="Merging-Write-Tasks"><a href="#Merging-Write-Tasks" class="headerlink" title="Merging Write Tasks"></a>Merging Write Tasks</h3><p>Next is the logic for merging writes. Here’s the <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1224">core code</a>:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">uint64_t</span> last_sequence = versions_-&gt;<span class="built_in">LastSequence</span>();</span><br><span class="line">Writer* last_writer = &amp;w;</span><br><span class="line"><span class="keyword">if</span> (status.<span class="built_in">ok</span>() &amp;&amp; updates != <span class="literal">nullptr</span>) {  <span class="comment">// nullptr batch is for compactions</span></span><br><span class="line">  WriteBatch* write_batch = <span class="built_in">BuildBatchGroup</span>(&amp;last_writer);</span><br><span class="line">  WriteBatchInternal::<span class="built_in">SetSequence</span>(write_batch, last_sequence + <span class="number">1</span>);</span><br><span class="line">  last_sequence += WriteBatchInternal::<span class="built_in">Count</span>(write_batch);</span><br><span class="line"></span><br><span class="line">  {</span><br><span class="line">   <span class="comment">// ... specific writing to WAL and memtable </span></span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">if</span> (write_batch == tmp_batch_) tmp_batch_-&gt;<span class="built_in">Clear</span>();</span><br><span class="line"></span><br><span class="line">  versions_-&gt;<span class="built_in">SetLastSequence</span>(last_sequence);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>First, it gets the current global sequence value. Here, <strong>sequence is used to record the version number of written key-value pairs, which increases monotonically globally</strong>. Each write request is assigned a unique sequence value, implementing features like MVCC through the version number mechanism. When writing the current batch of key-value pairs, it first sets the sequence value, and after successful writing, it updates the last_sequence value.</p>
<p>To <strong>improve write concurrency performance, each write not only needs to write the front task but also attempts to merge subsequent write tasks in the queue</strong>. The merging logic is placed in <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1280">BuildBatchGroup</a>, which mainly traverses the entire write queue, <strong>continuously merging subsequent write tasks into the front write task while controlling the overall batch size and ensuring the disk flush level</strong>. The overall constructed write batch is placed in a temporary object tmp_batch_, which is cleared after the complete write operation is finished.</p>
<p>We mentioned that each write task is actually encapsulated as a WriteBatch object, whose implementation supports merging different write tasks and getting task sizes. For detailed implementation, you can refer to my previous article <a href="https://selfboot.cn/en/2025/01/13/leveldb_source_write_batch/">LevelDB Explained - Elegant Merging of Write and Delete Operations</a>.</p>
<p>The code above actually omitted the core logic of writing to WAL and memtable, let’s look at this part’s implementation.</p>
<h3 id="Writing-to-WAL-and-MemTable"><a href="#Writing-to-WAL-and-MemTable" class="headerlink" title="Writing to WAL and MemTable"></a>Writing to WAL and MemTable</h3><p>In LevelDB, when writing key-value pairs, it first writes to the WAL log, then writes to the memtable. The WAL log is key to implementing data recovery in LevelDB, while the memtable is key to implementing memory caching and fast queries. Here’s the critical write code:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Add to log and apply to memtable.  We can release the lock</span></span><br><span class="line"><span class="comment">// during this phase since &amp;w is currently responsible for logging</span></span><br><span class="line"><span class="comment">// and protects against concurrent loggers and concurrent writes</span></span><br><span class="line"><span class="comment">// into mem_.</span></span><br><span class="line">{</span><br><span class="line">  mutex_.<span class="built_in">Unlock</span>();</span><br><span class="line">  status = log_-&gt;<span class="built_in">AddRecord</span>(WriteBatchInternal::<span class="built_in">Contents</span>(write_batch));</span><br><span class="line">  <span class="type">bool</span> sync_error = <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">if</span> (status.<span class="built_in">ok</span>() &amp;&amp; options.sync) {</span><br><span class="line">    status = logfile_-&gt;<span class="built_in">Sync</span>();</span><br><span class="line">    <span class="keyword">if</span> (!status.<span class="built_in">ok</span>()) {</span><br><span class="line">      sync_error = <span class="literal">true</span>;</span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">if</span> (status.<span class="built_in">ok</span>()) {</span><br><span class="line">    status = WriteBatchInternal::<span class="built_in">InsertInto</span>(write_batch, mem_);</span><br><span class="line">  }</span><br><span class="line">  mutex_.<span class="built_in">Lock</span>();</span><br><span class="line">  <span class="keyword">if</span> (sync_error) {</span><br><span class="line">    <span class="comment">// The state of the log file is indeterminate: the log record we</span></span><br><span class="line">    <span class="comment">// just added may or may not show up when the DB is re-opened.</span></span><br><span class="line">    <span class="comment">// So we force the DB into a mode where all future writes fail.</span></span><br><span class="line">    <span class="built_in">RecordBackgroundError</span>(status);</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>Here, <strong>when writing to WAL and memtable, the mutex_ lock is first released, and then reacquired after completion</strong>. The comments specifically explain that while the current front <code>&amp;w</code> is responsible for writing to WAL and memtable, subsequent write calls can acquire the mutex_ lock to complete queue entry operations. However, since they’re not at the front, they need to wait on the condition variable, and only when the current task completes do they have a chance to execute. Therefore, <strong>although the lock is released during the process of writing to WAL and memtable, the overall write is still serialized</strong>. WAL and memtable themselves don’t need to ensure thread safety.</p>
<p>However, since writing to WAL and memtable is relatively time-consuming, after releasing the lock, other operations that need mutex_ can acquire it and continue executing, improving overall system concurrency.</p>
<p>WAL (Write-Ahead Logging) is a logging mechanism that allows recording logs before data is written to disk. <strong>WAL logs are written sequentially, and disk sequential IO performance is better than random IO performance, so sequential writes are generally more efficient</strong>. After successfully writing to WAL, data is placed in the memtable, which is a memory structure with high write efficiency. When enough data accumulates in memory, it’s written to disk. If the system crashes and restarts, data in the memtable may be lost, but through WAL logs, write operations can be replayed to restore the data state, ensuring data integrity.</p>
<p>The specific write here simply calls the AddRecord method of the log::Writer object log_ to write WriteBatch data. log::Writer will organize this data and write it to disk at appropriate times. For detailed implementation, you can refer to my previous article <a href="https://selfboot.cn/en/2024/08/14/leveldb_source_wal_log/">LevelDB Explained - How To Read and Write WAL Logs</a>.</p>
<p>Of course, if the write comes with sync=true, after successfully writing to WAL, the logfile_-&gt;Sync() method will be called to force disk flush. To clarify, <strong>writing content to files is done through the system call <code>write</code>, but success of this system call doesn’t guarantee the data has been written to disk. File systems generally put data in a buffer first, then choose appropriate times to flush to disk based on circumstances</strong>. To ensure data is written to disk, additional system calls are needed, with different platforms having different interfaces. For details, refer to my previous article <a href="https://selfboot.cn/en/2024/08/02/leveldb_source_env_posixfile/">LevelDB Explained - Posix File Operation Details</a>.</p>
<p>If an error occurs during forced disk flush, the RecordBackgroundError method is called to record the error status in bg_error_, causing all subsequent write operations to fail directly.</p>
<p>After successfully writing to WAL, we can write to the memtable. Here, the WriteBatchInternal::InsertInto method is called to insert WriteBatch data into the memtable. I’ll cover the implementation of memtable in detail in a future article.</p>
<h3 id="Updating-Batch-Write-Task-Status"><a href="#Updating-Batch-Write-Task-Status" class="headerlink" title="Updating Batch Write Task Status"></a>Updating Batch Write Task Status</h3><p>After completing the batch write, we need to update the status of batch write tasks, taking the Writer object from the front of the writers_ queue, then iterating until the last write task in the batch. Here we update the status of all completed tasks and wake up all waiting write tasks. The <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1259">core implementation</a> is as follows:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) {</span><br><span class="line">  Writer* ready = writers_.<span class="built_in">front</span>();</span><br><span class="line">  writers_.<span class="built_in">pop_front</span>();</span><br><span class="line">  <span class="keyword">if</span> (ready != &amp;w) {</span><br><span class="line">    ready-&gt;status = status;</span><br><span class="line">    ready-&gt;done = <span class="literal">true</span>;</span><br><span class="line">    ready-&gt;cv.<span class="built_in">Signal</span>();</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">if</span> (ready == last_writer) <span class="keyword">break</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Notify new head of write queue</span></span><br><span class="line"><span class="keyword">if</span> (!writers_.<span class="built_in">empty</span>()) {</span><br><span class="line">  writers_.<span class="built_in">front</span>()-&gt;cv.<span class="built_in">Signal</span>();</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>Finally, if there are still write tasks in the queue, we need to wake up the front write task to continue processing. At this point, the entire write process is complete, and we can return the write result to the caller.</p>
<h2 id="Other-Engineering-Implementation-Details"><a href="#Other-Engineering-Implementation-Details" class="headerlink" title="Other Engineering Implementation Details"></a>Other Engineering Implementation Details</h2><p>While we’ve analyzed the complete write process, there are some engineering implementation details worth examining.</p>
<h3 id="Handling-Mixed-Sync-and-Non-sync-Writes"><a href="#Handling-Mixed-Sync-and-Non-sync-Writes" class="headerlink" title="Handling Mixed Sync and Non-sync Writes"></a>Handling Mixed Sync and Non-sync Writes</h3><p>How does LevelDB internally handle a batch of write requests that includes both sync and non-sync writes?</p>
<p>From our previous analysis, we can see that after taking the front write task from the queue, it attempts to merge subsequent write tasks in the queue. Since each write task can either force sync disk flush or not, how are write tasks with different sync configurations handled during merging?</p>
<p>Here, when <strong>sync=true is configured, writes will force disk flush. For merged batch writes, the sync setting of the front task is used</strong>. The <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1237">core code</a> is as follows:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Status <span class="title">DBImpl::Write</span><span class="params">(<span class="type">const</span> WriteOptions&amp; options, WriteBatch* updates)</span> </span>{</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">  <span class="keyword">if</span> (status.<span class="built_in">ok</span>() &amp;&amp; updates != <span class="literal">nullptr</span>) {  <span class="comment">// nullptr batch is for compactions</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    {</span><br><span class="line">      mutex_.<span class="built_in">Unlock</span>();</span><br><span class="line">      status = log_-&gt;<span class="built_in">AddRecord</span>(WriteBatchInternal::<span class="built_in">Contents</span>(write_batch));</span><br><span class="line">      <span class="type">bool</span> sync_error = <span class="literal">false</span>;</span><br><span class="line">      <span class="keyword">if</span> (status.<span class="built_in">ok</span>() &amp;&amp; options.sync) {</span><br><span class="line">        status = logfile_-&gt;<span class="built_in">Sync</span>();</span><br><span class="line">        <span class="keyword">if</span> (!status.<span class="built_in">ok</span>()) {</span><br><span class="line">          sync_error = <span class="literal">true</span>;</span><br><span class="line">        }</span><br><span class="line">      }</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>Therefore, if the front task doesn’t require disk flush, then during merging, it cannot merge write tasks with sync=true. The <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1302">core implementation code</a> is as follows:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (; iter != writers_.<span class="built_in">end</span>(); ++iter) {</span><br><span class="line">  Writer* w = *iter;</span><br><span class="line">  <span class="keyword">if</span> (w-&gt;sync &amp;&amp; !first-&gt;sync) {</span><br><span class="line">    <span class="comment">// Do not include a sync write into a batch handled by a non-sync write.</span></span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  }</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>However, if the front task has sync=true, then during merging, we don’t need to consider the sync settings of the write tasks being merged. This is because the entire merged batch will be forced to flush to disk. This design <strong>ensures that the write durability guarantee level isn’t reduced while potentially improving it</strong>. Of course, improving the write durability guarantee level here doesn’t actually increase overall latency, because the front task must flush to disk anyway, and including additional write tasks that don’t require disk flush won’t increase the latency.</p>
<h3 id="Optimizing-Large-Batch-Small-Key-Write-Latency"><a href="#Optimizing-Large-Batch-Small-Key-Write-Latency" class="headerlink" title="Optimizing Large Batch Small Key Write Latency"></a>Optimizing Large Batch Small Key Write Latency</h3><p>As we can see from the above implementation, during large-scale concurrent writes, write requests are first placed in a queue and then written serially. If the keys being written are relatively small, then after taking a write task from the front of the queue, it’s merged with other writes in the current queue into a batch. When merging, a max_size needs to be set to limit the number of merged keys. What’s a reasonable value for this max_size?</p>
<p>LevelDB provides an empirical value, defaulting to 1 &lt;&lt; 20 bytes. However, consider a scenario where all keys being written are relatively small - during merging, many keys might be merged, leading to longer write latency. <strong>Since these are small key writes, long write latency doesn’t provide a good user experience</strong>.</p>
<p>Therefore, a small optimization was added: if the overall size of the current front write task is less than 128 &lt;&lt; 10 bytes, then max_size will be much smaller. Of course, this value seems to be empirical as well; I haven’t found official documentation explaining it. The relevant code is in <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/db/db_impl.cc#L1289">BuildBatchGroup</a>:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Allow the group to grow up to a maximum size, but if the</span></span><br><span class="line"><span class="comment">// original write is small, limit the growth so we do not slow</span></span><br><span class="line"><span class="comment">// down the small write too much.</span></span><br><span class="line"><span class="type">size_t</span> max_size = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line"><span class="keyword">if</span> (size &lt;= (<span class="number">128</span> &lt;&lt; <span class="number">10</span>)) {</span><br><span class="line">  max_size = size + (<span class="number">128</span> &lt;&lt; <span class="number">10</span>);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h3 id="WAL-Success-but-MemTable-Failure"><a href="#WAL-Success-but-MemTable-Failure" class="headerlink" title="WAL Success but MemTable Failure"></a>WAL Success but MemTable Failure</h3><p>During the write process, WAL is written first, and only after successful WAL writing is the memtable written. These two writes aren’t atomic. If WAL writing fails, we don’t write to the memtable and block all subsequent writes, which is reasonable and won’t cause issues.</p>
<p>However, if WAL writing succeeds but writing to the memtable fails, the status returned to the caller indicates write failure, and the caller might consider the write failed. But since WAL was successfully written and other processes executed normally, including updating the global sequence, <strong>if a failure occurs at this point, during recovery from WAL, the failed memtable writes will also be recovered, resulting in data that the caller thought failed actually being successfully written</strong>.</p>
<p>I haven’t fully understood this issue yet - it might be a potential bug in LevelDB, or I might not have fully grasped the overall code. I tried analyzing this with GPT O1 and Deepseek’s R1 deep thinking, and both lean towards this being a bug. Deepseek’s thought process was quite interesting:</p>
<blockquote>
<p>I need to confirm how likely this situation is in actual LevelDB operation. What typically causes MemTable write failures? Memory shortage or other unrecoverable errors? For temporary errors, does LevelDB have a retry mechanism?</p>
<p>Also, checking LevelDB’s official documentation or related discussions, is there any mention of how to handle this situation? For example, does LevelDB actively delete or invalidate corresponding WAL records when MemTable writes fail?</p>
<p>We might also need to consider the database recovery process. When LevelDB restarts, it reads WAL and rebuilds MemTable. If a WAL record corresponds to a failed MemTable write but WAL exists, recovery will attempt to write to MemTable again. If conditions allow (e.g., sufficient memory), the write might succeed, causing inconsistency. However, if the original failure was persistent (like data structure error), recovery might fail again, preventing the database from opening and thus avoiding inconsistency.</p>
<p>Additionally, does the user’s question assume certain specific scenarios? For example, MemTable write fails but the database doesn’t crash, then closes normally, and WAL isn’t cleared, so it’s reapplied during recovery. Is this scenario possible?</p>
</blockquote>
<p>AI’s deep thinking and logical reasoning capabilities are quite strong now, considering issues quite comprehensively. Readers are welcome to discuss this issue in the comments.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>The article is quite long, so let’s summarize. LevelDB’s write operation design fully considers high concurrency and performance optimization, implementing efficient key-value pair writing through a series of ingenious mechanisms. Here are some design aspects worth learning from:</p>
<ol>
<li><p><strong>Batch Write Merging</strong>: LevelDB merges multiple write requests through a Writer queue, avoiding frequent disk IO. Each write request is placed in the queue, with the queue head responsible for merging subsequent requests into a large WriteBatch. This design significantly improves throughput, especially suitable for high-concurrency small key-value pair write scenarios.</p>
</li>
<li><p><strong>WAL Log Crash Recovery</strong>: Write-Ahead Logging (WAL): All write operations are first sequentially written to WAL logs, ensuring data durability. Only after writing to WAL is the MemTable in memory updated. This “log first, memory second” design is the cornerstone of LevelDB’s crash recovery.</p>
</li>
<li><p><strong>Memory Double Buffering</strong>: When MemTable is full, it converts to Immutable MemTable and triggers background compaction while creating a new MemTable and WAL file. This <strong>double buffering mechanism avoids write blocking and achieves smooth memory-to-disk data transfer</strong>.</p>
</li>
<li><p><strong>Write Throttling and Adaptive Delay</strong>: Through kL0_SlowdownWritesTrigger and kL0_StopWritesTrigger thresholds, actively introducing write delays or pausing writes when there are too many Level 0 files. This “soft throttling” strategy prevents system avalanche effects after overload.</p>
</li>
<li><p><strong>Dynamic Batch Merging</strong>: Dynamically adjusting maximum batch size based on current queue head request size (e.g., 128KB for small requests, 1MB for large requests), balancing throughput and latency.</p>
</li>
<li><p><strong>Condition Variable Wake-up</strong>: Implementing efficient thread wait-notify through CondVar, ensuring merged writes don’t block subsequent requests for too long.</p>
</li>
<li><p><strong>Mixed Sync Handling</strong>: Supporting simultaneous handling of requests requiring forced disk flush (sync=true) and non-forced flush, prioritizing the persistence level of the queue head request without compromising data safety.</p>
</li>
<li><p><strong>Error Isolation</strong>: WAL write failures mark global error state bg_error_, directly rejecting all subsequent write requests to prevent data inconsistency.</p>
</li>
</ol>
<p>Finally, welcome to discuss in the comments and learn LevelDB’s implementation details together.</p>
</div><div class="article-footer-copyright"><p> Written in Chinese, LLM translated into English</p><p>Non-commercial reproduction is allowed with proper attribution to the author and source. </p><p>For commercial reproduction, please contact the <a href="mailto:xuezaigds@gmail.com">author</a></p></div><div class="post-donate"><div class="donate_bar center" id="donate_board"><a class="btn_donate" id="btn_donate" href="javascript:;" title="Sponsor"></a><div class="donate_txt"> ↑<br>Good content, Sponsor it<br></div></div><div class="donate_bar center hidden" id="donate_guide"><img src="https://slefboot-1251736664.file.myqcloud.com/weixin.jpg" title="WeChat Sponsor"><img src="https://slefboot-1251736664.file.myqcloud.com/zhifubao.jpg" title="Alipay Sponsor"></div><script type="text/javascript">document.getElementById('btn_donate').onclick = function(){
    $('#donate_board').addClass('hidden');
    $('#donate_guide').removeClass('hidden');
}</script></div><div class="addthis_sharing_toolbox"></div><div class="tags"><a href="/tags/C/"><i class="fa fa-tag"></i>C++</a><a href="/tags/LevelDB/"><i class="fa fa-tag"></i>LevelDB</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://selfboot.cn/en/2025/01/24/leveldb_source_writedb/';
    this.page.identifier = 'en/2025/01/24/leveldb_source_writedb/';
    this.page.title = 'LevelDB Explained - Implementation and Optimization Details of Key-Value Writing';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//xuelangZF.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//xuelangZF.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://xuelangZF.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });</script></div><script type="text/javascript">document.addEventListener('DOMContentLoaded', function () {
  var disqusThread = document.getElementById('disqus_thread');
  if (!disqusThread) return;
  
  function removeAdIframes() {
    var iframes = disqusThread.getElementsByTagName('iframe');
    for (var i = iframes.length - 1; i >= 0; i--) {
      var iframe = iframes[i];
      if (iframe.src && iframe.src.indexOf("tempest.services.disqus.com/ads-iframe") !== -1) {
        iframe.parentNode.removeChild(iframe);
      }
    }
  }
  
  removeAdIframes();
  
  var observer = new MutationObserver(function(mutations) {
    mutations.forEach(function(mutation) {
      removeAdIframes();
    });
  });
  observer.observe(disqusThread, { childList: true, subtree: true });
});
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="https://www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://selfboot.cn"></form></div><div class="widget recommendations-widget"><div class="widget-title"> Recommended</div><div class="recommendations-container"><div class="recommendation-item"><a class="promo-link" href="https://puzzles-game.com/" target="_blank"><div class="promo-content"><i class="fa fa-gamepad"></i><span class="promo-text">Train Your Brain And Stay Smart</span></div></a></div><div class="recommendation-item"><a class="promo-link" href="https://gallery.selfboot.cn" target="_blank"><div class="promo-content"><i class="fa fa-robot"></i><span class="promo-text">Use AI And Help Me Make Things</span></div></a></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"></i><span>Categories</span></div><ul><li><a href="/en/categories/Programming/">Programming</a> (19)</li><li><a href="/en/categories/Source-Code-Analysis/">Source Code Analysis</a> (21)</li><li><a href="/en/categories/Artificial-Intelligence/">Artificial Intelligence</a> (14)</li><li><a href="/en/categories/Discovery/">Discovery</a> (1)</li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"></i><span>Tags</span></div><div class="tagcloud"><a href="/en/tags/Python/" style="font-size: 15.00px;">Python</a> <a href="/en/tags/Google/" style="font-size: 15.00px;">Google</a> <a href="/en/tags/C/" style="font-size: 15.00px;">C++</a> <a href="/en/tags/ChatGPT/" style="font-size: 15.00px;">ChatGPT</a> <a href="/en/tags/Prompt/" style="font-size: 15.00px;">Prompt</a> <a href="/en/tags/Redis/" style="font-size: 15.00px;">Redis</a> <a href="/en/tags/Debug/" style="font-size: 15.00px;">Debug</a> <a href="/en/tags/eBPF/" style="font-size: 15.00px;">eBPF</a> <a href="/en/tags/Go/" style="font-size: 15.00px;">Go</a> <a href="/en/tags/Frontend/" style="font-size: 15.00px;">Frontend</a> <a href="/en/tags/Gemini/" style="font-size: 15.00px;">Gemini</a> <a href="/en/tags/SEO/" style="font-size: 15.00px;">SEO</a> <a href="/en/tags/LLM/" style="font-size: 15.00px;">LLM</a> <a href="/en/tags/Web/" style="font-size: 15.00px;">Web</a> <a href="/en/tags/LevelDB/" style="font-size: 15.00px;">LevelDB</a></div></div><!-- Debug: page.path = en/2025/01/24/leveldb_source_writedb/ --><div class="widget"><div class="widget-title"><i class="fa fa-file-o"></i><span>Recent</span></div><ul><li><a href="/en/2025/06/27/leveldb_source_table_build/" title="LevelDB Explained - A Step by Step Guide to SSTable Build">LevelDB Explained - A Step by Step Guide to SSTable Build</a></li><li><a href="/en/2025/06/13/leveldb_source_LRU_cache/" title="LevelDB Explained - The Implementation Details of a High-Performance LRU Cache">LevelDB Explained - The Implementation Details of a High-Performance LRU Cache</a></li><li><a href="/en/2025/06/11/leveldb_source_memtable/" title="LevelDB Explained - The Implementation Details of MemTable">LevelDB Explained - The Implementation Details of MemTable</a></li><li><a href="/en/2025/06/10/leveldb_mvcc_intro/" title="LevelDB Explained - Understanding Multi-Version Concurrency Control (MVCC)">LevelDB Explained - Understanding Multi-Version Concurrency Control (MVCC)</a></li><li><a href="/en/2025/05/23/mcp_user_report/" title="In-depth Experience with 3 MCP Servers via Cursor: Impressive but Not Yet Practical?">In-depth Experience with 3 MCP Servers via Cursor: Impressive but Not Yet Practical?</a></li><li><a href="/en/2025/01/24/leveldb_source_writedb/" title="LevelDB Explained - Implementation and Optimization Details of Key-Value Writing">LevelDB Explained - Implementation and Optimization Details of Key-Value Writing</a></li><li><a href="/en/2025/01/13/leveldb_source_write_batch/" title="LevelDB Explained - Elegant Merging of Write and Delete Operations">LevelDB Explained - Elegant Merging of Write and Delete Operations</a></li><li><a href="/en/2025/01/10/c++_crash_cases/" title="5 Real-world Cases of C++ Process Crashes from Production">5 Real-world Cases of C++ Process Crashes from Production</a></li><li><a href="/en/2025/01/02/leveldb_source_thread_anno/" title="LevelDB Explained - Static Thread Safety Analysis with Clang">LevelDB Explained - Static Thread Safety Analysis with Clang</a></li><li><a href="/en/2024/12/25/leveldb_source_hashtable/" title="LevelDB Explained - How to Design a High-Performance HashTable">LevelDB Explained - How to Design a High-Performance HashTable</a></li></ul></div><!-- Debug: Current Language = en, Filtered Posts Count = 10 --><div class="widget" id="toc"><div class="widget-title"><i class="fa fa-list-ul"></i><span> Contents</span></div><ul class="dsq-widget-list"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Two-Ways-to-Write-Keys-in-LevelDB"><span class="toc-number">1.</span> <span class="toc-text">Two Ways to Write Keys in LevelDB</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Detailed-Write-Steps-in-LevelDB"><span class="toc-number">2.</span> <span class="toc-text">Detailed Write Steps in LevelDB</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Writer-Structure-and-Processing-Queue"><span class="toc-number">2.1.</span> <span class="toc-text">Writer Structure and Processing Queue</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pre-allocating-Space"><span class="toc-number">2.2.</span> <span class="toc-text">Pre-allocating Space</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Merging-Write-Tasks"><span class="toc-number">2.3.</span> <span class="toc-text">Merging Write Tasks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Writing-to-WAL-and-MemTable"><span class="toc-number">2.4.</span> <span class="toc-text">Writing to WAL and MemTable</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Updating-Batch-Write-Task-Status"><span class="toc-number">2.5.</span> <span class="toc-text">Updating Batch Write Task Status</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Other-Engineering-Implementation-Details"><span class="toc-number">3.</span> <span class="toc-text">Other Engineering Implementation Details</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Handling-Mixed-Sync-and-Non-sync-Writes"><span class="toc-number">3.1.</span> <span class="toc-text">Handling Mixed Sync and Non-sync Writes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Optimizing-Large-Batch-Small-Key-Write-Latency"><span class="toc-number">3.2.</span> <span class="toc-text">Optimizing Large Batch Small Key Write Latency</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#WAL-Success-but-MemTable-Failure"><span class="toc-number">3.3.</span> <span class="toc-text">WAL Success but MemTable Failure</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Summary"><span class="toc-number">4.</span> <span class="toc-text">Summary</span></a></li></ol></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Just For Fun.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/selfboot/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/selfboot"> SelfBoot.</a><p><span id="busuanzi_container_site_pv"></span>Total Site Visits:  <span id="busuanzi_value_site_pv"></span> Times，<span id="busuanzi_container_site_uv"></span>Unique Visitors:  <span id="busuanzi_value_site_uv"></span> People</p><p>Friend Link:<a rel="dofollow" target="_blank" href="https://puzzles-game.com/">Puzzle Games</a></p></div></div></div><link rel="stylesheet" type="text/css" href="/css/donate.css"><link rel="stylesheet" type="text/css" href="/css/copyright.css"><link rel="stylesheet" type="text/css" href="/css/recommendations.css"><a class="show" id="rocket" href="#top" aria-label="Back to top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script><script type="text/javascript" src="/js/jquery.fancybox.min.js" defer=""></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" defer=""></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.min.css"><script type="text/javascript" src="/js/toc.js?v=1.0.0"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>