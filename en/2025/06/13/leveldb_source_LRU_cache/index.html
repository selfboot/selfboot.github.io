<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="baidu-site-verification" content="codeva-NxrUEx2EFN"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="This article delves into the implementation details of the high-performance LRU cache in LevelDB, covering its cache interface design, the LRUHandle data structure, doubly linked list optimizations, and sharding mechanism. By analyzing core designs like clever reference counting management, the dummy node technique, and lock sharding to reduce contention, it showcases the optimization strategies for an industrial-grade cache system. Combining code and diagrams, the article helps readers understand how LevelDB achieves a high-concurrency, high-performance cache and how these design techniques can be applied in their own projects."><title>LevelDB Explained - The Implementation Details of a High-Performance LRU Cache</title><link rel="stylesheet" type="text/css" href="/css/normalize.min.css"><link rel="stylesheet" type="text/css" href="/css/pure-min.min.css"><link rel="stylesheet" type="text/css" href="/css/grids-responsive-min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"><script type="text/javascript" src="/js/jquery.min.js"></script><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml,en/atom.xml"><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;973c5bad683a49b8af76df256779f523&quot;}"></script>
<script defer="" src="https://cloud.umami.is/script.js" data-website-id="3578d9f2-9ea0-49d4-8781-e8d1217ab924" data-domains="selfboot.cn"></script><script async="" src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="referrer" content="no-referrer-when-downgrade"><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QNFB9JLSPV"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QNFB9JLSPV');
</script><script async="" type="text/javascript" src="/js/clipboard.min.js"></script><script async="" type="text/javascript" src="/js/toastr.min.js"></script><link rel="stylesheet" href="/css/toastr.min.css"><script>function switchLanguage(lang) {
  var currentPath = window.location.pathname;
  var newPath;
  if (lang === 'en') {
    newPath = '/en' + currentPath.replace(/^\/(zh-CN\/)?/, '/');
  } else {
    newPath = currentPath.replace(/^\/en/, '');
  }
  window.location.href = newPath;
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><a id="logo" href="/.">Just For Fun</a><div class="lang-select-wrapper"><i class="fas fa-globe"></i><select id="lang-select" onchange="switchLanguage(this.value)"><option value="zh-CN">中文</option><option value="en" selected="">English</option></select></div><p class="description">Know what it is, and know why it is so. The breadth of knowledge is a byproduct of its depth!</p></div><div id="nav-menu"><a href="/en/."><i class="fa fa-home"></i><span> Home</span></a><a href="/en/archives/"><i class="fa fa-archive"></i><span> Archive</span></a><a href="/en/aboutme.html"><i class="fa fa-user"></i><span> About</span></a><a href="/en/atom.xml"><i class="fa fa-rss"></i><span> RSS</span></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">LevelDB Explained - The Implementation Details of a High-Performance LRU Cache</h1><div class="post-meta">2025/06/13<span> | </span><span class="category"><a href="/categories/Source-Code-Analysis/">Source Code Analysis</a></span><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a class="disqus-comment-count" data-disqus-identifier="en/2025/06/13/leveldb_source_LRU_cache/" href="/en/2025/06/13/leveldb_source_LRU_cache/#disqus_thread"></a><div class="post-content"><p>In computer systems, caches are ubiquitous. From CPU caches to memory caches, from disk caches to network caches, they are everywhere. The core idea of a cache is to trade space for time by storing frequently accessed “hot” data in high-performance storage to improve performance. Since caching devices are expensive, their storage size is limited, which means some cached data must be evicted. The eviction policy is crucial here; an unreasonable policy might evict data that is about to be accessed, leading to a very low cache hit rate.</p>
<p>There are many cache eviction policies, such as LRU, LFU, and FIFO. Among them, LRU (Least Recently Used) is a classic strategy. Its core idea is: <strong>when the cache is full, evict the least recently used data</strong>. This is based on the empirical assumption that “<strong>if data has been accessed recently, it is more likely to be accessed again in the future</strong>.” As long as this assumption holds, LRU can significantly improve the cache hit rate.</p>
<p>LevelDB implements an in-memory LRU Cache to store hot data, enhancing read and write performance. By default, LevelDB caches sstable indexes and data blocks. The sstable cache is configured to hold 990 (1000-10) entries, while the data block cache is allocated 8MB by default.</p>
<p>The LRU cache implemented in LevelDB is a sharded LRU with many detailed optimizations, making it an excellent case study. This article will start with the classic LRU implementation and then progressively analyze the implementation details of LevelDB’s <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/util/cache.cc">LRU Cache</a>.</p>
<h2 id="Classic-LRU-Implementation"><a href="#Classic-LRU-Implementation" class="headerlink" title="Classic LRU Implementation"></a>Classic LRU Implementation</h2><p>A well-implemented LRU needs to support insertion, lookup, and deletion operations in $O(1)$ time complexity. The classic approach uses <strong>a doubly linked list and a hash table</strong>:</p>
<ul>
<li><strong>The doubly linked list</strong> stores the cache entries and maintains their usage order. Recently accessed items are moved to the head of the list, while the least recently used items gradually move towards the tail. When the cache reaches its capacity and needs to evict data, the item at the tail of the list (the least recently used item) is removed.</li>
<li><strong>The hash table</strong> stores the mapping from keys to their corresponding nodes in the doubly linked list, allowing any data item to be accessed and located in constant time. The hash table’s keys are the data item keys, and the values are pointers to the corresponding nodes in the doubly linked list.</li>
</ul>
<p><strong>The doubly linked list ensures constant-time node addition and removal, while the hash table provides constant-time data access</strong>. For a get operation, the hash table quickly locates the node in the list. If it exists, it’s moved to the head of the list, marking it as recently used. For an insert operation, if the data already exists, its value is updated, and the node is moved to the head. If it doesn’t exist, a new node is inserted at the head, a mapping is added to the hash table, and if the capacity is exceeded, the tail node is removed from the list and its mapping is deleted from the hash table.</p>
<p>This implementation approach is familiar to anyone who has studied algorithms. There are LRU implementation problems on LeetCode, such as <a target="_blank" rel="noopener" href="https://leetcode.com/problems/lru-cache/">146. LRU Cache</a>, which requires implementing the following interface:</p>
<figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LRUCache</span> {</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">LRUCache</span>(<span class="type">int</span> capacity) {</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> key)</span> </span>{</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">put</span><span class="params">(<span class="type">int</span> key, <span class="type">int</span> value)</span> </span>{</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure>

<p>However, implementing an industrial-grade, high-performance LRU cache is still quite challenging. Next, let’s see how LevelDB does it.</p>
<h2 id="Cache-Design-Dependency-Inversion"><a href="#Cache-Design-Dependency-Inversion" class="headerlink" title="Cache Design: Dependency Inversion"></a>Cache Design: Dependency Inversion</h2><p>Before diving into LevelDB’s LRU Cache implementation, let’s look at how the cache is used. For example, in <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/db/table_cache.cc">db/table_cache.cc</a>, to cache SSTable metadata, the TableCache class defines a member variable of type Cache and uses it to perform various cache operations.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Cache* <span class="title">cache_</span><span class="params">(NewLRUCache(entries))</span></span>;</span><br><span class="line"></span><br><span class="line">*handle = cache_-&gt;<span class="built_in">Lookup</span>(key);</span><br><span class="line">*handle = cache_-&gt;<span class="built_in">Insert</span>(key, tf, <span class="number">1</span>, &amp;DeleteEntry);</span><br><span class="line">cache_-&gt;<span class="built_in">Release</span>(handle);</span><br><span class="line">cache_-&gt;<span class="built_in">Erase</span>(<span class="built_in">Slice</span>(buf, <span class="built_in">sizeof</span>(buf)));</span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></tbody></table></figure>

<p>Here, Cache is an abstract class that defines the various interfaces for cache operations, as defined in <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/include/leveldb/cache.h">include/leveldb/cache.h</a>. It specifies basic operations like Insert, Lookup, Release, and Erase. It also defines the Cache::Handle type to represent a cache entry. User code interacts only with this abstract interface, without needing to know the concrete implementation.</p>
<p>Then there is the LRUCache class, which is the concrete implementation of a complete LRU cache. This class is not directly exposed to the outside world, nor does it directly inherit from Cache. There is also a ShardedLRUCache class that inherits from Cache and implements the cache interfaces. It contains 16 LRUCache “shards,” each responsible for caching a portion of the data.</p>
<p>This design allows callers to <strong>easily swap out different cache implementations without modifying the code that uses the cache</strong>. Ha, isn’t this the classic <strong>Dependency Inversion Principle from SOLID object-oriented programming</strong>? The application layer depends on an abstract interface (Cache) rather than a concrete implementation (LRUCache). This reduces code coupling and improves the system’s extensibility and maintainability.</p>
<p>When using the cache, a factory function is used to create the concrete cache implementation, <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/util/cache.cc#L339">ShardedLRUCache</a>:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Cache* <span class="title">NewLRUCache</span><span class="params">(<span class="type">size_t</span> capacity)</span> </span>{ <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">ShardedLRUCache</span>(capacity); }</span><br><span class="line"></span><br><span class="line"><span class="comment">// New cache implementations can be added at any time.</span></span><br><span class="line"><span class="comment">// Cache* NewClockCache(size_t capacity);</span></span><br></pre></td></tr></tbody></table></figure>

<p>LRUCache is the core part of the cache, but for now, let’s put aside its implementation and first look at the <strong>design of the cache entry, Handle</strong>.</p>
<h2 id="LRUHandle-Class-Implementation"><a href="#LRUHandle-Class-Implementation" class="headerlink" title="LRUHandle Class Implementation"></a>LRUHandle Class Implementation</h2><p>In LevelDB, a cached data item is an LRUHandle class, defined in <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/util/cache.cc#L43">util/cache.cc</a>. The comments here explain that LRUHandle is a heap-allocated, variable-length structure that is stored in a doubly-linked list, ordered by access time. Let’s look at the members of this struct:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">LRUHandle</span> {</span><br><span class="line">  <span class="type">void</span>* value;</span><br><span class="line">  <span class="built_in">void</span> (*deleter)(<span class="type">const</span> Slice&amp;, <span class="type">void</span>* value);</span><br><span class="line">  LRUHandle* next_hash;</span><br><span class="line">  LRUHandle* next;</span><br><span class="line">  LRUHandle* prev;</span><br><span class="line">  <span class="type">size_t</span> charge;  <span class="comment">// TODO(opt): Only allow uint32_t?</span></span><br><span class="line">  <span class="type">size_t</span> key_length;</span><br><span class="line">  <span class="type">bool</span> in_cache;      <span class="comment">// Whether entry is in the cache.</span></span><br><span class="line">  <span class="type">uint32_t</span> refs;      <span class="comment">// References, including cache reference, if present.</span></span><br><span class="line">  <span class="type">uint32_t</span> hash;      <span class="comment">// Hash of key(); used for fast sharding and comparisons</span></span><br><span class="line">  <span class="type">char</span> key_data[<span class="number">1</span>];   <span class="comment">// Beginning of key</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>This is a bit complex, and each field is quite important. Let’s go through them one by one.</p>
<ul>
<li><strong>value</strong>: Stores the actual value of the cache entry. It’s a <code>void*</code> pointer, meaning the cache layer is agnostic to the value’s structure; it only needs the object’s address.</li>
<li><strong>deleter</strong>: A function pointer to a callback used to delete the cached value. When a cache entry is removed, this is used to free the memory of the cached value.</li>
<li><strong>next_hash</strong>: The LRU cache implementation requires a hash table. LevelDB implements its own high-performance hash table. As we discussed in <a href="/en/2024/12/25/leveldb_source_hashtable/">LevelDB Explained - How to Design a High-Performance HashTable</a>, the next_hash field of LRUHandle is used to resolve hash collisions.</li>
<li><strong>prev/next</strong>: Pointers to the previous/next node in the doubly linked list, used to maintain the list for fast insertion and deletion of nodes.</li>
<li><strong>charge</strong>: Represents the cost of this cache entry (usually its memory size), used to calculate the total cache usage and determine if eviction is necessary.</li>
<li><strong>key_length</strong>: The length of the key, used to construct a Slice object representing the key.</li>
<li><strong>in_cache</strong>: A flag indicating whether the entry is in the cache. If true, it means the cache holds a reference to this entry.</li>
<li><strong>refs</strong>: A reference count, including the cache’s own reference (if in the cache) and references from users. When the count drops to 0, the entry can be deallocated.</li>
<li><strong>hash</strong>: The hash value of the key, used for fast lookups and sharding. Storing the hash value here avoids re-computation for the same key.</li>
<li><strong>key_data</strong>: A flexible array member that stores the actual key data. malloc is used to allocate enough space to store the entire key. We also discussed flexible array members in <a href="/en/2024/08/13/leveldb_source_unstand_c++/">LevelDB Explained - Understanding Advanced C++ Techniques</a>.</li>
</ul>
<p>The design of LRUHandle allows the cache to efficiently manage data items, track their references, and implement the LRU eviction policy. In particular, the combination of the in_cache and refs fields allows us to distinguish between items that are “<strong>cached but not referenced by clients</strong>“ and those that are “<strong>referenced by clients</strong>,” enabling an efficient eviction strategy using two linked lists.</p>
<p>Next, we’ll examine the implementation details of the LRUCache class to better understand the purpose of these fields.</p>
<h2 id="LRUCache-Class-Implementation-Details"><a href="#LRUCache-Class-Implementation-Details" class="headerlink" title="LRUCache Class Implementation Details"></a>LRUCache Class Implementation Details</h2><p>Having looked at the design of the cache entry, we can now examine the concrete implementation details of LevelDB’s LRUCache. The core caching logic is implemented in the LRUCache class in <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/util/cache.cc#L151">util/cache.cc</a>. This class contains the core logic for operations like insertion, lookup, deletion, and eviction.</p>
<p>The comments here (LevelDB’s comments are always worth reading carefully) mention that it uses two doubly linked lists to maintain cache items. Why is that?</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The cache keeps two linked lists of items in the cache.  All items in the</span></span><br><span class="line"><span class="comment">// cache are in one list or the other, and never both.  Items still referenced</span></span><br><span class="line"><span class="comment">// by clients but erased from the cache are in neither list.  The lists are:</span></span><br><span class="line"><span class="comment">// - in-use:  contains the items currently referenced by clients, in no</span></span><br><span class="line"><span class="comment">//   particular order.  (This list is used for invariant checking.  If we</span></span><br><span class="line"><span class="comment">//   removed the check, elements that would otherwise be on this list could be</span></span><br><span class="line"><span class="comment">//   left as disconnected singleton lists.)</span></span><br><span class="line"><span class="comment">// - LRU:  contains the items not currently referenced by clients, in LRU order</span></span><br><span class="line"><span class="comment">// Elements are moved between these lists by the Ref() and Unref() methods,</span></span><br><span class="line"><span class="comment">// when they detect an element in the cache acquiring or losing its only</span></span><br><span class="line"><span class="comment">// external reference.</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="Why-Use-Two-Doubly-Linked-Lists"><a href="#Why-Use-Two-Doubly-Linked-Lists" class="headerlink" title="Why Use Two Doubly Linked Lists?"></a>Why Use Two Doubly Linked Lists?</h3><p>As mentioned earlier, a typical LRU Cache implementation uses a single doubly linked list. Each time a cache item is used, it’s moved to the head of the list, making the tail the least recently used item. For eviction, the node at the tail is simply removed. The LeetCode problem mentioned earlier can be solved this way, where each cache item is just an int, and its value is copied on access. <strong>If the cached items are simple value types that can be copied directly on read without needing references, a single linked list is sufficient</strong>.</p>
<p>However, in LevelDB, the cached data items are LRUHandle objects, which are dynamically allocated, variable-length structures. For high concurrency and performance, they cannot be managed by simple value copying but must be managed through reference counting. If we were to use a single linked list, consider this scenario.</p>
<p>We access items A, C, and D in order, and finally access B. Item B is referenced by a client (refs &gt; 1) and is at the head of the list, as shown in the initial state in the diagram below. Over time, A, C, and D are accessed, but B is not. According to the LRU rule, A, C, and D are moved to the head. <strong>Although B is still referenced, its relative position moves towards the tail because it hasn’t been accessed for a long time</strong>. After A and D are accessed and quickly released, they have no external references. When eviction is needed, we start from the tail and find that item B (refs &gt; 1) cannot be evicted. We would have to skip it and continue traversing the list to find an evictable item.</p>
<p><img src="https://slefboot-1251736664.file.myqcloud.com/20250612_leveldb_source_lru_cache_en.webp/webp" alt="LRUCache Doubly Linked List State" srcset="https://slefboot-1251736664.file.myqcloud.com/20250612_leveldb_source_lru_cache_en.webp/webp 1278w, https://slefboot-1251736664.file.myqcloud.com/20250612_leveldb_source_lru_cache_en.webp/webp400 400w, https://slefboot-1251736664.file.myqcloud.com/20250612_leveldb_source_lru_cache_en.webp/webp800 800w" sizes="(min-width: 1150px) 723px, (min-width: 48em) calc((100vw - 120px) * 3 / 4 - 50px), (min-width: 35.5em) calc((100vw - 75px), calc(100vw - 40px)" width="1278" height="298"></p>
<p>In other words, in this reference-based scenario, when evicting a node, if the node at the tail of the list is currently referenced externally (refs &gt; 1), it cannot be evicted. This requires <strong>traversing the list to find an evictable node, which is inefficient</strong>. In the worst case, if all nodes are referenced, the entire list might be traversed without being able to evict anything.</p>
<p>To solve this problem, the LRUCache implementation uses two doubly linked lists. One is <strong>in_use_</strong>, which stores referenced cache items. The other is <strong>lru_</strong>, which stores unreferenced cache items. Each cache item can only be in one of these lists at a time, never both. However, an item can move between the two lists depending on whether it’s currently referenced. This way, when a node needs to be evicted, it can be taken directly from the lru_ list without traversing the in_use_ list.</p>
<p>That’s the introduction to the dual linked list design. We’ll understand it better by looking at the core implementation of LRUCache.</p>
<h3 id="Cache-Insertion-Deletion-and-Lookup"><a href="#Cache-Insertion-Deletion-and-Lookup" class="headerlink" title="Cache Insertion, Deletion, and Lookup"></a>Cache Insertion, Deletion, and Lookup</h3><p>Let’s first look at node insertion, implemented in <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/util/cache.cc#L267">util/cache.cc</a>. In short, an LRUHandle object is created, placed in the in_use_ doubly linked list, and the hash table is updated. If the cache capacity is reached after insertion, a node needs to be evicted. However, the implementation has many subtle details, and LevelDB’s code is indeed concise.</p>
<p>Let’s look at the parameters: key and value are passed by the client, hash is the hash of the key, charge is the cost of the cache item, and deleter is the callback function for deleting the item. Since LRUHandle has a flexible array member at the end, we first manually calculate the size of the LRUHandle object, allocate memory, and then initialize its members. Here, refs is initialized to 1 because a Handle pointer is returned.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Cache::Handle* <span class="title">LRUCache::Insert</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">uint32_t</span> hash, <span class="type">void</span>* value,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">size_t</span> charge,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">void</span> (*deleter)(<span class="type">const</span> Slice&amp; key,</span></span></span><br><span class="line"><span class="params"><span class="function">                                                <span class="type">void</span>* value))</span> </span>{</span><br><span class="line">  <span class="function">MutexLock <span class="title">l</span><span class="params">(&amp;mutex_)</span></span>;</span><br><span class="line"></span><br><span class="line">  LRUHandle* e =</span><br><span class="line">      <span class="built_in">reinterpret_cast</span>&lt;LRUHandle*&gt;(<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(LRUHandle) - <span class="number">1</span> + key.<span class="built_in">size</span>()));</span><br><span class="line">  e-&gt;value = value;</span><br><span class="line">  e-&gt;deleter = deleter;</span><br><span class="line">  e-&gt;charge = charge;</span><br><span class="line">  e-&gt;key_length = key.<span class="built_in">size</span>();</span><br><span class="line">  e-&gt;hash = hash;</span><br><span class="line">  e-&gt;in_cache = <span class="literal">false</span>;</span><br><span class="line">  e-&gt;refs = <span class="number">1</span>;  <span class="comment">// for the returned handle.</span></span><br><span class="line">  std::<span class="built_in">memcpy</span>(e-&gt;key_data, key.<span class="built_in">data</span>(), key.<span class="built_in">size</span>());</span><br></pre></td></tr></tbody></table></figure>

<p>The next part is also interesting. LevelDB’s LRUCache implementation supports a cache capacity of 0, which means no data is cached. To cache an item, in_cache is set to true, and the refs count is incremented because the Handle object is placed in the in_use_ list. The handle is also inserted into the hash table. Note the FinishErase call here, which is worth discussing.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (capacity_ &gt; <span class="number">0</span>) {</span><br><span class="line">  e-&gt;refs++;  <span class="comment">// for the cache's reference.</span></span><br><span class="line">  e-&gt;in_cache = <span class="literal">true</span>;</span><br><span class="line">  <span class="built_in">LRU_Append</span>(&amp;in_use_, e);</span><br><span class="line">  usage_ += charge;</span><br><span class="line">  <span class="built_in">FinishErase</span>(table_.<span class="built_in">Insert</span>(e));</span><br><span class="line">} <span class="keyword">else</span> {  <span class="comment">// don't cache. (capacity_==0 is supported and turns off caching.)</span></span><br><span class="line">  <span class="comment">// next is read by key() in an assert, so it must be initialized</span></span><br><span class="line">  e-&gt;next = <span class="literal">nullptr</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>As we discussed in the hash table implementation, if a key already exists when inserting into the hash table, the old Handle object is returned. The FinishErase function is used to clean up this old Handle object.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// If e != nullptr, finish removing *e from the cache; it has already been</span></span><br><span class="line"><span class="comment">// removed from the hash table.  Return whether e != nullptr.</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">LRUCache::FinishErase</span><span class="params">(LRUHandle* e)</span> </span>{</span><br><span class="line">  <span class="keyword">if</span> (e != <span class="literal">nullptr</span>) {</span><br><span class="line">    <span class="built_in">assert</span>(e-&gt;in_cache);</span><br><span class="line">    <span class="built_in">LRU_Remove</span>(e);</span><br><span class="line">    e-&gt;in_cache = <span class="literal">false</span>;</span><br><span class="line">    usage_ -= e-&gt;charge;</span><br><span class="line">    <span class="built_in">Unref</span>(e);</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">return</span> e != <span class="literal">nullptr</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>The cleanup involves several steps. First, the old handle is removed from either the in_use_ or lru_ list. It’s not certain which list the old Handle object is in, but that’s okay—<strong>LRU_Remove can handle it without knowing which list it’s in</strong>. The LRU_Remove function is very simple, just two lines of code. If you’re unsure, try drawing a diagram:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">LRUCache::LRU_Remove</span><span class="params">(LRUHandle* e)</span> </span>{</span><br><span class="line">  e-&gt;next-&gt;prev = e-&gt;prev;</span><br><span class="line">  e-&gt;prev-&gt;next = e-&gt;next;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>Next, in_cache is set to false, indicating it’s no longer in the cache. Then, the cache capacity is updated by decrementing usage_. Finally, Unref is called to decrement the reference count of this Handle object, which might still be referenced elsewhere. Only when all references are released will the Handle object be truly deallocated. The <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/util/cache.cc#L226">Unref function</a> is also quite interesting; I’ll post the code here:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">LRUCache::Unref</span><span class="params">(LRUHandle* e)</span> </span>{</span><br><span class="line">  <span class="built_in">assert</span>(e-&gt;refs &gt; <span class="number">0</span>);</span><br><span class="line">  e-&gt;refs--;</span><br><span class="line">  <span class="keyword">if</span> (e-&gt;refs == <span class="number">0</span>) {  <span class="comment">// Deallocate.</span></span><br><span class="line">    <span class="built_in">assert</span>(!e-&gt;in_cache);</span><br><span class="line">    (*e-&gt;deleter)(e-&gt;<span class="built_in">key</span>(), e-&gt;value);</span><br><span class="line">    <span class="built_in">free</span>(e);</span><br><span class="line">  } <span class="keyword">else</span> <span class="keyword">if</span> (e-&gt;in_cache &amp;&amp; e-&gt;refs == <span class="number">1</span>) {</span><br><span class="line">    <span class="comment">// No longer in use; move to lru_ list.</span></span><br><span class="line">    <span class="built_in">LRU_Remove</span>(e);</span><br><span class="line">    <span class="built_in">LRU_Append</span>(&amp;lru_, e);</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>First, the count is decremented. If it becomes 0, it means there are <strong>no external references, and the memory can be safely deallocated</strong>. Deallocation has two parts: first, the deleter callback is used to clean up the memory for the value, and then free is used to release the memory for the LRUHandle pointer. If the count becomes 1 and the handle is still in the cache, it means only the cache itself holds a reference. In this case, <strong>the Handle object needs to be removed from the in_use_ list and moved to the lru_ list</strong>. If a node needs to be evicted later, this node in the lru_ list can be evicted directly.</p>
<p>Now for the final step of the insertion operation: checking if the cache has remaining capacity. If not, eviction begins. As long as the capacity is insufficient, the node at the head of the lru_ list is taken, <strong>removed from the hash table, and then cleaned up using FinishErase</strong>. Checking if the doubly linked list is empty is also interesting; it uses a dummy node, which we’ll discuss later.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (usage_ &gt; capacity_ &amp;&amp; lru_.next != &amp;lru_) {</span><br><span class="line">  LRUHandle* old = lru_.next;</span><br><span class="line">  <span class="built_in">assert</span>(old-&gt;refs == <span class="number">1</span>);</span><br><span class="line">  <span class="type">bool</span> erased = <span class="built_in">FinishErase</span>(table_.<span class="built_in">Remove</span>(old-&gt;<span class="built_in">key</span>(), old-&gt;hash));</span><br><span class="line">  <span class="keyword">if</span> (!erased) {  <span class="comment">// to avoid unused variable when compiled NDEBUG</span></span><br><span class="line">    <span class="built_in">assert</span>(erased);</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>The entire insertion function, including the eviction logic, and indeed the entire LevelDB codebase, is filled with assert statements for various checks, ensuring that the process terminates immediately if something goes wrong, preventing error propagation.</p>
<p>After seeing insertion, deletion is straightforward. The implementation is simple: remove the node from the hash table and then call FinishErase to clean it up.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">LRUCache::Erase</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">uint32_t</span> hash)</span> </span>{</span><br><span class="line">  <span class="function">MutexLock <span class="title">l</span><span class="params">(&amp;mutex_)</span></span>;</span><br><span class="line">  <span class="built_in">FinishErase</span>(table_.<span class="built_in">Remove</span>(key, hash));</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>Node lookup is also relatively simple. It looks up directly from the hash table. If found, the reference count is incremented, and the Handle object is returned. Just like insertion, returning a Handle object increments its reference count. So, if it’s not used externally, you must remember to call the Release method to manually release the reference, otherwise, you could have a memory leak.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Cache::Handle* <span class="title">LRUCache::Lookup</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">uint32_t</span> hash)</span> </span>{</span><br><span class="line">  <span class="function">MutexLock <span class="title">l</span><span class="params">(&amp;mutex_)</span></span>;</span><br><span class="line">  LRUHandle* e = table_.<span class="built_in">Lookup</span>(key, hash);</span><br><span class="line">  <span class="keyword">if</span> (e != <span class="literal">nullptr</span>) {</span><br><span class="line">    <span class="built_in">Ref</span>(e);</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">reinterpret_cast</span>&lt;Cache::Handle*&gt;(e);</span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">LRUCache::Release</span><span class="params">(Cache::Handle* handle)</span> </span>{</span><br><span class="line">  <span class="function">MutexLock <span class="title">l</span><span class="params">(&amp;mutex_)</span></span>;</span><br><span class="line">  <span class="built_in">Unref</span>(<span class="built_in">reinterpret_cast</span>&lt;LRUHandle*&gt;(handle));</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>Additionally, the Cache interface also implements a Prune method for proactively cleaning the cache. The method is similar to the cleanup logic in insertion, but it clears out all nodes in the lru_ list. This function is not used anywhere in LevelDB.</p>
<h3 id="Doubly-Linked-List-Operations"><a href="#Doubly-Linked-List-Operations" class="headerlink" title="Doubly Linked List Operations"></a>Doubly Linked List Operations</h3><p>Let’s discuss the doubly linked list operations in more detail. We already know there are two lists: lru_ and in_use_. The comments make it clearer:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Dummy head of LRU list.</span></span><br><span class="line"><span class="comment">// lru.prev is newest entry, lru.next is oldest entry.</span></span><br><span class="line"><span class="comment">// Entries have refs==1 and in_cache==true.</span></span><br><span class="line"><span class="function">LRUHandle lru_ <span class="title">GUARDED_BY</span><span class="params">(mutex_)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Dummy head of in-use list.</span></span><br><span class="line"><span class="comment">// Entries are in use by clients, and have refs &gt;= 2 and in_cache==true.</span></span><br><span class="line"><span class="function">LRUHandle in_use_ <span class="title">GUARDED_BY</span><span class="params">(mutex_)</span></span>;</span><br></pre></td></tr></tbody></table></figure>

<p>The lru_ member is the list’s dummy node. Its next member points to the oldest cache item in the lru_ list, and its prev member points to the newest. In the LRUCache constructor, the next and prev of lru_ both point to itself, indicating an empty list. Remember how we checked for evictable nodes during insertion? It was with lru_.next != &amp;lru_.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">LRUCache::<span class="built_in">LRUCache</span>() : <span class="built_in">capacity_</span>(<span class="number">0</span>), <span class="built_in">usage_</span>(<span class="number">0</span>) {</span><br><span class="line">  <span class="comment">// Make empty circular linked lists.</span></span><br><span class="line">  lru_.next = &amp;lru_;</span><br><span class="line">  lru_.prev = &amp;lru_;</span><br><span class="line">  in_use_.next = &amp;in_use_;</span><br><span class="line">  in_use_.prev = &amp;in_use_;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>A <strong>dummy node</strong> is a technique used in many data structure implementations to simplify boundary condition handling. In the context of an LRU cache, a dummy node is mainly used as the head of the list, so the head always exists, even when the list is empty. This approach simplifies insertion and deletion operations because <strong>you don’t need special handling for an empty list</strong>.</p>
<p>For example, when adding a new element to the list, you can insert it directly between the dummy node and the current first element without checking if the list is empty. Similarly, when deleting an element, you don’t have to worry about updating the list head after deleting the last element, because the dummy node is always there.</p>
<p>We’ve already seen LRU_Remove, which is just two lines of code. For adding a node to the list, I’ve created a diagram that, combined with the code, should make it easy to understand:</p>
<p><img src="https://slefboot-1251736664.file.myqcloud.com/20250613_leveldb_source_lru_cache_linkedlist.webp/webp" alt="LRUCache Doubly Linked List Operations" srcset="https://slefboot-1251736664.file.myqcloud.com/20250613_leveldb_source_lru_cache_linkedlist.webp/webp 2467w, https://slefboot-1251736664.file.myqcloud.com/20250613_leveldb_source_lru_cache_linkedlist.webp/webp400 400w, https://slefboot-1251736664.file.myqcloud.com/20250613_leveldb_source_lru_cache_linkedlist.webp/webp800 800w, https://slefboot-1251736664.file.myqcloud.com/20250613_leveldb_source_lru_cache_linkedlist.webp/webp1600 1600w" sizes="(min-width: 1150px) 723px, (min-width: 48em) calc((100vw - 120px) * 3 / 4 - 50px), (min-width: 35.5em) calc((100vw - 75px), calc(100vw - 40px)" width="2467" height="1643"></p>
<p>Here, e is the new node being inserted, and list is the dummy node of the list. I’ve used circles for list’s prev and next to indicate they can point to list itself, as in an initial empty list. The insertion happens before the dummy node, so list-&gt;prev is always the newest node in the list, and list-&gt;next is always the oldest. For this kind of list manipulation, drawing a diagram makes everything clear.</p>
<h3 id="reinterpret-cast-Conversion"><a href="#reinterpret-cast-Conversion" class="headerlink" title="reinterpret_cast Conversion"></a>reinterpret_cast Conversion</h3><p>Finally, let’s briefly touch on the use of reinterpret_cast in the code to convert between LRUHandle* and Cache::Handle*. <strong>reinterpret_cast forcibly converts a pointer of one type to a pointer of another type without any type checking</strong>. It doesn’t adjust the underlying data; it just tells the compiler: “Treat this memory address as if it were of this other type.” This operation is generally dangerous and not recommended.</p>
<p>However, LevelDB does this to separate the interface from the implementation. It exposes the concrete internal data structure LRUHandle* to external users as an abstract, opaque handle Cache::Handle*, while internally converting this opaque handle back to the concrete data structure for operations.</p>
<p><strong>In this specific, controlled design pattern, it is completely safe</strong>. This is because only the LRUCache internal code can create an LRUHandle. Any Cache::Handle* returned to an external user always points to a valid LRUHandle object. Any Cache::Handle* passed to LRUCache must have been previously returned by the same LRUCache instance.</p>
<p>As long as these conventions are followed, reinterpret_cast is just switching between “views” of the pointer; the pointer itself always points to a valid, correctly typed object. If a user tries to forge a Cache::Handle* or pass in an unrelated pointer, the program will have undefined behavior, but that’s a misuse of the API.</p>
<h2 id="ShardedLRUCache-Implementation"><a href="#ShardedLRUCache-Implementation" class="headerlink" title="ShardedLRUCache Implementation"></a>ShardedLRUCache Implementation</h2><p>In the LRUCache implementation, insertion, lookup, and deletion operations must be protected by a single mutex. In a multi-threaded environment, if there’s only one large cache, <strong>this lock becomes a global bottleneck</strong>. When multiple threads access the cache simultaneously, only one thread can acquire the lock, and all others must wait, which severely impacts concurrency performance.</p>
<p>To improve performance, ShardedLRUCache divides the cache into multiple shards (the shard_ array), each with its own independent lock. When a request arrives, it is routed to a specific shard based on the key’s hash value. This way, different threads accessing different shards can proceed in parallel because they acquire different locks, thereby reducing lock contention and increasing overall throughput. A diagram might make this clearer (Mermaid code is <a href="/downloads/mermaid_leveldb_lru_cache_shard.txt">here</a>).</p>
<p><img src="https://slefboot-1251736664.file.myqcloud.com/20250613_leveldb_source_lru_cache_shard_en.webp/webp" alt="ShardedLRUCache Implementation" srcset="https://slefboot-1251736664.file.myqcloud.com/20250613_leveldb_source_lru_cache_shard_en.webp/webp 1824w, https://slefboot-1251736664.file.myqcloud.com/20250613_leveldb_source_lru_cache_shard_en.webp/webp400 400w, https://slefboot-1251736664.file.myqcloud.com/20250613_leveldb_source_lru_cache_shard_en.webp/webp800 800w, https://slefboot-1251736664.file.myqcloud.com/20250613_leveldb_source_lru_cache_shard_en.webp/webp1600 1600w" sizes="(min-width: 1150px) 723px, (min-width: 48em) calc((100vw - 120px) * 3 / 4 - 50px), (min-width: 35.5em) calc((100vw - 75px), calc(100vw - 40px)" width="1824" height="843"></p>
<p>So how many shards are needed? LevelDB hardcodes $kNumShards = 1 \ll kNumShardBits$, which evaluates to 16. This is an empirical choice. If the number of shards is too small, say 2 or 4, lock contention can still be severe on servers with many cores. If there are too many shards, the capacity of each shard becomes very small. This could lead to a “hot” shard frequently evicting data while a “cold” shard has plenty of free space, thus lowering the overall cache hit rate.</p>
<p>Choosing 16 provides sufficient concurrency for typical 8-core or 16-core servers without introducing excessive overhead. Also, choosing a power of two allows for fast shard index calculation using the bitwise operation $hash \gg (32 - kNumShardBits)$.</p>
<p>With sharding, the original LRUCache class is wrapped. The constructor needs to specify the number of shards, the capacity per shard, etc. The <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/util/cache.cc#L352">implementation</a> is as follows:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span>:</span><br><span class="line"> <span class="function"><span class="keyword">explicit</span> <span class="title">ShardedLRUCache</span><span class="params">(<span class="type">size_t</span> capacity)</span> : last_id_(<span class="number">0</span>) {</span></span><br><span class="line">   <span class="type">const</span> <span class="type">size_t</span> per_shard = (capacity + (kNumShards - <span class="number">1</span>)) / kNumShards;</span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> s = <span class="number">0</span>; s &lt; kNumShards; s++) {</span><br><span class="line">     shard_[s].<span class="built_in">SetCapacity</span>(per_shard);</span><br><span class="line">   }</span><br><span class="line"> }</span><br></pre></td></tr></tbody></table></figure>

<p>Other related cache operations, like insertion, lookup, and deletion, use the Shard function to determine which shard to operate on. Here is insertion as an example, <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/util/cache.cc%23L359">implemented here</a>:</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Handle* <span class="title">Insert</span><span class="params">(<span class="type">const</span> Slice&amp; key, <span class="type">void</span>* value, <span class="type">size_t</span> charge,</span></span></span><br><span class="line"><span class="params"><span class="function">               <span class="type">void</span> (*deleter)(<span class="type">const</span> Slice&amp; key, <span class="type">void</span>* value))</span> <span class="keyword">override</span> </span>{</span><br><span class="line">  <span class="type">const</span> <span class="type">uint32_t</span> hash = <span class="built_in">HashSlice</span>(key);</span><br><span class="line">  <span class="keyword">return</span> shard_[<span class="built_in">Shard</span>(hash)].<span class="built_in">Insert</span>(key, hash, value, charge, deleter);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>The HashSlice and Shard functions are straightforward, so we’ll skip them. It’s also worth noting that ShardedLRUCache inherits from the Cache abstract class and implements its various interfaces. This allows it to be used wherever a Cache interface is expected.</p>
<p>Finally, there’s one more small detail worth mentioning: the Cache interface has a NewId function. I haven’t seen other LRU cache implementations that support generating an ID from the cache. <strong>Why does LevelDB do this?</strong></p>
<h3 id="Cache-ID-Generation"><a href="#Cache-ID-Generation" class="headerlink" title="Cache ID Generation"></a>Cache ID Generation</h3><p>LevelDB provides comments, but they might not be clear without context. Let’s analyze this with a use case.</p>
<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Return a new numeric id.  May be used by multiple clients who are</span></span><br><span class="line"><span class="comment">// sharing the same cache to partition the key space.  Typically the</span></span><br><span class="line"><span class="comment">// client will allocate a new id at startup and prepend the id to</span></span><br><span class="line"><span class="comment">// its cache keys.</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="type">uint64_t</span> <span class="title">NewId</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br></pre></td></tr></tbody></table></figure>

<p>For some background, when we open a LevelDB database, we can create a Cache object and pass it in options.block_cache to cache data blocks and filter blocks from SSTable files. If we don’t pass one, LevelDB creates an 8 MB ShardedLRUCache object by default. This <strong>Cache object is globally shared; all Table objects in the database use this same BlockCache instance to cache their data blocks</strong>.</p>
<p>In Table::Open in <a target="_blank" rel="noopener" href="https://github.com/google/leveldb/blob/main/table/table.cc#L72">table/table.cc</a>, we see that every time an SSTable file is opened, NewId is called to generate a cache_id. Under the hood, a mutex ensures that the generated ID is globally increasing. Later, when we need to read a data block at offset from an SSTable file, we use &lt;cache_id, offset&gt; as the cache key. This way, different SSTable files have different cache_ids, so even if their offsets are the same, the cache keys will be different, preventing collisions.</p>
<p>Simply put, ShardedLRUCache provides globally increasing IDs mainly to distinguish between different SSTable files, saving each file from having to maintain its own unique ID for cache keys.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Alright, that’s our analysis of LevelDB’s LRU Cache. We’ve seen the design philosophy and implementation details of an industrial-grade, high-performance cache. Let’s summarize the key points:</p>
<ol>
<li><strong>Interface and Implementation Separation</strong>: By using an abstract Cache interface, the cache’s users are decoupled from its concrete implementation, reflecting the Dependency Inversion Principle of object-oriented design. User code only needs to interact with the Cache interface.</li>
<li><strong>Carefully Designed Cache Entries</strong>: The LRUHandle struct includes metadata like reference counts and cache flags. It uses a flexible array member to store variable-length keys, reducing memory allocation overhead and improving performance.</li>
<li><strong>Dual Linked List Optimization</strong>: Using two doubly linked lists (in_use_ and lru_) to manage “in-use” and “evictable” cache items avoids traversing the entire list during eviction, thus improving eviction efficiency.</li>
<li><strong>Dummy Node Technique</strong>: Using dummy nodes simplifies linked list operations by eliminating the need to handle special cases for empty lists, making the code more concise.</li>
<li><strong>Sharding to Reduce Lock Contention</strong>: ShardedLRUCache divides the cache into multiple shards, each with its own lock, significantly improving concurrency performance in multi-threaded environments.</li>
<li><strong>Reference Counting for Memory Management</strong>: Precise reference counting ensures that cache entries are not deallocated while still referenced externally, and that memory is reclaimed promptly when they are no longer needed.</li>
<li><strong>Assertions for Correctness</strong>: Extensive use of assertions checks preconditions and invariants, ensuring that errors are detected early.</li>
</ol>
<p>These design ideas and implementation techniques are well worth learning from for our own projects. Especially in high-concurrency, high-performance scenarios, the optimization methods used in LevelDB can help us design more efficient cache systems.</p>
</div><div class="article-footer-copyright"><p> Written in Chinese, LLM translated into English</p><p>Non-commercial reproduction is allowed with proper attribution to the author and source. </p><p>For commercial reproduction, please contact the <a href="mailto:xuezaigds@gmail.com">author</a></p></div><div class="post-donate"><div class="donate_bar center" id="donate_board"><a class="btn_donate" id="btn_donate" href="javascript:;" title="Sponsor"></a><div class="donate_txt"> ↑<br>Good content, Sponsor it<br></div></div><div class="donate_bar center hidden" id="donate_guide"><img src="https://slefboot-1251736664.file.myqcloud.com/weixin.jpg" title="WeChat Sponsor"><img src="https://slefboot-1251736664.file.myqcloud.com/zhifubao.jpg" title="Alipay Sponsor"></div><script type="text/javascript">document.getElementById('btn_donate').onclick = function(){
    $('#donate_board').addClass('hidden');
    $('#donate_guide').removeClass('hidden');
}</script></div><div class="addthis_sharing_toolbox"></div><div class="tags"><a href="/tags/C/"><i class="fa fa-tag"></i>C++</a><a href="/tags/LevelDB/"><i class="fa fa-tag"></i>LevelDB</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://selfboot.cn/en/2025/06/13/leveldb_source_LRU_cache/';
    this.page.identifier = 'en/2025/06/13/leveldb_source_LRU_cache/';
    this.page.title = 'LevelDB Explained - The Implementation Details of a High-Performance LRU Cache';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//xuelangZF.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//xuelangZF.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://xuelangZF.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });</script></div><script type="text/javascript">document.addEventListener('DOMContentLoaded', function () {
  var disqusThread = document.getElementById('disqus_thread');
  if (!disqusThread) return;
  
  function removeAdIframes() {
    var iframes = disqusThread.getElementsByTagName('iframe');
    for (var i = iframes.length - 1; i >= 0; i--) {
      var iframe = iframes[i];
      if (iframe.src && iframe.src.indexOf("tempest.services.disqus.com/ads-iframe") !== -1) {
        iframe.parentNode.removeChild(iframe);
      }
    }
  }
  
  removeAdIframes();
  
  var observer = new MutationObserver(function(mutations) {
    mutations.forEach(function(mutation) {
      removeAdIframes();
    });
  });
  observer.observe(disqusThread, { childList: true, subtree: true });
});
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="https://www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://selfboot.cn"></form></div><div class="widget recommendations-widget"><div class="widget-title"> Recommended</div><div class="recommendations-container"><div class="recommendation-item"><a class="promo-link" href="https://puzzles-game.com/" target="_blank"><div class="promo-content"><i class="fa fa-gamepad"></i><span class="promo-text">Train Your Brain And Stay Smart</span></div></a></div><div class="recommendation-item"><a class="promo-link" href="https://gallery.selfboot.cn" target="_blank"><div class="promo-content"><i class="fa fa-robot"></i><span class="promo-text">Use AI And Help Me Make Things</span></div></a></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"></i><span>Categories</span></div><ul><li><a href="/en/categories/Programming/">Programming</a> (19)</li><li><a href="/en/categories/Source-Code-Analysis/">Source Code Analysis</a> (21)</li><li><a href="/en/categories/Artificial-Intelligence/">Artificial Intelligence</a> (14)</li><li><a href="/en/categories/Discovery/">Discovery</a> (1)</li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"></i><span>Tags</span></div><div class="tagcloud"><a href="/en/tags/Python/" style="font-size: 15.00px;">Python</a> <a href="/en/tags/Google/" style="font-size: 15.00px;">Google</a> <a href="/en/tags/C/" style="font-size: 15.00px;">C++</a> <a href="/en/tags/ChatGPT/" style="font-size: 15.00px;">ChatGPT</a> <a href="/en/tags/Prompt/" style="font-size: 15.00px;">Prompt</a> <a href="/en/tags/Redis/" style="font-size: 15.00px;">Redis</a> <a href="/en/tags/Debug/" style="font-size: 15.00px;">Debug</a> <a href="/en/tags/eBPF/" style="font-size: 15.00px;">eBPF</a> <a href="/en/tags/Go/" style="font-size: 15.00px;">Go</a> <a href="/en/tags/Frontend/" style="font-size: 15.00px;">Frontend</a> <a href="/en/tags/Gemini/" style="font-size: 15.00px;">Gemini</a> <a href="/en/tags/SEO/" style="font-size: 15.00px;">SEO</a> <a href="/en/tags/LLM/" style="font-size: 15.00px;">LLM</a> <a href="/en/tags/Web/" style="font-size: 15.00px;">Web</a> <a href="/en/tags/LevelDB/" style="font-size: 15.00px;">LevelDB</a></div></div><!-- Debug: page.path = en/2025/06/13/leveldb_source_LRU_cache/ --><div class="widget"><div class="widget-title"><i class="fa fa-file-o"></i><span>Recent</span></div><ul><li><a href="/en/2025/06/27/leveldb_source_table_build/" title="LevelDB Explained - A Step by Step Guide to SSTable Build">LevelDB Explained - A Step by Step Guide to SSTable Build</a></li><li><a href="/en/2025/06/13/leveldb_source_LRU_cache/" title="LevelDB Explained - The Implementation Details of a High-Performance LRU Cache">LevelDB Explained - The Implementation Details of a High-Performance LRU Cache</a></li><li><a href="/en/2025/06/11/leveldb_source_memtable/" title="LevelDB Explained - The Implementation Details of MemTable">LevelDB Explained - The Implementation Details of MemTable</a></li><li><a href="/en/2025/06/10/leveldb_mvcc_intro/" title="LevelDB Explained - Understanding Multi-Version Concurrency Control (MVCC)">LevelDB Explained - Understanding Multi-Version Concurrency Control (MVCC)</a></li><li><a href="/en/2025/05/23/mcp_user_report/" title="In-depth Experience with 3 MCP Servers via Cursor: Impressive but Not Yet Practical?">In-depth Experience with 3 MCP Servers via Cursor: Impressive but Not Yet Practical?</a></li><li><a href="/en/2025/01/24/leveldb_source_writedb/" title="LevelDB Explained - Implementation and Optimization Details of Key-Value Writing">LevelDB Explained - Implementation and Optimization Details of Key-Value Writing</a></li><li><a href="/en/2025/01/13/leveldb_source_write_batch/" title="LevelDB Explained - Elegant Merging of Write and Delete Operations">LevelDB Explained - Elegant Merging of Write and Delete Operations</a></li><li><a href="/en/2025/01/10/c++_crash_cases/" title="5 Real-world Cases of C++ Process Crashes from Production">5 Real-world Cases of C++ Process Crashes from Production</a></li><li><a href="/en/2025/01/02/leveldb_source_thread_anno/" title="LevelDB Explained - Static Thread Safety Analysis with Clang">LevelDB Explained - Static Thread Safety Analysis with Clang</a></li><li><a href="/en/2024/12/25/leveldb_source_hashtable/" title="LevelDB Explained - How to Design a High-Performance HashTable">LevelDB Explained - How to Design a High-Performance HashTable</a></li></ul></div><!-- Debug: Current Language = en, Filtered Posts Count = 10 --><div class="widget" id="toc"><div class="widget-title"><i class="fa fa-list-ul"></i><span> Contents</span></div><ul class="dsq-widget-list"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Classic-LRU-Implementation"><span class="toc-number">1.</span> <span class="toc-text">Classic LRU Implementation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cache-Design-Dependency-Inversion"><span class="toc-number">2.</span> <span class="toc-text">Cache Design: Dependency Inversion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LRUHandle-Class-Implementation"><span class="toc-number">3.</span> <span class="toc-text">LRUHandle Class Implementation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LRUCache-Class-Implementation-Details"><span class="toc-number">4.</span> <span class="toc-text">LRUCache Class Implementation Details</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Why-Use-Two-Doubly-Linked-Lists"><span class="toc-number">4.1.</span> <span class="toc-text">Why Use Two Doubly Linked Lists?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cache-Insertion-Deletion-and-Lookup"><span class="toc-number">4.2.</span> <span class="toc-text">Cache Insertion, Deletion, and Lookup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Doubly-Linked-List-Operations"><span class="toc-number">4.3.</span> <span class="toc-text">Doubly Linked List Operations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reinterpret-cast-Conversion"><span class="toc-number">4.4.</span> <span class="toc-text">reinterpret_cast Conversion</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ShardedLRUCache-Implementation"><span class="toc-number">5.</span> <span class="toc-text">ShardedLRUCache Implementation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Cache-ID-Generation"><span class="toc-number">5.1.</span> <span class="toc-text">Cache ID Generation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Summary"><span class="toc-number">6.</span> <span class="toc-text">Summary</span></a></li></ol></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Just For Fun.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/selfboot/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/selfboot"> SelfBoot.</a><p><span id="busuanzi_container_site_pv"></span>Total Site Visits:  <span id="busuanzi_value_site_pv"></span> Times，<span id="busuanzi_container_site_uv"></span>Unique Visitors:  <span id="busuanzi_value_site_uv"></span> People</p><p>Friend Link:<a rel="dofollow" target="_blank" href="https://puzzles-game.com/">Puzzle Games</a></p></div></div></div><link rel="stylesheet" type="text/css" href="/css/donate.css"><link rel="stylesheet" type="text/css" href="/css/copyright.css"><link rel="stylesheet" type="text/css" href="/css/recommendations.css"><a class="show" id="rocket" href="#top" aria-label="Back to top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script><script type="text/javascript" src="/js/jquery.fancybox.min.js" defer=""></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" defer=""></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async=""></script><script type="text/javascript" src="/js/toc.js?v=1.0.0"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>